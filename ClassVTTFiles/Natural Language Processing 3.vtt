WEBVTT - This file was automatically generated by VIMEO

0
00:00:11.705 --> 00:00:12.245
Hey everybody.

1
00:00:12.945 --> 00:00:15.685
Uh, welcome to the session.

2
00:00:16.535 --> 00:00:20.285
We're gonna wait like two minutes just to make sure

3
00:00:20.315 --> 00:00:24.245
that everybody is here, um,

4
00:00:24.345 --> 00:00:25.605
before we get started.

5
00:00:26.545 --> 00:00:29.565
So let's just settle in for another one or two minutes

6
00:00:30.065 --> 00:00:32.625
and then we will

7
00:00:33.645 --> 00:00:36.415
discuss natural language processing part three.

8
00:00:44.245 --> 00:00:45.275
Great. Uh,

9
00:00:45.275 --> 00:00:47.035
we got a pretty packed agenda, so let's get started.

10
00:00:48.355 --> 00:00:51.895
Um, let's see if we're good. Yep.

11
00:00:53.255 --> 00:00:56.855
Uh, pen is working. Let's get moving.

12
00:00:58.955 --> 00:01:02.135
Uh, so natural language processing part three.

13
00:01:03.075 --> 00:01:06.295
So we first had, uh, natural language processing one

14
00:01:06.475 --> 00:01:08.415
and then two, and then now we are in three.

15
00:01:09.515 --> 00:01:10.895
My name is Ben Hanks.

16
00:01:11.095 --> 00:01:15.015
I will be your mc slash teacher, uh, for the morning,

17
00:01:15.125 --> 00:01:16.975
afternoon, evening, depending on

18
00:01:16.975 --> 00:01:18.415
where you are currently located.

19
00:01:21.525 --> 00:01:24.865
And I am a data

20
00:01:24.965 --> 00:01:28.105
and applied scientist at, uh, Microsoft,

21
00:01:28.415 --> 00:01:31.145
currently sitting in Seattle, Washington.

22
00:01:31.845 --> 00:01:33.185
Um, probably about 20,

23
00:01:33.185 --> 00:01:35.345
30 minutes drive away from the headquarters.

24
00:01:36.005 --> 00:01:39.345
Uh, I work from home in my little shed here.

25
00:01:40.285 --> 00:01:43.465
Um, and I really like working there. Um, it's been fun.

26
00:01:43.845 --> 00:01:47.865
Uh, the same title would at different companies potentially

27
00:01:47.865 --> 00:01:52.155
be people, science analyst, uh, pe, um,

28
00:01:52.385 --> 00:01:56.155
data analyst, data scientist, like that kind of realm.

29
00:01:56.775 --> 00:02:00.075
Um, but Microsoft's interpretation of this role is data

30
00:02:00.095 --> 00:02:03.685
and applied scientist to, Uh,

31
00:02:03.845 --> 00:02:06.205
I have my master's in data analytics

32
00:02:06.925 --> 00:02:08.165
from Oregon State University.

33
00:02:08.355 --> 00:02:09.965
It's from the College of Statistics

34
00:02:10.025 --> 00:02:11.485
or within the College of Statistics.

35
00:02:11.485 --> 00:02:13.805
So it's largely theoretical, um,

36
00:02:14.585 --> 00:02:16.565
and a little bit applied.

37
00:02:17.805 --> 00:02:21.265
Um, and my undergraduate degree is also from Oregon State

38
00:02:21.265 --> 00:02:22.785
University and it's manufacturing

39
00:02:23.005 --> 00:02:25.745
and industrial engineering, which is, uh,

40
00:02:25.985 --> 00:02:27.465
building stuff and statistics.

41
00:02:33.335 --> 00:02:34.885
Let's get to know each other a little bit.

42
00:02:35.145 --> 00:02:39.585
Uh, so it's going to be me talking today

43
00:02:39.645 --> 00:02:40.665
for about four hours.

44
00:02:41.525 --> 00:02:44.625
Uh, so as much interactivity and back

45
00:02:44.625 --> 00:02:47.105
and forth as we can get, especially in the chat, uh,

46
00:02:47.245 --> 00:02:51.025
is gonna be super helpful for making a good engaging class.

47
00:02:51.565 --> 00:02:54.665
So let's start that right now at the start of the class.

48
00:02:55.445 --> 00:02:57.065
Um, so drop into the chat.

49
00:02:57.215 --> 00:02:59.485
Your name, your role, your company,

50
00:02:59.795 --> 00:03:01.525
like rough area where you're based.

51
00:03:02.975 --> 00:03:04.195
Thanks, Vasan already.

52
00:03:04.815 --> 00:03:07.835
Uh, dropping into the chat, some from California.

53
00:03:08.655 --> 00:03:11.075
Uh, GM software engineer from Texas.

54
00:03:11.265 --> 00:03:13.195
Another Ben, a wonderful name.

55
00:03:14.245 --> 00:03:15.655
I've been to Austin a couple of times.

56
00:03:16.075 --> 00:03:17.575
Um, great cycling around there.

57
00:03:18.755 --> 00:03:23.735
Um, someone from Lima, a data engineer or San Francisco.

58
00:03:24.515 --> 00:03:24.735
Um,

59
00:03:29.395 --> 00:03:33.455
how is an EM in San Francisco?

60
00:03:37.925 --> 00:03:39.545
And then while responses come in, uh,

61
00:03:39.545 --> 00:03:40.865
just a fun fact about me.

62
00:03:41.205 --> 00:03:44.025
Um, I ride my bike a lot after this lecture.

63
00:03:44.165 --> 00:03:45.505
I'm going on a mountain bike ride.

64
00:03:46.445 --> 00:03:50.025
Um, and bikes are a great way to

65
00:03:51.085 --> 00:03:54.865
get exercise and have an

66
00:03:55.065 --> 00:03:56.265
interesting hobby all at the same time.

67
00:03:56.805 --> 00:03:58.425
Oh, if a sun rides a road bike. Awesome.

68
00:03:58.845 --> 00:04:01.585
Uh, behind me in the blur, you can see my track bike.

69
00:04:01.935 --> 00:04:03.465
I've had a couple of tracks as well.

70
00:04:03.605 --> 00:04:05.905
Um, actually in a race, I broke, uh, the frame

71
00:04:05.905 --> 00:04:08.465
with my kneecap once, uh, crashed

72
00:04:08.525 --> 00:04:10.305
and smashed the frame with my knee.

73
00:04:11.255 --> 00:04:12.615
I was surprisingly fine.

74
00:04:15.395 --> 00:04:17.495
Uh, all right, let's talk data science.

75
00:04:19.345 --> 00:04:23.925
Uh, so like I said before, um, I am here for you.

76
00:04:24.565 --> 00:04:27.045
I am here to discuss things with you all.

77
00:04:27.675 --> 00:04:30.845
This is a class, um,

78
00:04:31.075 --> 00:04:33.485
that you are supposed to get value out of.

79
00:04:33.625 --> 00:04:36.365
So don't be shy. There are no stupid questions.

80
00:04:36.785 --> 00:04:38.205
Um, we are here to learn.

81
00:04:38.625 --> 00:04:41.485
If you already knew the topic, you would not be here.

82
00:04:41.905 --> 00:04:44.525
So feel free to ask any question, no matter

83
00:04:44.945 --> 00:04:47.045
how potentially embarrassed you may be.

84
00:04:47.505 --> 00:04:50.245
Um, I will be happy when we get any question.

85
00:04:51.535 --> 00:04:53.515
Um, so don't be shy to speak up.

86
00:04:53.855 --> 00:04:56.435
We do have a TA support today, uh,

87
00:04:56.435 --> 00:05:00.435
and they will be active in the chat to help answer questions

88
00:05:00.745 --> 00:05:02.675
that I'm too distracted to get to.

89
00:05:04.585 --> 00:05:07.725
Um, we're gonna be doing a review session on

90
00:05:08.085 --> 00:05:09.365
Thursday for assignments.

91
00:05:09.585 --> 00:05:13.605
And then, um, there is a career, uh, review session

92
00:05:14.185 --> 00:05:16.685
and like collaboration session on Wednesday,

93
00:05:17.345 --> 00:05:18.405
uh, Pacific time.

94
00:05:18.965 --> 00:05:20.725
I think we're, a lot of us are on Pacific.

95
00:05:22.085 --> 00:05:27.065
Um, and yeah, just let me know if we want to recover

96
00:05:27.245 --> 00:05:31.065
or review a subject, um, on any of these slides.

97
00:05:39.185 --> 00:05:41.565
All right. So just a quick review

98
00:05:41.655 --> 00:05:43.525
where we are in the natural language

99
00:05:43.575 --> 00:05:45.525
processing, uh, framework.

100
00:05:46.065 --> 00:05:48.805
We are here, natural language processing three.

101
00:05:49.345 --> 00:05:50.485
We are almost done.

102
00:05:51.065 --> 00:05:52.925
We are here today at the end

103
00:05:52.925 --> 00:05:54.485
of this session, we will end here.

104
00:05:55.185 --> 00:05:57.925
So I am stoked to get to the, the end of this vector.

105
00:05:59.155 --> 00:06:03.135
Um, so natural language processing one, that's

106
00:06:03.135 --> 00:06:06.135
where we talked about text, pre-processing, uh,

107
00:06:06.135 --> 00:06:09.055
exploratory data analysis using some libraries,

108
00:06:09.775 --> 00:06:11.455
sentiment analysis, image captioning.

109
00:06:11.455 --> 00:06:13.575
That was really cool. Um,

110
00:06:13.925 --> 00:06:16.535
natural language processing part two.

111
00:06:17.315 --> 00:06:19.575
Uh, we discussed a little bit

112
00:06:19.575 --> 00:06:21.095
of like sequence to sequence modeling.

113
00:06:21.185 --> 00:06:23.055
We're gonna touch on that again today.

114
00:06:23.755 --> 00:06:25.855
Um, and then transformers, part one.

115
00:06:26.515 --> 00:06:29.535
Um, today we are going to talk about transformers,

116
00:06:29.685 --> 00:06:31.135
part two, revenge of the fallen.

117
00:06:31.835 --> 00:06:36.815
Uh, and then Bert, uh, which is a really cool algorithm,

118
00:06:37.395 --> 00:06:38.935
um, that we will, uh,

119
00:06:39.065 --> 00:06:41.735
focus on a little bit towards the end of the class.

120
00:06:46.275 --> 00:06:48.565
Like I said before, sequence sequence models.

121
00:06:48.785 --> 00:06:53.365
Uh, we discussed some well-known archetypes architectures,

122
00:06:54.555 --> 00:06:57.935
um, and we leverage some nns for current neural networks

123
00:06:58.315 --> 00:07:02.895
to drive the sequence encoder context, vector decoder output

124
00:07:03.535 --> 00:07:06.635
sequence, um, pipeline.

125
00:07:06.775 --> 00:07:10.395
And then in this class, we're gonna be using transformers

126
00:07:10.395 --> 00:07:13.155
to fix some of the shortcomings that we had in, uh,

127
00:07:13.155 --> 00:07:16.555
the previous more vanilla algorithms.

128
00:07:19.785 --> 00:07:21.005
So we are here.

129
00:07:22.365 --> 00:07:27.145
Um, we're going to start with, uh, the recap,

130
00:07:27.145 --> 00:07:28.785
which we already discussed a little bit.

131
00:07:29.055 --> 00:07:31.785
Then we're gonna discuss some transformers, um,

132
00:07:32.185 --> 00:07:33.705
transformer components, dive a little bit

133
00:07:33.705 --> 00:07:34.985
deeper into transformers.

134
00:07:35.535 --> 00:07:39.025
Then, um, we're gonna take a little bit of a break.

135
00:07:39.215 --> 00:07:41.865
I'll try and do two to three breaks

136
00:07:41.935 --> 00:07:43.345
that are around five or 10 minutes.

137
00:07:44.205 --> 00:07:46.345
Um, we do have a lot of content to cover today,

138
00:07:46.485 --> 00:07:49.785
so I will try and balance, uh, how terrible the test

139
00:07:49.785 --> 00:07:51.825
of endurance is with, uh,

140
00:07:52.165 --> 00:07:54.705
how well we are doing progressing through the class.

141
00:07:56.125 --> 00:07:57.505
Um, and then

142
00:07:58.035 --> 00:08:01.865
after the first section, we're going to discuss Bert,

143
00:08:02.255 --> 00:08:05.425
Roberta, uh, a couple of other Bert algorithms

144
00:08:05.565 --> 00:08:07.065
and Burt adjacent algorithms.

145
00:08:07.485 --> 00:08:09.545
Um, and then we're gonna do some code demos.

146
00:08:20.425 --> 00:08:22.605
All right, let's get into transformers.

147
00:08:23.825 --> 00:08:28.205
So problems with what we've learned so far, we've had kind

148
00:08:28.205 --> 00:08:32.445
of the traditional setup of, um, the sequence

149
00:08:32.445 --> 00:08:33.525
to sequence pipeline.

150
00:08:34.435 --> 00:08:36.975
So s two s sequence, two sequence.

151
00:08:37.315 --> 00:08:40.975
We have the input, um, and then we have our encoder,

152
00:08:43.295 --> 00:08:46.075
and then we have our context vector.

153
00:08:49.245 --> 00:08:51.265
Um, and then that is decoded.

154
00:08:53.955 --> 00:08:56.685
And then we have an output of a sequence.

155
00:09:00.535 --> 00:09:02.515
So we have sequence in and then sequence out.

156
00:09:02.975 --> 00:09:04.795
Um, also feel free to drop in the chat.

157
00:09:04.975 --> 00:09:07.795
If, uh, you cannot read my writing, I will try harder.

158
00:09:08.975 --> 00:09:12.835
Um, so the biggest issue with this methodology is

159
00:09:12.835 --> 00:09:17.595
that everything needs to fit in this context vector.

160
00:09:20.555 --> 00:09:24.655
Uh, so like, let's say we have a huge text, like pages

161
00:09:24.875 --> 00:09:27.775
of a book, like an entire Iliad or something like that.

162
00:09:28.395 --> 00:09:30.415
Um, that means that everything needs

163
00:09:30.415 --> 00:09:33.015
to fit into this context vector and then be translated.

164
00:09:33.595 --> 00:09:37.695
Um, and then like if we pass on the vector

165
00:09:38.025 --> 00:09:41.695
after that, I believe in Santa very short vector, um,

166
00:09:41.965 --> 00:09:44.935
then we, um, are not adaptable in size.

167
00:09:45.755 --> 00:09:50.415
So that sequence could take, um, a significant amount of

168
00:09:51.305 --> 00:09:54.455
extra storage space, so it's not quite as efficient.

169
00:09:56.545 --> 00:10:01.165
Um, also decoders decode one token at a

170
00:10:01.165 --> 00:10:02.765
time sequentially.

171
00:10:03.585 --> 00:10:05.885
And in this presentation,

172
00:10:05.885 --> 00:10:09.805
we're gonna be talking a lot about parallel work, um, which

173
00:10:10.415 --> 00:10:12.925
makes the process much more efficient.

174
00:10:14.675 --> 00:10:18.215
So especially in large strings of text, which we're, um,

175
00:10:18.545 --> 00:10:19.695
gonna be doing a lot,

176
00:10:19.835 --> 00:10:23.615
and this extra power we get through parallel, um,

177
00:10:23.845 --> 00:10:26.775
work really helps us, uh, in more

178
00:10:27.315 --> 00:10:30.215
robust corpuses lots and lots and lots of texts.

179
00:10:30.875 --> 00:10:31.095
So

180
00:10:37.165 --> 00:10:38.895
what is a transformer?

181
00:10:42.455 --> 00:10:46.675
So long story short on transformers is we take a text

182
00:10:47.075 --> 00:10:48.355
sequence as an input

183
00:10:49.175 --> 00:10:52.555
and we produce another text sequence as an output.

184
00:10:52.935 --> 00:10:57.555
So we transform, a classic example is input

185
00:10:58.455 --> 00:10:59.955
in a specific language

186
00:11:00.255 --> 00:11:02.515
and then output in a different language.

187
00:11:02.855 --> 00:11:07.295
So like Google translating, um,

188
00:11:07.915 --> 00:11:09.095
tra uh, encoding that,

189
00:11:09.115 --> 00:11:11.615
and then decoding it as, I am a student,

190
00:11:12.285 --> 00:11:14.535
this is French, or I am a student.

191
00:11:15.035 --> 00:11:17.135
And this is English for the same concept.

192
00:11:21.275 --> 00:11:24.575
So structurally, transformers consists

193
00:11:24.755 --> 00:11:28.695
of two main concepts, the encoder here,

194
00:11:32.475 --> 00:11:34.295
and the encoder processes

195
00:11:34.435 --> 00:11:36.935
and embeds the input sequence information,

196
00:11:37.635 --> 00:11:39.055
and the decoder here,

197
00:11:40.715 --> 00:11:44.505
which generates the desired output while preserving the

198
00:11:44.505 --> 00:11:46.425
context, which is very important.

199
00:11:46.425 --> 00:11:49.105
And we'll get into and semantics.

200
00:11:51.875 --> 00:11:55.125
So in language translation, encoder process, uh,

201
00:11:55.185 --> 00:11:57.085
on the input language, embeds

202
00:11:58.475 --> 00:12:01.005
that value into a context vector.

203
00:12:02.385 --> 00:12:04.445
So you can see context vector right here.

204
00:12:05.865 --> 00:12:08.445
And then the decoder uses the context vector

205
00:12:08.665 --> 00:12:10.205
to generate the translation,

206
00:12:11.045 --> 00:12:13.005
maintaining the original meaning and context.

207
00:12:15.295 --> 00:12:18.635
And with these concepts in the next coming slides,

208
00:12:18.725 --> 00:12:21.875
we're gonna explore how transformers paved the way

209
00:12:22.095 --> 00:12:23.955
for advanced models like Bert,

210
00:12:24.095 --> 00:12:25.915
and even a little bit chat, GBT,

211
00:12:25.915 --> 00:12:28.715
which is the coolest model in the world right

212
00:12:28.715 --> 00:12:30.695
now, debatably.

213
00:12:34.235 --> 00:12:35.775
All right, so there's a lot

214
00:12:35.775 --> 00:12:37.615
of stuff going on in this slide, don't worry.

215
00:12:37.625 --> 00:12:38.655
We'll get through it together.

216
00:12:40.875 --> 00:12:44.875
Um, so here's translation.

217
00:12:45.305 --> 00:12:47.835
Translation is really, really complex.

218
00:12:48.775 --> 00:12:53.435
And in modern day, it's almost, uh, a non-issue as long

219
00:12:53.435 --> 00:12:54.955
as you have a computer by your side.

220
00:12:55.255 --> 00:12:57.315
And that's because of natural language processing.

221
00:12:57.375 --> 00:13:00.395
We have solved a lot of this problem.

222
00:13:02.005 --> 00:13:06.025
Um, so one of the biggest problems in translation

223
00:13:06.645 --> 00:13:10.625
is going to be things like word by word or ordering.

224
00:13:12.375 --> 00:13:17.035
Um, so if we look here at the top,

225
00:13:17.775 --> 00:13:20.955
can you help this sentence to, or can you help?

226
00:13:21.215 --> 00:13:23.475
See, even I'm having a hard time looking at the order.

227
00:13:23.975 --> 00:13:27.635
Can you help this sentence to translate?

228
00:13:28.055 --> 00:13:30.235
That's if you take this sentence

229
00:13:30.735 --> 00:13:32.595
and have direct translation,

230
00:13:33.185 --> 00:13:36.795
because in English, we have one set of grammar rules

231
00:13:37.215 --> 00:13:39.115
that's not the same in other languages.

232
00:13:40.015 --> 00:13:43.675
So to make sense, we need to have

233
00:13:44.335 --> 00:13:47.755
the AI algorithm use this methodology.

234
00:13:48.935 --> 00:13:53.435
So that is a much more robust, more difficult

235
00:13:54.035 --> 00:13:57.585
analysis, because this word needs

236
00:13:57.585 --> 00:13:58.865
to move all the way over here.

237
00:13:58.885 --> 00:14:02.545
We can't just translate it, um, straight as shown here.

238
00:14:05.545 --> 00:14:09.045
So to address this, attention mechanisms were developed,

239
00:14:09.575 --> 00:14:10.885
which allow models

240
00:14:11.425 --> 00:14:14.445
to access all input sequence elements simultaneously,

241
00:14:15.605 --> 00:14:18.865
and select words that are the most contextually relevant

242
00:14:19.605 --> 00:14:22.935
at each time, at each step.

243
00:14:24.545 --> 00:14:28.525
So right around 2017, the transformer architecture, uh,

244
00:14:28.875 --> 00:14:32.925
took another step forward by introducing, uh,

245
00:14:32.925 --> 00:14:36.165
concepts like self attention, which enabled it

246
00:14:36.165 --> 00:14:38.965
to process words all in parallel.

247
00:14:39.055 --> 00:14:41.525
Again, there's that parallel word, uh,

248
00:14:41.655 --> 00:14:44.965
which bypassed the need for recurrent neural networks

249
00:14:45.705 --> 00:14:47.485
and enhanced the scalability

250
00:14:47.625 --> 00:14:50.725
and efficiency of natural language processing.

251
00:14:52.535 --> 00:14:53.875
So with the foundation of

252
00:14:53.895 --> 00:14:57.835
how attention mechanisms enhance sequence modeling, uh,

253
00:14:57.925 --> 00:15:00.315
we're gonna move on to explore the ins

254
00:15:00.315 --> 00:15:01.835
and outs of more advanced models

255
00:15:02.695 --> 00:15:04.995
and ensure that we understand the building blocks

256
00:15:04.995 --> 00:15:07.675
that make them very, very effective in modern day.

257
00:15:10.145 --> 00:15:13.485
Um, again, feel free to ask as many questions as you can.

258
00:15:14.665 --> 00:15:17.565
Um, let's get stoked on natural language processing.

259
00:15:21.185 --> 00:15:22.685
All right, let's talk about transformers.

260
00:15:23.955 --> 00:15:26.095
Um, so first we're gonna kind

261
00:15:26.095 --> 00:15:28.055
of talk about the training phase of the transformers.

262
00:15:28.795 --> 00:15:31.935
Um, and then we're going to, uh, potentially make inference

263
00:15:32.115 --> 00:15:34.895
and, uh, prediction, uh, through

264
00:15:35.905 --> 00:15:37.255
these next few slides.

265
00:15:41.435 --> 00:15:46.085
So, unpacking transformer training, uh, let's demystify

266
00:15:47.145 --> 00:15:50.445
how transformers operate during their training phase,

267
00:15:50.695 --> 00:15:53.805
where models learn to make accurate predictions

268
00:15:53.985 --> 00:15:57.565
by adjusting its parameters based on the input

269
00:15:58.315 --> 00:15:59.575
and the target output.

270
00:16:00.195 --> 00:16:02.455
So here you can see the transformer.

271
00:16:03.155 --> 00:16:07.655
Let me switch to a highlighter, so it's a little bit easier

272
00:16:07.755 --> 00:16:08.775
to see the background.

273
00:16:09.275 --> 00:16:10.295
So this here is the process.

274
00:16:10.955 --> 00:16:13.695
The input text is, you are welcome right here.

275
00:16:14.915 --> 00:16:17.895
Oh my gosh, that's terrible. Highlighting. You are welcome.

276
00:16:18.875 --> 00:16:20.815
And then if we follow this pipeline,

277
00:16:21.435 --> 00:16:23.495
we enter the encoder phase, part one,

278
00:16:24.195 --> 00:16:26.055
and then we have a context factor out.

279
00:16:26.755 --> 00:16:30.635
Uh, and then here is an exciting concept.

280
00:16:30.895 --> 00:16:35.355
We can encoder it again to improve, um,

281
00:16:36.295 --> 00:16:38.815
the model accuracy.

282
00:16:39.545 --> 00:16:41.255
We'll talk about that in the next few slides.

283
00:16:41.325 --> 00:16:43.655
Just we can do it for now.

284
00:16:44.595 --> 00:16:48.535
Uh, and then we continue training, have the encoder out,

285
00:16:48.875 --> 00:16:52.825
and then we go through some decoder processes, um,

286
00:16:53.165 --> 00:16:57.065
and then have the very simple output of donata

287
00:16:57.805 --> 00:16:59.305
at the end of all these processes.

288
00:17:04.385 --> 00:17:08.375
So just voicing that a little bit in more detail.

289
00:17:09.225 --> 00:17:12.365
Um, initially the input sequence undergoes

290
00:17:12.365 --> 00:17:13.645
several transformations.

291
00:17:14.515 --> 00:17:19.425
It's tokenized into smaller units, uh, such

292
00:17:19.425 --> 00:17:21.985
as words and sub words and things like that.

293
00:17:22.405 --> 00:17:25.265
Uh, one hot vectors, so that it's computable

294
00:17:25.365 --> 00:17:27.625
and that it's supplemented with potential embeddings

295
00:17:27.625 --> 00:17:29.185
to preserve the order of the words

296
00:17:29.245 --> 00:17:30.465
and the context which they're in.

297
00:17:31.605 --> 00:17:35.945
Um, and then as we go through the encoders, um,

298
00:17:36.605 --> 00:17:40.385
the process input navigates through lots

299
00:17:40.385 --> 00:17:42.465
of different layers, sometimes as many

300
00:17:42.485 --> 00:17:46.105
as six encoding layers, um, where each layer works

301
00:17:46.125 --> 00:17:50.835
to refine, refine, refine each time it goes

302
00:17:50.835 --> 00:17:52.115
through another encoder.

303
00:17:54.595 --> 00:17:57.215
Uh, and then looking at attention vectors,

304
00:17:57.215 --> 00:17:59.175
which we'll discuss a little bit more coming out,

305
00:17:59.175 --> 00:18:00.895
this is kind of the 30,000 foot view.

306
00:18:02.415 --> 00:18:04.395
Um, once we reach the final encoder,

307
00:18:04.455 --> 00:18:05.915
we obtain the attention vector.

308
00:18:07.395 --> 00:18:11.525
This vector has a ton of contextual information

309
00:18:12.065 --> 00:18:13.965
for each token in the input sequence.

310
00:18:14.785 --> 00:18:19.765
And that is gonna be pivotal in generating the output, uh,

311
00:18:19.995 --> 00:18:21.885
subsequence and the decoding phase.

312
00:18:23.615 --> 00:18:25.955
During the decoding phase, uh,

313
00:18:26.025 --> 00:18:30.915
that attention vector is utilized within the decoder, uh,

314
00:18:31.015 --> 00:18:34.515
to produce the desired output sequence, which ensures

315
00:18:34.825 --> 00:18:36.275
that it is contextually

316
00:18:36.815 --> 00:18:39.635
and semantically aligned with the input.

317
00:18:43.835 --> 00:18:48.015
And as we transition into discussing, uh, the encoding phase

318
00:18:48.115 --> 00:18:49.295
and then the decoding phase

319
00:18:49.295 --> 00:18:53.095
after that, uh, just to remember that this attention vector

320
00:18:53.115 --> 00:18:57.215
and context vectors, they both play crucial roles in, uh,

321
00:18:57.575 --> 00:18:59.935
bridging that encoded output with the decoded output.

322
00:19:01.295 --> 00:19:03.235
So during this first part of the presentation,

323
00:19:03.565 --> 00:19:06.195
we're gonna have encoder, which is the start,

324
00:19:06.775 --> 00:19:08.795
and that encodes the input.

325
00:19:09.135 --> 00:19:11.955
And then we're going to have decoder at the end,

326
00:19:12.175 --> 00:19:14.035
and that decodes the input.

327
00:19:14.655 --> 00:19:17.075
And then after that, we've got the outputs.

328
00:19:30.895 --> 00:19:35.245
So again, at a top level, um,

329
00:19:36.425 --> 00:19:40.365
we can look at the decoder phase right over here

330
00:19:43.745 --> 00:19:45.275
with trending of the transformers.

331
00:19:46.135 --> 00:19:49.715
Um, unlike traditional methods, the entire target sequence

332
00:19:50.625 --> 00:19:54.525
is gonna be fed into the decoder, which is a technique kind

333
00:19:54.525 --> 00:19:55.925
of like teacher forcing.

334
00:19:57.995 --> 00:20:02.245
So teacher forcing allows all output tokens

335
00:20:02.245 --> 00:20:05.665
to be predicted in a single step, which is really cool.

336
00:20:09.385 --> 00:20:12.805
Um, the processed target sequence is gonna be passed

337
00:20:12.805 --> 00:20:14.085
through several decoder layers,

338
00:20:15.585 --> 00:20:17.315
just like we had with encoder.

339
00:20:17.545 --> 00:20:18.875
This could be repeated many times,

340
00:20:22.935 --> 00:20:27.515
um, and the output is going to be the most

341
00:20:28.195 --> 00:20:30.315
accurate embedding possible.

342
00:20:33.785 --> 00:20:37.785
Ultimately, the decoder outputs a sequence that corresponds

343
00:20:37.785 --> 00:20:40.305
to the tokens with the highest probability

344
00:20:40.765 --> 00:20:45.545
of being the accurate Next word, crafting a coherent,

345
00:20:45.705 --> 00:20:48.485
contextually relevant output sequence.

346
00:20:53.205 --> 00:20:54.425
How? That's a great question.

347
00:20:54.805 --> 00:20:58.425
Uh, looking at how teacher forcing is done

348
00:20:58.425 --> 00:20:59.425
through the next few steps.

349
00:21:00.005 --> 00:21:01.345
Um, we will be getting to

350
00:21:01.345 --> 00:21:03.185
that a little bit in the next couple of slides.

351
00:21:10.935 --> 00:21:12.795
All right, looking at the inference phase.

352
00:21:21.555 --> 00:21:23.615
So the only difference between the training

353
00:21:23.715 --> 00:21:26.575
and the inference phase target sequence

354
00:21:27.115 --> 00:21:31.335
is fed sequentially to the decoder part of the prediction.

355
00:21:31.955 --> 00:21:35.175
Um, and we don't have access to those future tokens.

356
00:21:36.575 --> 00:21:40.195
Um, so let's explore what occurs during

357
00:21:40.195 --> 00:21:44.075
that inference phase, which is when the trained model

358
00:21:44.735 --> 00:21:47.675
is deployed to make predictions on unseen data.

359
00:21:49.295 --> 00:21:52.035
Um, different differentiating the training

360
00:21:52.175 --> 00:21:53.195
and the inference here.

361
00:21:55.005 --> 00:21:56.145
Um, training

362
00:21:56.165 --> 00:21:58.145
and inference phrases might seem pretty similar,

363
00:21:58.845 --> 00:22:01.905
but the fundamental distinction is during inference,

364
00:22:02.845 --> 00:22:06.625
we lack access to the actual target sequence.

365
00:22:08.075 --> 00:22:10.615
So predictions have to be made sequentially

366
00:22:10.675 --> 00:22:11.895
as opposed to in parallel.

367
00:22:13.305 --> 00:22:15.165
Um, unlike in the training phase,

368
00:22:15.995 --> 00:22:19.495
the entire sequence is available, the entire target sequence

369
00:22:20.075 --> 00:22:22.175
and can be fed into the decoder.

370
00:22:22.975 --> 00:22:24.995
Um, and during inference,

371
00:22:25.415 --> 00:22:27.635
the target sequence is input into the

372
00:22:27.635 --> 00:22:29.035
decoder token by token.

373
00:22:29.755 --> 00:22:32.095
And predictions are made in a stepwise manner

374
00:22:32.435 --> 00:22:33.735
as opposed to parallel.

375
00:22:36.075 --> 00:22:40.285
Um, the token by token sequence, uh, can have

376
00:22:40.845 --> 00:22:43.125
implications on the output quality

377
00:22:43.945 --> 00:22:45.805
and consistency, uh, given

378
00:22:45.805 --> 00:22:49.045
that the model does not have future context to rely on.

379
00:22:49.865 --> 00:22:51.005
And, uh, we have

380
00:22:51.005 --> 00:22:53.565
to make the best prediction possible based on the

381
00:22:53.565 --> 00:22:54.805
current and past tokens.

382
00:22:59.455 --> 00:23:01.075
And with a grasp on

383
00:23:01.075 --> 00:23:04.275
what differentiates the input, the inference phase.

384
00:23:04.895 --> 00:23:07.675
Uh, let's get a little bit deeper into further aspects

385
00:23:07.675 --> 00:23:08.795
of transformer models

386
00:23:08.975 --> 00:23:12.755
and kind of explore, uh, how they're adapted

387
00:23:12.755 --> 00:23:17.035
and fine tuned for specific natural language processing, um,

388
00:23:17.485 --> 00:23:19.435
tasks in the upcoming discussion.

389
00:23:25.845 --> 00:23:27.275
Great questions so far.

390
00:23:27.615 --> 00:23:31.795
Uh, thank you, uh, to the TA for answering those quickly.

391
00:23:33.215 --> 00:23:34.475
Um, oh,

392
00:23:34.475 --> 00:23:37.715
and it looks like we've got already an answer on the, uh,

393
00:23:37.715 --> 00:23:38.795
teacher forcing function.

394
00:23:40.115 --> 00:23:43.795
Thank you. Keep the questions coming.

395
00:23:47.105 --> 00:23:48.605
Um, best way to learn.

396
00:23:53.395 --> 00:23:57.655
So some advantages, some advantages of transformers.

397
00:23:58.245 --> 00:24:02.855
Like I was saying, um, parallel processing really improves

398
00:24:03.395 --> 00:24:06.865
the speed of the models.

399
00:24:08.535 --> 00:24:12.995
And while sometimes we can run those, uh, models

400
00:24:13.455 --> 00:24:15.275
and algorithms in the backend,

401
00:24:15.415 --> 00:24:19.915
and speed is not a huge concern, um, what makes

402
00:24:20.875 --> 00:24:24.155
parallel processing and speed important is

403
00:24:24.155 --> 00:24:26.395
because we can do larger tasks.

404
00:24:27.935 --> 00:24:31.735
So in prior days

405
00:24:31.735 --> 00:24:33.375
before all this parallel processing

406
00:24:33.375 --> 00:24:35.055
and increased, uh, computing power

407
00:24:35.115 --> 00:24:39.135
and speed, um, we would try and do these big tasks

408
00:24:39.315 --> 00:24:41.455
and fit models on, you know, billions of records,

409
00:24:41.955 --> 00:24:43.095
and it would just not work.

410
00:24:43.165 --> 00:24:45.295
Even if we gave it a billion hours of time,

411
00:24:46.195 --> 00:24:48.855
it would just have overflow errors and things like that.

412
00:24:49.475 --> 00:24:52.575
Um, so now with all these improved parallel, uh,

413
00:24:52.575 --> 00:24:54.255
methodologies and cloud computing

414
00:24:54.255 --> 00:24:57.295
and things like that, we can solve these problems that

415
00:24:57.295 --> 00:24:58.775
before would just error out

416
00:24:58.915 --> 00:25:01.055
and just, uh, have overload issues.

417
00:25:01.595 --> 00:25:05.055
So that, um, while it may not seem super critical,

418
00:25:05.675 --> 00:25:08.135
is a huge importance.

419
00:25:10.095 --> 00:25:13.195
Um, and then another thing is having effective handling

420
00:25:13.195 --> 00:25:14.675
of long-term dependencies.

421
00:25:16.215 --> 00:25:19.035
Um, increased model capacity, which is really related

422
00:25:19.055 --> 00:25:21.835
to this step as well, um, or advantage.

423
00:25:22.335 --> 00:25:25.075
And then the flexibility, uh, with variable length

424
00:25:25.075 --> 00:25:27.075
and sequence, um, again,

425
00:25:27.075 --> 00:25:29.715
really compresses our models and things like that.

426
00:25:34.345 --> 00:25:38.695
Right, let's keep rolling. Uh, so some limitations.

427
00:25:41.015 --> 00:25:43.715
Um, we're still gonna get pretty high computational costs,

428
00:25:44.455 --> 00:25:46.635
so we're just kind of doing a ton of stuff.

429
00:25:47.375 --> 00:25:50.905
Um, and with transformers,

430
00:25:51.555 --> 00:25:56.225
while we have severely improved our computational expense,

431
00:25:56.855 --> 00:25:58.265
it's still going to be a lot of stuff.

432
00:25:59.275 --> 00:26:01.215
Um, there is some inefficiency.

433
00:26:01.835 --> 00:26:05.775
Um, we're always with AI going to be, have to, uh, going

434
00:26:05.775 --> 00:26:07.895
to have to consider overfitting, uh,

435
00:26:08.215 --> 00:26:11.255
transformers is especially, uh, non robust to that.

436
00:26:11.915 --> 00:26:14.095
And then hyper parameterization tuning

437
00:26:14.835 --> 00:26:16.895
and, uh, performance optimization is something

438
00:26:16.895 --> 00:26:19.015
that is really going to improve our models.

439
00:26:32.125 --> 00:26:34.745
All right, so that is most of the transformer section.

440
00:26:34.745 --> 00:26:37.465
We're gonna be talking about it more in the coming slides.

441
00:26:38.165 --> 00:26:41.785
Um, but that's most of the education on transformers.

442
00:26:43.085 --> 00:26:47.865
Um, now that we have that complete checked off,

443
00:26:49.255 --> 00:26:53.825
uh, let's talk more about in more detail encoders

444
00:26:54.245 --> 00:26:55.465
and decoders.

445
00:26:59.115 --> 00:27:02.095
So we're going to start this conversation

446
00:27:02.355 --> 00:27:03.695
by looking at incoders.

447
00:27:04.715 --> 00:27:06.335
Here's, remember

448
00:27:06.335 --> 00:27:09.055
before when we were talking about how we have encoder one,

449
00:27:09.125 --> 00:27:10.495
encoder two, and coer three

450
00:27:10.495 --> 00:27:12.175
and coer four, et cetera, et cetera.

451
00:27:12.795 --> 00:27:15.215
Uh, so here's that in a little bit more detail.

452
00:27:15.875 --> 00:27:18.775
Um, this could be encoder one, this could be encoder two,

453
00:27:18.775 --> 00:27:20.975
this could be encoder three, this could be encoder four,

454
00:27:21.675 --> 00:27:24.135
and we can repeat this whole block.

455
00:27:24.275 --> 00:27:26.655
We can repeat each one of these processing steps

456
00:27:26.795 --> 00:27:30.135
as many times as we want for additional model accuracy.

457
00:27:33.685 --> 00:27:37.525
Um, so let's talk about it at kind

458
00:27:37.525 --> 00:27:39.085
of a high level overview first.

459
00:27:40.955 --> 00:27:44.575
Um, so we're gonna talk about encoder layer unveiling

460
00:27:46.455 --> 00:27:50.995
Within every encoder, there's three primary layers,

461
00:27:51.855 --> 00:27:55.075
all interconnected with residual connections

462
00:27:55.135 --> 00:27:58.995
to avoid vanishing or exploding gradient or other issues.

463
00:27:59.965 --> 00:28:02.025
Uh, first the self attention layer,

464
00:28:03.965 --> 00:28:08.025
which we're gonna talk about first allows the model

465
00:28:08.125 --> 00:28:11.665
to focus on different words for a given input.

466
00:28:13.155 --> 00:28:16.325
Next, the layer normalization, commonly known as layer,

467
00:28:16.395 --> 00:28:21.125
norm, uh, stabilizes activations of the neurons.

468
00:28:21.385 --> 00:28:24.765
And finally, the feed forward neural network, uh,

469
00:28:24.765 --> 00:28:27.565
performs specific transformations of the activation layers.

470
00:28:27.855 --> 00:28:29.685
We're gonna be focusing mostly on the self

471
00:28:29.685 --> 00:28:30.885
attention and layer norm.

472
00:28:31.825 --> 00:28:34.445
Um, and then feed forward will be in a couple of examples,

473
00:28:34.445 --> 00:28:36.165
but we're not gonna touch on that explicitly.

474
00:28:38.385 --> 00:28:39.685
Um, and then after

475
00:28:39.685 --> 00:28:41.645
that we're gonna talk about decoder layers,

476
00:28:42.375 --> 00:28:45.665
which is this half over here, right before the output.

477
00:28:47.605 --> 00:28:50.625
Um, it has some similar things going on.

478
00:28:51.005 --> 00:28:55.065
Uh, you'll recognize a few words like layer, norm, uh,

479
00:28:55.065 --> 00:28:56.825
and self attention and feed forward.

480
00:28:57.575 --> 00:29:00.025
This is different though, encoder decoder attention.

481
00:29:00.165 --> 00:29:02.305
We are gonna talk about that at length.

482
00:29:04.935 --> 00:29:06.995
Um, but just in general,

483
00:29:07.625 --> 00:29:10.715
that additional layer helps the coder focus on the relevant

484
00:29:10.715 --> 00:29:13.155
parts of the input sentence, ensuring

485
00:29:13.155 --> 00:29:14.915
that the output is contextually

486
00:29:15.335 --> 00:29:17.515
and semantically aligned with the input.

487
00:29:20.685 --> 00:29:23.065
Um, and then exploring residual connection.

488
00:29:23.065 --> 00:29:26.425
Briefly, it's worth noting that the inclusion

489
00:29:26.565 --> 00:29:28.425
of residual connections within these

490
00:29:28.425 --> 00:29:29.985
layers is really important.

491
00:29:32.275 --> 00:29:34.855
Um, these connections allow the gradients to flow

492
00:29:34.855 --> 00:29:38.615
through the network more easily by bypassing certain layers,

493
00:29:38.835 --> 00:29:42.445
and that aids in training deeper, more robust models.

494
00:29:44.325 --> 00:29:47.025
And as we step into the intricacies of each

495
00:29:47.025 --> 00:29:50.385
of these layers in subsequent slides, just remember

496
00:29:50.855 --> 00:29:53.625
that the strategic construction of these layers is

497
00:29:53.695 --> 00:29:56.945
what empowers transformer models, uh,

498
00:29:56.945 --> 00:29:59.865
to effectively manage sequential data

499
00:30:00.405 --> 00:30:01.705
in natural language processing.

500
00:30:06.595 --> 00:30:08.735
Alright, embeddings.

501
00:30:12.055 --> 00:30:16.725
So, um, before we get into the, um,

502
00:30:17.485 --> 00:30:20.125
encoders, uh, let's talk about embeddings, which is usually,

503
00:30:20.305 --> 00:30:21.725
uh, the entry point.

504
00:30:23.895 --> 00:30:27.475
So when we wanna start using the transformer models,

505
00:30:28.555 --> 00:30:31.975
we create these embeddings, these word embeddings.

506
00:30:32.835 --> 00:30:37.495
Um, it allows the computer and the models to understand

507
00:30:38.035 --> 00:30:40.215
and process natural language data.

508
00:30:41.345 --> 00:30:43.215
These embeddings are achieved

509
00:30:43.215 --> 00:30:46.895
through algorithms like we talked about in NLP one, NLP two,

510
00:30:47.475 --> 00:30:49.495
uh, like word vec and glove.

511
00:30:50.195 --> 00:30:53.535
Uh, and these transformed each word into a high dimensional

512
00:30:53.585 --> 00:30:57.895
space where semantic similarities between words correlate

513
00:30:58.125 --> 00:30:59.375
with spatial proximity.

514
00:31:01.685 --> 00:31:06.145
The key in this is converting linguistic information

515
00:31:06.975 --> 00:31:08.545
into a format that's computable,

516
00:31:08.875 --> 00:31:11.065
while maintaining semantic relationships.

517
00:31:15.935 --> 00:31:19.315
Um, and focusing on the bottommost encoder,

518
00:31:21.825 --> 00:31:25.055
we're gonna be looking at this embedding process happening

519
00:31:25.965 --> 00:31:28.095
only in the bottommost encoder.

520
00:31:28.275 --> 00:31:31.655
Uh, so despite all encoders sharing a common structural

521
00:31:31.675 --> 00:31:34.815
design, it's this initial encoder

522
00:31:35.245 --> 00:31:37.135
that ingests the original output

523
00:31:37.715 --> 00:31:40.615
and translates their words into vector representations.

524
00:31:43.775 --> 00:31:46.155
And then another thing is gonna be uniformity

525
00:31:46.155 --> 00:31:47.395
across encoder blocks.

526
00:31:48.095 --> 00:31:52.845
Uh, every single encoder is going to receive a list

527
00:31:52.845 --> 00:31:56.015
of vectors, each of size, n uh,

528
00:31:56.535 --> 00:31:57.655
representing the input words.

529
00:31:57.835 --> 00:32:00.415
So that n is gonna be flexible depending on the input words.

530
00:32:02.445 --> 00:32:06.905
Um, so that uniformity or max input words I should say.

531
00:32:07.405 --> 00:32:10.745
Um, and then the overall uniformity is where each

532
00:32:11.335 --> 00:32:14.705
encoder block processes the input and the shape

533
00:32:14.885 --> 00:32:17.625
and the type, and that allows the model to be scalable

534
00:32:18.325 --> 00:32:20.345
and manage various que uh,

535
00:32:20.625 --> 00:32:21.705
sequence lengths and complexities.

536
00:32:21.705 --> 00:32:23.905
We're gonna talk a little bit about pads, um,

537
00:32:23.905 --> 00:32:26.265
and how that helps with, uh, managing sequence.

538
00:32:28.785 --> 00:32:33.445
Um, and going forward, just think about how embedding

539
00:32:33.445 --> 00:32:37.805
and initial encoding lay the foundation for processing steps

540
00:32:38.625 --> 00:32:41.125
in the transformer models that we're gonna be talking about.

541
00:32:42.195 --> 00:32:45.685
Okay, so today we are talking about transformers,

542
00:32:47.455 --> 00:32:51.355
and transformers have encoder steps

543
00:32:51.895 --> 00:32:53.035
and decoder steps.

544
00:32:53.995 --> 00:32:56.535
Before we get to encoders,

545
00:32:57.875 --> 00:33:01.615
we have the embedding, and that is the input to the encoder.

546
00:33:01.995 --> 00:33:05.255
And then the output, uh, is going to go through lots

547
00:33:05.255 --> 00:33:08.495
of decoder phases, and then we're gonna get some theoretical

548
00:33:08.555 --> 00:33:10.375
output that might be a translated sentence

549
00:33:10.375 --> 00:33:11.415
or something like that at the end.

550
00:33:11.675 --> 00:33:13.175
So that's kind of like contextualizing

551
00:33:13.175 --> 00:33:14.455
where we're currently at in the lecture.

552
00:33:18.145 --> 00:33:19.445
All right, next slide.

553
00:33:21.895 --> 00:33:23.375
Positional encoding.

554
00:33:26.965 --> 00:33:28.505
Uh, so let's talk about

555
00:33:29.425 --> 00:33:32.265
a little bit about positional encoding an important step

556
00:33:32.695 --> 00:33:34.465
that helps us kind of model

557
00:33:34.765 --> 00:33:38.745
and understand where each word is in a sentence.

558
00:33:40.415 --> 00:33:43.875
Um, like we saw in the previous slide with, uh,

559
00:33:44.055 --> 00:33:48.235
the language translations, it's gonna be really key

560
00:33:48.695 --> 00:33:51.315
to remember just the context of the word,

561
00:33:51.775 --> 00:33:53.515
not necessarily the order

562
00:33:56.255 --> 00:33:57.435
or sorry, and the order as well.

563
00:33:59.045 --> 00:34:03.025
Um, so the cat chases the dog, for example,

564
00:34:03.875 --> 00:34:06.985
means something very different, opposite, in fact,

565
00:34:07.335 --> 00:34:09.145
than the dog chases the cat.

566
00:34:10.435 --> 00:34:14.015
So we need to use this positional encoding to make sure

567
00:34:14.015 --> 00:34:16.815
that the model is aware of word order,

568
00:34:16.815 --> 00:34:19.615
otherwise there's gonna be he hallucination in word salad,

569
00:34:19.675 --> 00:34:21.135
and, uh, issues like that.

570
00:34:22.785 --> 00:34:27.245
So positional encoding involves adding special vectors

571
00:34:27.555 --> 00:34:30.935
that you can see here, these vectors, um,

572
00:34:31.115 --> 00:34:33.775
or lists of numbers to the word embeddings.

573
00:34:34.605 --> 00:34:36.465
So we start with these word embeddings,

574
00:34:36.845 --> 00:34:39.745
and then we add the positional encoding here.

575
00:34:40.445 --> 00:34:44.185
And then this gives us a more accurate contextual vector

576
00:34:44.575 --> 00:34:47.785
that has positions and the word embedding.

577
00:34:52.795 --> 00:34:56.695
Um, and adding that allows our models

578
00:34:56.835 --> 00:34:59.535
to get extra help, understanding word order

579
00:34:59.595 --> 00:35:00.815
and creating outputs

580
00:35:00.815 --> 00:35:04.375
that make sense in a natural language processing algorithm.

581
00:35:05.995 --> 00:35:10.765
Um, and in

582
00:35:10.785 --> 00:35:14.435
the next few slides, we're gonna be moving on

583
00:35:14.895 --> 00:35:19.035
and talking about how transformer models use this

584
00:35:19.635 --> 00:35:23.755
positional encoding to make models handle this

585
00:35:24.275 --> 00:35:25.635
language in specific ways.

586
00:35:31.345 --> 00:35:34.325
All right, so there is some math on this slide.

587
00:35:35.285 --> 00:35:37.035
We're not going to do any proofs today.

588
00:35:37.705 --> 00:35:39.685
We are just going to talk about it very briefly.

589
00:35:40.305 --> 00:35:43.185
And the important thing to remember is

590
00:35:45.085 --> 00:35:49.105
this is kind of how it works here.

591
00:35:49.805 --> 00:35:52.305
So we have this positional encoding, um,

592
00:35:52.605 --> 00:35:54.265
and we have the position here.

593
00:35:55.045 --> 00:35:58.505
Here's the input, uh, text we have, hello,

594
00:35:59.125 --> 00:36:00.385
my five word sequence.

595
00:36:01.445 --> 00:36:03.145
And then we just use sign

596
00:36:03.145 --> 00:36:07.905
and cosign as a simple coding function, uh, to have, uh,

597
00:36:08.145 --> 00:36:09.145
positional on coatings.

598
00:36:09.485 --> 00:36:12.105
So sign is going to be all of the even cells.

599
00:36:13.045 --> 00:36:16.385
So like even cells, we start at zero.

600
00:36:16.745 --> 00:36:20.215
'cause this is based in code.

601
00:36:20.795 --> 00:36:23.015
Uh, so this is going to be sign.

602
00:36:23.595 --> 00:36:26.775
So this is gonna be even, even even.

603
00:36:27.595 --> 00:36:30.535
And then co-sign is going to be all of the odd cells.

604
00:36:31.275 --> 00:36:33.725
So we have co-sign, co-sign,

605
00:36:34.735 --> 00:36:36.675
and then that allows us a little bit

606
00:36:36.675 --> 00:36:37.675
of mathematical mapping.

607
00:36:40.095 --> 00:36:42.395
Uh, looks like there's some questions coming in.

608
00:36:42.485 --> 00:36:44.835
Thank you to the TA for answering those.

609
00:36:48.865 --> 00:36:52.945
Um, and this is mostly on this slide, just kind

610
00:36:52.945 --> 00:36:57.185
of a peak at the math, um, of positional embedding.

611
00:36:57.985 --> 00:36:58.925
So let's keep rolling.

612
00:37:08.585 --> 00:37:11.435
Alright, looking at the input for transformers.

613
00:37:12.435 --> 00:37:15.735
Um, diving in a little bit deeper in our transformer models,

614
00:37:16.595 --> 00:37:19.975
the input is actually a three dimensional structure here,

615
00:37:20.835 --> 00:37:24.535
and we can describe it like B,

616
00:37:26.385 --> 00:37:31.185
L, and E, where B is the batch size,

617
00:37:32.045 --> 00:37:34.425
and that's the number of samples in the batch.

618
00:37:38.305 --> 00:37:43.045
Bigger batch sizes mean we're giving the data more data at,

619
00:37:43.065 --> 00:37:44.525
or model more data at once.

620
00:37:45.465 --> 00:37:47.555
So that's gonna be computational load,

621
00:37:47.655 --> 00:37:48.995
but also additional context.

622
00:37:50.185 --> 00:37:51.365
Uh, pros and cons there.

623
00:37:52.645 --> 00:37:55.195
Sequence length is l number

624
00:37:55.215 --> 00:37:57.875
of words slash token in a sequence.

625
00:37:59.435 --> 00:38:02.055
Uh, so like if I is 10, uh,

626
00:38:02.085 --> 00:38:05.015
then our sample batch has 10 words, for example.

627
00:38:06.955 --> 00:38:09.175
Um, e is embedding size.

628
00:38:10.975 --> 00:38:14.635
So this is how long our embedding vector is

629
00:38:14.855 --> 00:38:16.115
for each word or token.

630
00:38:16.535 --> 00:38:18.475
So like, if e is 300 word

631
00:38:18.495 --> 00:38:22.555
or 300, that means that each word is represented by a list

632
00:38:22.655 --> 00:38:23.875
of 300 numbers.

633
00:38:27.765 --> 00:38:31.945
And looking at it overall, our output is A 3D structure

634
00:38:33.095 --> 00:38:36.535
of B, L, and E where we define

635
00:38:36.595 --> 00:38:38.455
how many samples we put into the input.

636
00:38:38.855 --> 00:38:42.175
B, how many words are in each sample L

637
00:38:42.875 --> 00:38:46.935
and how many numbers are used to represent each word.

638
00:38:47.535 --> 00:38:51.735
E. And understanding these input dimensions is gonna be key

639
00:38:51.875 --> 00:38:53.815
as we move forward

640
00:38:54.475 --> 00:38:56.375
and explore how these inputs are

641
00:38:56.375 --> 00:38:57.815
processed by the transformer.

642
00:39:05.275 --> 00:39:08.095
Uh, what is B, l and E in the figure?

643
00:39:10.545 --> 00:39:13.645
So looking at this figure, uh, let's see,

644
00:39:16.245 --> 00:39:18.435
we've got B as batch size.

645
00:39:20.535 --> 00:39:23.595
Uh, so number of samples in that batch.

646
00:39:24.495 --> 00:39:26.995
So that's going to be this five here.

647
00:39:28.685 --> 00:39:32.065
Um, so that is going to be the length.

648
00:39:34.415 --> 00:39:38.035
And then four is the number of tokens in a sequence.

649
00:39:38.855 --> 00:39:43.685
So that's gonna be l that's going to be, sorry, let me

650
00:39:45.125 --> 00:39:47.105
do a different color there.

651
00:39:48.195 --> 00:39:50.535
So this is gonna be l that's gonna be the width,

652
00:39:53.355 --> 00:39:57.475
and then e is going to be the embedding size.

653
00:39:57.575 --> 00:40:00.155
So that's gonna be three, and that's gonna be the depth.

654
00:40:02.245 --> 00:40:03.105
How does that answer your

655
00:40:07.945 --> 00:40:07.985
question?

656
00:40:07.985 --> 00:40:12.785
Uh, answer live. All right. Cool.

657
00:40:13.555 --> 00:40:14.585
Great question so far.

658
00:40:21.135 --> 00:40:24.675
All right, here we go. We have all of the context we need.

659
00:40:25.375 --> 00:40:29.475
Uh, let us zoom in and talk about ENC coders.

660
00:40:37.255 --> 00:40:39.275
So you'll recognize this slide.

661
00:40:42.275 --> 00:40:46.695
This we've given context to, uh, embeddings.

662
00:40:46.695 --> 00:40:48.655
We've given context to positional embeddings.

663
00:40:48.655 --> 00:40:53.605
We've given context to, we are now here specifically,

664
00:40:55.925 --> 00:40:58.155
we are here.

665
00:41:00.205 --> 00:41:02.775
Self attention is what we're gonna be talking about next.

666
00:41:04.725 --> 00:41:07.025
So, self attention is a mechanism

667
00:41:07.735 --> 00:41:12.025
that allows neural networks to weigh the importance

668
00:41:12.125 --> 00:41:16.025
of differing elements in a sequence relative to each other.

669
00:41:16.855 --> 00:41:21.025
It's been particularly influential, influential in NLP,

670
00:41:21.405 --> 00:41:24.865
and it's a key component of transformers in general,

671
00:41:25.345 --> 00:41:27.305
specifically the encoders that we're talking about now.

672
00:41:28.765 --> 00:41:30.265
Um, attention mechanisms.

673
00:41:30.595 --> 00:41:33.510
Mechanisms were initially developed to kind

674
00:41:33.510 --> 00:41:37.485
of like mimic the human ability to focus on a specific part

675
00:41:37.485 --> 00:41:39.845
of a scene or a sequence when processing

676
00:41:39.845 --> 00:41:41.125
information, which is pretty cool.

677
00:41:42.065 --> 00:41:44.365
Um, in the context of neural networks,

678
00:41:44.365 --> 00:41:46.885
attention mechanisms enable models

679
00:41:46.985 --> 00:41:49.125
to selectively focus on different parts

680
00:41:49.265 --> 00:41:53.015
of the input sequence When making predictions, uh,

681
00:41:53.495 --> 00:41:55.895
specifically the self detention mechanism, um,

682
00:41:56.555 --> 00:41:59.255
allows a model to consider the relationships

683
00:41:59.255 --> 00:42:02.375
between different elements within the same sequence.

684
00:42:03.475 --> 00:42:08.345
In the context of NLP, these elements are usually words

685
00:42:09.205 --> 00:42:10.865
or tokens in a sequence.

686
00:42:11.485 --> 00:42:15.585
And the key idea here is that each element can attend

687
00:42:16.285 --> 00:42:19.465
to all other elements in a sequence including itself,

688
00:42:19.555 --> 00:42:21.825
kinda like a correlation matrix, similar to

689
00:42:21.825 --> 00:42:23.425
what you're gonna see in the next couple slides

690
00:42:24.665 --> 00:42:26.645
and some benefits, uh,

691
00:42:26.645 --> 00:42:28.645
we can capture long range dependencies.

692
00:42:28.645 --> 00:42:30.405
Things like RNs struggle

693
00:42:30.675 --> 00:42:32.605
with capturing long range dependencies.

694
00:42:33.145 --> 00:42:36.645
Um, self attention on the other hand, allows the model

695
00:42:36.645 --> 00:42:39.325
to consider all elements in a sequence simultaneously.

696
00:42:39.625 --> 00:42:41.605
So it's gonna be really helpful for those translation

697
00:42:41.865 --> 00:42:43.765
and, uh, switching those words around, uh,

698
00:42:43.765 --> 00:42:44.805
helping handle that.

699
00:42:45.825 --> 00:42:48.005
Uh, and that makes it a lot more effective for tasks

700
00:42:48.005 --> 00:42:49.125
that require understanding,

701
00:42:49.125 --> 00:42:51.245
like the global concept or context.

702
00:42:52.665 --> 00:42:55.885
Uh, paralyzation, like I was saying before, very important.

703
00:42:56.265 --> 00:43:00.325
Um, and self attention methodology is very, um,

704
00:43:00.855 --> 00:43:03.285
plays well with paralyzation, uh,

705
00:43:03.285 --> 00:43:07.325
and that allows for more efficient processing of sequences.

706
00:43:08.635 --> 00:43:11.815
Um, this is in contrast to recurrent neural networks, uh,

707
00:43:11.865 --> 00:43:15.295
which are usually very, very sequential.

708
00:43:21.385 --> 00:43:23.125
All right, here's the first practical example.

709
00:43:24.345 --> 00:43:28.965
Um, so self attention is often commonly referred to

710
00:43:29.265 --> 00:43:30.285
as intra attention.

711
00:43:31.425 --> 00:43:33.665
I think self attention makes a little bit more sense when

712
00:43:33.665 --> 00:43:36.065
you're communicating it personally.

713
00:43:36.685 --> 00:43:41.465
Uh, so here we've got a couple of sentences.

714
00:43:42.205 --> 00:43:46.395
Um, the monkey ate that banana

715
00:43:46.905 --> 00:43:48.915
because it was hungry to,

716
00:43:50.295 --> 00:43:55.075
and what we're going to ask here is, um,

717
00:43:56.925 --> 00:43:58.975
what is it referring to?

718
00:44:00.515 --> 00:44:03.695
Is it the banana or is it the monkey?

719
00:44:04.875 --> 00:44:07.735
And enriching the context of each token

720
00:44:08.565 --> 00:44:10.615
will really improve our results,

721
00:44:12.255 --> 00:44:14.555
and that is a, a visual demonstration of

722
00:44:15.715 --> 00:44:17.175
the use of self attention.

723
00:44:19.825 --> 00:44:24.255
And here's another one. Um, so we're looking at

724
00:44:24.875 --> 00:44:28.305
the cat here, and

725
00:44:28.885 --> 00:44:29.945
we have two examples.

726
00:44:30.445 --> 00:44:32.525
The cat drank the milk because it was hungry.

727
00:44:33.075 --> 00:44:36.575
This is pretty clearly talking about the cat.

728
00:44:38.125 --> 00:44:40.225
Uh, the cat drank the milk because it was sweet.

729
00:44:41.425 --> 00:44:42.835
Very similar sentence structure,

730
00:44:44.075 --> 00:44:48.055
but it is referencing the milk, not the cat.

731
00:44:50.085 --> 00:44:52.945
So how do we get the model to determine which one is which

732
00:44:53.405 --> 00:44:54.585
and where it's representing.

733
00:44:55.205 --> 00:44:57.625
Um, another example would be like in job title,

734
00:44:57.655 --> 00:45:00.875
natural language processing, um,

735
00:45:01.815 --> 00:45:04.475
and job title, natural language processing.

736
00:45:04.475 --> 00:45:07.495
You can consider a picker working

737
00:45:07.595 --> 00:45:10.335
for Amazon would have a very different job

738
00:45:10.925 --> 00:45:15.615
than a picker working for, um, a farming company.

739
00:45:16.355 --> 00:45:18.895
Um, so that is going to be, um,

740
00:45:19.045 --> 00:45:20.935
very different depending on context,

741
00:45:23.845 --> 00:45:25.625
and we can understand theoretically.

742
00:45:26.005 --> 00:45:28.345
Um, but in the next couple of slides, we're going

743
00:45:28.345 --> 00:45:29.945
to be looking at it mathematically.

744
00:45:40.995 --> 00:45:44.095
So calculating self attention involves lots

745
00:45:44.095 --> 00:45:46.855
of different steps, and it's commonly used within

746
00:45:47.715 --> 00:45:50.295
the context of the transformer architecture

747
00:45:50.295 --> 00:45:51.375
like we're talking about now.

748
00:45:53.045 --> 00:45:54.665
Um, the main

749
00:45:56.275 --> 00:46:00.585
organizing concepts are going to be the key, uh, query

750
00:46:00.725 --> 00:46:02.425
and value vectors.

751
00:46:03.325 --> 00:46:04.785
So here we have query vectors.

752
00:46:04.785 --> 00:46:07.385
Here we have key vectors, and here we have value vectors.

753
00:46:09.035 --> 00:46:12.125
Um, all of these vectors are going to be,

754
00:46:12.185 --> 00:46:13.925
the embedding vectors are gonna be created

755
00:46:13.985 --> 00:46:18.665
by taking the dot product of the embedding of each word,

756
00:46:20.755 --> 00:46:22.935
um, that we learned during the training process

757
00:46:23.125 --> 00:46:25.575
that the model learned during the training process.

758
00:46:27.045 --> 00:46:28.945
Um, and for each element,

759
00:46:30.515 --> 00:46:33.645
they're derived from the original input embeddings.

760
00:46:34.545 --> 00:46:38.285
So the attention score between the query vector

761
00:46:38.665 --> 00:46:42.405
and a key vector is calculated using that dot product

762
00:46:42.585 --> 00:46:45.605
for each element I in the attention score,

763
00:46:48.035 --> 00:46:49.855
uh, scaling the attention scores

764
00:46:50.115 --> 00:46:52.295
to prevent the gradients from becoming too small

765
00:46:52.395 --> 00:46:55.775
during the training, the attention scores are usually scaled

766
00:46:55.995 --> 00:46:59.295
by the square root of the dimension of the key vectors.

767
00:46:59.295 --> 00:47:01.895
So that's gonna be kind of like a moderating denominator

768
00:47:01.895 --> 00:47:03.215
that we're gonna look at in a little bit.

769
00:47:03.975 --> 00:47:07.875
Um, we're also gonna be using the soft max activation, um,

770
00:47:08.415 --> 00:47:11.715
to, uh, normalize the weights even further.

771
00:47:12.415 --> 00:47:15.635
And that ensures that the attention weights will sum to one,

772
00:47:16.135 --> 00:47:18.555
uh, for each element in the sequence, which, you know,

773
00:47:18.555 --> 00:47:22.635
probabilistically probabilistically something to one, um,

774
00:47:22.695 --> 00:47:24.075
is going to be important as well.

775
00:47:24.095 --> 00:47:27.515
So we ensure we have an output each time, um,

776
00:47:27.665 --> 00:47:29.900
weighting the sum of, of each of the values.

777
00:47:30.905 --> 00:47:34.405
The final step of this process involves taking a weighted

778
00:47:34.465 --> 00:47:39.435
sum of each of these value vectors,

779
00:47:40.365 --> 00:47:42.635
using the calculated attention weights.

780
00:47:43.215 --> 00:47:45.795
Uh, and that results in context vector

781
00:47:45.795 --> 00:47:50.195
for each element is a combination of all of those values

782
00:47:50.295 --> 00:47:51.355
and all those elements,

783
00:47:53.865 --> 00:47:57.085
and all of those steps are performed independently

784
00:47:58.255 --> 00:48:00.275
for each element in the sequence.

785
00:48:01.015 --> 00:48:05.445
And that results in a whole new set of context vectors, uh,

786
00:48:05.445 --> 00:48:10.405
the process allows the model to attend different parts, uh,

787
00:48:10.705 --> 00:48:12.085
for each sequence in the element,

788
00:48:12.345 --> 00:48:13.765
and that captures the context

789
00:48:13.765 --> 00:48:15.285
information really effectively.

790
00:48:17.115 --> 00:48:18.815
Uh, lemme pull up the qa.

791
00:48:22.035 --> 00:48:25.135
Um, it's important to note that the context

792
00:48:25.315 --> 00:48:28.055
of the transformer architecture, um,

793
00:48:28.865 --> 00:48:32.095
these computations are typically implemented using

794
00:48:32.545 --> 00:48:36.535
operations for efficiency during training and interference.

795
00:48:37.315 --> 00:48:39.255
Uh, the self attention mechanism contributes

796
00:48:39.395 --> 00:48:42.975
to this model ability to capture dependencies

797
00:48:43.075 --> 00:48:44.815
and relationships in the input sequence.

798
00:48:47.555 --> 00:48:51.855
Um, got some shout outs from Benjamin. Uh, thank you.

799
00:48:52.035 --> 00:48:53.975
I'm assuming we're talking about the TA

800
00:48:55.115 --> 00:48:59.335
and, um, appreciate the, uh,

801
00:48:59.625 --> 00:49:00.695
stoke on the class.

802
00:49:03.585 --> 00:49:08.245
Um, all right, diving a little bit deeper into

803
00:49:08.795 --> 00:49:13.005
self attention, I know this is scary, don't worry.

804
00:49:13.425 --> 00:49:14.445
I'm gonna go through it with you.

805
00:49:14.575 --> 00:49:15.605
We'll get through it together.

806
00:49:18.285 --> 00:49:21.345
Um, so starting with our words,

807
00:49:22.325 --> 00:49:25.625
and for each one we're gonna calculate three things.

808
00:49:26.075 --> 00:49:30.005
We're gonna calculate query, which is the query,

809
00:49:30.425 --> 00:49:31.525
uh, process here.

810
00:49:32.935 --> 00:49:36.465
Uh, and then we're gonna calculate key vector,

811
00:49:38.195 --> 00:49:41.605
then we're gonna calculate, uh, the value vector.

812
00:49:44.605 --> 00:49:46.305
And with those powers combined,

813
00:49:46.305 --> 00:49:49.645
we're gonna get a strong output vector.

814
00:49:50.725 --> 00:49:53.505
Um, so getting it step by step,

815
00:49:59.425 --> 00:50:00.565
we need to figure out

816
00:50:00.625 --> 00:50:03.365
how much each word should pay attention to every other word.

817
00:50:03.705 --> 00:50:05.445
Uh, remember that correlation vector,

818
00:50:06.665 --> 00:50:08.445
or sorry, correlation matrix concept.

819
00:50:09.695 --> 00:50:11.595
Um, this is our attention score,

820
00:50:12.735 --> 00:50:14.875
and that's kind of like a measure between the similarity

821
00:50:14.895 --> 00:50:17.395
of each words, between each words.

822
00:50:17.535 --> 00:50:21.755
And it's calculated using, um, this soft max formula here,

823
00:50:22.315 --> 00:50:23.995
Z is getting red.

824
00:50:28.135 --> 00:50:31.915
Um, here we're gonna be dividing by the square root of dk,

825
00:50:32.375 --> 00:50:35.315
and that is gonna be that moderating influence that I was,

826
00:50:35.335 --> 00:50:36.675
uh, discussing previously.

827
00:50:38.795 --> 00:50:43.715
Um, and the size of our key vectors is going to, um,

828
00:50:43.785 --> 00:50:45.515
keep our numbers nice and stable.

829
00:50:47.065 --> 00:50:51.285
Um, what happens is we will turn these weights

830
00:50:51.825 --> 00:50:53.405
or turn these scores into weights using

831
00:50:53.425 --> 00:50:54.605
the soft max function.

832
00:50:55.145 --> 00:50:56.885
Um, this makes sure that all the weights

833
00:50:57.065 --> 00:50:58.565
of each word add up to one.

834
00:50:58.665 --> 00:51:00.885
So all of this Z is going to add up to one.

835
00:51:02.835 --> 00:51:04.735
Um, and then that's pretty much it.

836
00:51:04.835 --> 00:51:08.655
So this single vector made up from mixing all

837
00:51:08.915 --> 00:51:13.175
of the sub vectors here, uh, gets passed down

838
00:51:13.175 --> 00:51:14.455
to the next layer of the model.

839
00:51:14.795 --> 00:51:17.575
And then we repeat the process over and over and over again.

840
00:51:17.955 --> 00:51:19.255
And it may sound tricky,

841
00:51:19.715 --> 00:51:23.205
but all we're doing here is helping the model figure out

842
00:51:24.525 --> 00:51:28.015
what to focus on when it sees a bunch of words.

843
00:51:29.465 --> 00:51:32.935
Think of it as helping the model know which words

844
00:51:33.515 --> 00:51:36.735
are best buddies and need to stick together to make sense.

845
00:51:41.345 --> 00:51:42.455
Great questions coming in.

846
00:51:42.455 --> 00:51:44.535
Thank you to the TA for answering. All

847
00:51:49.805 --> 00:51:52.045
right, calculating the attention score here.

848
00:51:52.625 --> 00:51:55.205
So now we're on the final step.

849
00:51:56.415 --> 00:51:58.815
Remember, the scale attention scores are passed

850
00:51:58.815 --> 00:52:00.175
through the soft max function

851
00:52:00.635 --> 00:52:03.095
to obtain normalized attention weights.

852
00:52:04.225 --> 00:52:07.035
This ensures that attention weights some to one

853
00:52:07.135 --> 00:52:08.795
for each element in that sequence,

854
00:52:08.795 --> 00:52:10.675
because remember, this is all probability based

855
00:52:10.675 --> 00:52:12.475
and probability has to add to one.

856
00:52:13.415 --> 00:52:15.515
Um, and the attention score here

857
00:52:16.255 --> 00:52:19.195
is multiplying the dot product of the query

858
00:52:19.415 --> 00:52:22.435
and the key matrix normalized by the square root

859
00:52:22.895 --> 00:52:24.435
of the embedding vector here.

860
00:52:25.095 --> 00:52:26.675
Um, so that's just normalizing so

861
00:52:26.675 --> 00:52:29.235
that we can more easily process things.

862
00:52:30.135 --> 00:52:31.675
We have this vector space,

863
00:52:31.895 --> 00:52:35.355
and when we normalize it, um, it plays a lot nicer with,

864
00:52:35.455 --> 00:52:36.995
um, the model.

865
00:52:38.395 --> 00:52:41.455
So softmax function is used on the resulting matrix

866
00:52:42.115 --> 00:52:43.175
for that final output.

867
00:52:44.495 --> 00:52:45.675
And that's pretty much it.

868
00:52:46.205 --> 00:52:50.835
Great work that is self attention encoding.

869
00:52:52.745 --> 00:52:55.215
We're gonna touch on it again briefly when we, uh,

870
00:52:55.245 --> 00:52:57.135
talk about decoding process,

871
00:52:58.515 --> 00:53:01.915
but for now, let's get into layer

872
00:53:02.745 --> 00:53:06.605
normalization, frequently known as later norm.

873
00:53:07.465 --> 00:53:11.615
Um, so done, done.

874
00:53:12.285 --> 00:53:14.835
Wait, this is done.

875
00:53:15.985 --> 00:53:19.085
Uh, and then self attention. We just finished.

876
00:53:19.085 --> 00:53:22.465
Good job, everybody. And now we're gonna talk about,

877
00:53:25.945 --> 00:53:27.765
so layer normalization aims

878
00:53:27.785 --> 00:53:31.765
to normalize the activations within a layer that helps us,

879
00:53:32.185 --> 00:53:34.045
uh, con or that helps the model

880
00:53:34.725 --> 00:53:36.085
converge faster during training.

881
00:53:37.055 --> 00:53:41.325
Quick summary, um, normalization within a layer

882
00:53:41.895 --> 00:53:45.525
normalizes the values within the layer of a neural network

883
00:53:46.915 --> 00:53:49.845
that is applied independently, uh,

884
00:53:49.985 --> 00:53:53.045
to each neuron activation in each layer.

885
00:53:54.665 --> 00:53:56.685
So some learnable parameters here,

886
00:53:57.015 --> 00:54:01.125
layer normalization introduces learnable parameters, uh,

887
00:54:01.125 --> 00:54:04.925
which are gamma scale and beta shift.

888
00:54:05.845 --> 00:54:08.145
Um, and that allows the model to adapt

889
00:54:08.485 --> 00:54:11.705
and learn the optimal normalization for each neuron.

890
00:54:12.865 --> 00:54:15.085
Um, some benefits of layer normalization.

891
00:54:16.135 --> 00:54:19.475
Um, it's really good for stabilization training, uh,

892
00:54:19.475 --> 00:54:22.115
helps stabilize the process by reducing covariate shift.

893
00:54:22.465 --> 00:54:25.855
Covariant is varying together, um,

894
00:54:26.915 --> 00:54:31.605
and making it easier for the model to converge, uh,

895
00:54:31.625 --> 00:54:35.365
to an answer, reducing sensitivity and initialization.

896
00:54:36.385 --> 00:54:40.085
Um, so it just makes it more robust to departures from, um,

897
00:54:40.385 --> 00:54:41.645
uh, a central tendency.

898
00:54:44.275 --> 00:54:46.895
Um, applications like where we would use this,

899
00:54:47.125 --> 00:54:50.655
it's mostly used, um, it right, right

900
00:54:50.655 --> 00:54:52.135
after the self attention mechanism.

901
00:54:53.075 --> 00:54:56.695
Um, and it's sometimes applied just independently, uh,

902
00:54:56.715 --> 00:55:00.095
to each position in a sequence which enhances the, uh,

903
00:55:00.095 --> 00:55:04.135
our model's ability to handle varying scales and input data.

904
00:55:05.515 --> 00:55:08.975
Uh, lots of times it's used in machine vision, um,

905
00:55:09.065 --> 00:55:10.175
which is pretty cool.

906
00:55:10.515 --> 00:55:12.815
It, so machine vision, we're just talking about pictures.

907
00:55:13.715 --> 00:55:16.255
Um, and layer normalization allows us

908
00:55:16.255 --> 00:55:18.215
to normalize the layers in the image.

909
00:55:18.875 --> 00:55:21.615
So that does things like bring it to a standard format.

910
00:55:22.355 --> 00:55:24.575
So starting with a full, full color image, think

911
00:55:24.575 --> 00:55:25.895
of a big old giraffe picture.

912
00:55:26.115 --> 00:55:28.535
Really pretty lots of, uh, yellows

913
00:55:28.535 --> 00:55:30.135
and browns and, uh, Savannah.

914
00:55:30.955 --> 00:55:34.095
Um, so that brings it, uh, the normalization of

915
00:55:34.095 --> 00:55:36.375
that image allows the model to process it more easily.

916
00:55:36.955 --> 00:55:39.935
So if we start with a big huge rectangle picture,

917
00:55:40.275 --> 00:55:43.175
we might normalize it to a black

918
00:55:43.175 --> 00:55:45.695
and white four by three, uh, picture.

919
00:55:46.035 --> 00:55:49.695
And that allows the model to more easily, uh, represent it

920
00:55:53.675 --> 00:55:56.335
or fit it represent is a less accurate word.

921
00:55:59.665 --> 00:56:04.005
So diving a little bit deeper into layer normalization, um,

922
00:56:04.945 --> 00:56:08.115
in this model, uh,

923
00:56:08.115 --> 00:56:12.315
or in this cut of this model, uh, we're gonna talk about the

924
00:56:12.895 --> 00:56:14.555
add and normalized layer.

925
00:56:15.105 --> 00:56:18.675
It's often referred to as the residual connection followed

926
00:56:18.695 --> 00:56:19.835
by the layer normalization.

927
00:56:20.695 --> 00:56:24.845
Um, it also serves to stabilize the activations

928
00:56:24.985 --> 00:56:27.605
and facilitate the training of deep networks.

929
00:56:28.745 --> 00:56:32.965
Um, a couple of components we can add residual connection

930
00:56:33.185 --> 00:56:35.565
and normalize the layer normalization.

931
00:56:36.225 --> 00:56:38.845
Um, so we have this positional encoding,

932
00:56:41.335 --> 00:56:43.195
um, that we have as an input.

933
00:56:43.575 --> 00:56:45.955
And then here's the excellent positional coding vector.

934
00:56:46.415 --> 00:56:47.795
We have pass through self attention,

935
00:56:47.805 --> 00:56:50.155
which we all are super versed in right now.

936
00:56:50.935 --> 00:56:53.515
Um, and then we have this additional vector

937
00:56:53.825 --> 00:56:55.995
that we pass into layer normalization,

938
00:56:56.375 --> 00:56:57.955
and now we are normalizing.

939
00:57:02.485 --> 00:57:05.615
Um, so

940
00:57:05.835 --> 00:57:09.375
during this process we're going to do that draft picture,

941
00:57:09.635 --> 00:57:11.495
and then we cut into black

942
00:57:11.495 --> 00:57:13.215
and white, which makes it a little bit easier.

943
00:57:13.795 --> 00:57:17.335
And then from there we bring it into another step of, um,

944
00:57:17.835 --> 00:57:20.015
making it to the same aspect ratio.

945
00:57:21.055 --> 00:57:25.635
And that process we're gonna look at a little bit, uh,

946
00:57:25.635 --> 00:57:26.915
later in the practice.

947
00:57:27.095 --> 00:57:28.595
Um, and we're gonna be, uh,

948
00:57:28.765 --> 00:57:30.955
doing some coding demos and things like that.

949
00:57:38.445 --> 00:57:40.425
So, um, like we were saying

950
00:57:40.425 --> 00:57:44.025
before, layer normalization stabilizes the activations in

951
00:57:44.025 --> 00:57:45.545
the neural network and make sure

952
00:57:45.545 --> 00:57:47.145
that they don't reach extremely high

953
00:57:47.405 --> 00:57:49.105
or extremely low values.

954
00:57:50.255 --> 00:57:52.395
Um, and it normalizes the output

955
00:57:52.455 --> 00:57:54.675
of each feature across the dimension.

956
00:57:54.695 --> 00:57:57.235
So it's another encoder layer, um, that

957
00:57:57.785 --> 00:57:59.635
aids in the encoding of this process.

958
00:58:06.765 --> 00:58:08.945
And there's a couple different types of layer normalization.

959
00:58:09.285 --> 00:58:11.905
Uh, predominantly it's gonna be layer normalization

960
00:58:12.085 --> 00:58:13.545
and batch normalization.

961
00:58:14.565 --> 00:58:18.945
Um, so you can just think of it as batch x normalization,

962
00:58:19.435 --> 00:58:21.185
layer y normalization.

963
00:58:22.555 --> 00:58:26.335
So batch, we're gonna be looking at rows, row, row,

964
00:58:26.675 --> 00:58:27.895
row, and then layer.

965
00:58:27.895 --> 00:58:31.335
We're gonna be looking at columns column, column column.

966
00:58:32.625 --> 00:58:34.405
Um, and there's pros and cons of both.

967
00:58:37.285 --> 00:58:39.905
Uh, layer normalization has some advantages which

968
00:58:40.425 --> 00:58:44.145
mitigates the risk of unstable training dynamics, uh,

969
00:58:44.205 --> 00:58:48.065
by maintaining activations in a normalized range.

970
00:58:48.725 --> 00:58:51.745
Uh, this partially alleviates the covariate shift problem,

971
00:58:51.745 --> 00:58:55.025
which remember we were talking about before, before.

972
00:58:55.025 --> 00:58:58.965
Um, and it keeps the distribution of the inputs

973
00:58:59.025 --> 00:59:01.965
to a layer a little bit more stable during training.

974
00:59:03.745 --> 00:59:06.375
Batch normalization becomes a little unstable

975
00:59:06.525 --> 00:59:07.575
with small batches.

976
00:59:08.355 --> 00:59:11.615
Um, and that's gonna be needed when the input batch

977
00:59:11.615 --> 00:59:14.215
normalization are needed, when the inputs are large.

978
00:59:15.075 --> 00:59:18.975
Um, and for example, in like segmentation

979
00:59:19.075 --> 00:59:20.175
and detection tasks.

980
00:59:21.595 --> 00:59:26.385
Um, so different normalization approaches mainly differ in

981
00:59:26.525 --> 00:59:28.345
how, um, the average

982
00:59:28.605 --> 00:59:32.665
and mean estimates for the, um, eq.

983
00:59:40.415 --> 00:59:43.635
And that's the main layer versus batch normalization. So

984
00:59:50.555 --> 00:59:53.015
that's kind of the top level overview

985
00:59:53.635 --> 00:59:55.695
of encoders versus decoders.

986
00:59:57.115 --> 01:00:01.755
So we've got the input process, and then

987
01:00:01.755 --> 01:00:05.965
after that input process, we have the encoders,

988
01:00:06.345 --> 01:00:08.165
the encoders self attention

989
01:00:08.305 --> 01:00:10.765
and layer norm is what we talked about.

990
01:00:11.825 --> 01:00:16.205
And that can iterate many, many, many times, as many times

991
01:00:16.305 --> 01:00:19.445
as we need to get, uh, the most accurate model, um,

992
01:00:20.095 --> 01:00:21.525
while preventing overfitting.

993
01:00:22.105 --> 01:00:25.645
Um, and then after that process, that final context vector,

994
01:00:25.645 --> 01:00:29.605
which is the output of the encoding process, we pass that

995
01:00:30.065 --> 01:00:33.085
to the decoder, which we're gonna be talking about now.

996
01:00:45.945 --> 01:00:47.405
Um, Vasan was asking,

997
01:00:47.625 --> 01:00:50.325
can you explain more why there are two input machine

998
01:00:50.625 --> 01:00:55.255
and thinking three slides before, um,

999
01:01:01.045 --> 01:01:02.305
oh yeah, I see what you're talking about.

1000
01:01:02.685 --> 01:01:07.025
Um, so these are passing,

1001
01:01:07.755 --> 01:01:09.145
let's just go back to the slide quickly.

1002
01:01:11.595 --> 01:01:15.095
So this is reflecting how we can pass in parallel.

1003
01:01:15.795 --> 01:01:16.855
Um, so thinking

1004
01:01:16.855 --> 01:01:20.535
and machines are just, um, two different words that

1005
01:01:25.725 --> 01:01:26.565
we're passing through.

1006
01:01:27.785 --> 01:01:32.005
Um, so in this, uh, vector of interest,

1007
01:01:32.375 --> 01:01:35.245
we're going to be talking about thinking,

1008
01:01:36.345 --> 01:01:39.085
and we have the positional encoding for thinking.

1009
01:01:39.625 --> 01:01:43.205
And then we pass that word through the self attention

1010
01:01:43.385 --> 01:01:44.605
and then through layer norm,

1011
01:01:45.065 --> 01:01:46.805
and then all the way through the rest of the model.

1012
01:01:47.545 --> 01:01:50.125
And then we repeat that process, um,

1013
01:01:50.225 --> 01:01:52.445
and repeat is simplified.

1014
01:01:52.785 --> 01:01:56.285
Um, this happens in parallel machines is another word.

1015
01:01:56.865 --> 01:01:59.925
Um, and in parallel we pass that through self attention

1016
01:02:00.145 --> 01:02:02.245
and then layer norm, et cetera, et cetera.

1017
01:02:02.345 --> 01:02:03.925
And then we look at the relationship between those.

1018
01:02:04.305 --> 01:02:05.525
But great question. Sorry,

1019
01:02:05.645 --> 01:02:06.725
I should have clarified that earlier.

1020
01:02:10.705 --> 01:02:10.925
Um,

1021
01:02:17.355 --> 01:02:18.905
Let's keep moving.

1022
01:02:21.335 --> 01:02:24.965
We are on track. Good job, everybody.

1023
01:02:28.615 --> 01:02:31.675
Um, so we've got about 30 minutes to a break just to kind

1024
01:02:31.675 --> 01:02:36.555
of like prepare yourself, uh, getting into decoding.

1025
01:02:36.885 --> 01:02:40.235
Let's set shed some light on the decoder section

1026
01:02:40.655 --> 01:02:42.035
of the transformer model.

1027
01:02:42.545 --> 01:02:45.995
Even though it mirrors the encoder process in lots of ways,

1028
01:02:46.425 --> 01:02:49.595
there's a couple key distinctions that tailor it

1029
01:02:49.595 --> 01:02:52.635
to its specific role in the generation of translation tasks.

1030
01:02:54.625 --> 01:02:58.235
Um, so all of the stuff we talked about,

1031
01:02:59.815 --> 01:03:01.985
that kind of ends right here.

1032
01:03:03.215 --> 01:03:06.955
So the prior slides in the presentation are

1033
01:03:06.975 --> 01:03:08.195
all over here somewhere.

1034
01:03:09.095 --> 01:03:13.725
And now we're focused right here on the decoder.

1035
01:03:14.265 --> 01:03:17.365
So we have the input of the, um,

1036
01:03:18.915 --> 01:03:21.685
embedding the context vector, um,

1037
01:03:22.025 --> 01:03:25.325
and that goes into the decoder.

1038
01:03:27.535 --> 01:03:32.235
Um, so masking the input sequence, the decoder needs to,

1039
01:03:32.415 --> 01:03:34.035
uh, to handle data in a way

1040
01:03:34.035 --> 01:03:38.795
that prevents it from seeing future tokens in a sequence

1041
01:03:38.895 --> 01:03:43.155
during training, which is crucial to ensure

1042
01:03:43.155 --> 01:03:45.475
that it doesn't cheat the training process

1043
01:03:45.975 --> 01:03:48.195
by using information it shouldn't have access to

1044
01:03:49.575 --> 01:03:53.515
the masking technique, um, is used

1045
01:03:53.655 --> 01:03:55.075
to hide feature tokens.

1046
01:03:55.145 --> 01:03:58.435
What is masking? We will find out soon enough.

1047
01:03:58.815 --> 01:04:01.875
Uh, we've got some really interesting demos, um,

1048
01:04:01.885 --> 01:04:04.315
where we're gonna be talking about masking at a deep level.

1049
01:04:06.585 --> 01:04:09.245
Uh, but in general, it ensures that the prediction

1050
01:04:09.265 --> 01:04:11.285
for a word doesn't depend on the

1051
01:04:11.285 --> 01:04:12.445
subsequent words in the sequence.

1052
01:04:12.945 --> 01:04:15.325
Um, in practical terms, it's like making sure

1053
01:04:15.325 --> 01:04:17.005
that the model doesn't peek ahead

1054
01:04:17.555 --> 01:04:20.765
into the sentence when it's trying to predict the next word.

1055
01:04:23.305 --> 01:04:27.315
Um, and what the heck is this, right?

1056
01:04:27.375 --> 01:04:30.355
We didn't talk about this encoder decoder attention.

1057
01:04:31.095 --> 01:04:34.585
We know this. Uh, we know layer norm, uh,

1058
01:04:34.585 --> 01:04:36.825
we don't know encoder decoder attention.

1059
01:04:38.525 --> 01:04:42.625
Um, so while the encoder digests the input sequence,

1060
01:04:43.285 --> 01:04:47.265
the decoder ought to produce an output sequence that can be

1061
01:04:47.325 --> 01:04:49.745
of different lengths and is conditioned on

1062
01:04:49.745 --> 01:04:50.785
the encoder output.

1063
01:04:51.445 --> 01:04:54.465
And the deco encoder decoder attention layer

1064
01:04:55.425 --> 01:04:58.415
helps the decoder focus on relevant parts of the sequence

1065
01:04:58.995 --> 01:05:00.455
no matter what the length is.

1066
01:05:02.015 --> 01:05:06.715
Uh, in simpler terms, it allows each position in the decoder

1067
01:05:07.135 --> 01:05:10.595
to attend over all positions in the input sequence,

1068
01:05:10.615 --> 01:05:12.555
and that's captured in the encoders output.

1069
01:05:13.265 --> 01:05:16.555
This forms a kind of bridge, allowing the decoder

1070
01:05:16.555 --> 01:05:20.885
to consider information, um, from the input sequence

1071
01:05:21.105 --> 01:05:23.605
as it generates each word in the output.

1072
01:05:26.245 --> 01:05:28.265
So in top level summary,

1073
01:05:29.515 --> 01:05:32.545
while our decoder also has the self attention

1074
01:05:32.725 --> 01:05:35.225
and feed forward networks like the encoder process,

1075
01:05:36.285 --> 01:05:40.265
the two distinctive out aspects which are masking

1076
01:05:41.305 --> 01:05:44.925
and encoder decoder attention, uh, adapt it

1077
01:05:45.345 --> 01:05:47.645
for its role in generating coherent

1078
01:05:47.665 --> 01:05:49.685
and contextually relevant sequences,

1079
01:05:49.985 --> 01:05:52.165
taking into account both the proceeding words

1080
01:05:53.025 --> 01:05:54.845
and the encoder insights.

1081
01:05:55.545 --> 01:05:58.685
And as we get deeper into this presentation, uh,

1082
01:05:58.685 --> 01:06:02.815
just remember understanding each component in isolation will

1083
01:06:02.815 --> 01:06:05.615
help us appreciate how they work together

1084
01:06:06.315 --> 01:06:08.735
and make these models so powerful.

1085
01:06:09.085 --> 01:06:10.295
Hashtag better together.

1086
01:06:11.615 --> 01:06:15.875
Um, yeah, let's keep going.

1087
01:06:21.295 --> 01:06:25.105
All right, starting at the top, just like we did before.

1088
01:06:27.205 --> 01:06:31.795
Done, done, done. Same thing.

1089
01:06:31.905 --> 01:06:35.795
Done, done this whole process. Done. Good job, everybody.

1090
01:06:36.295 --> 01:06:39.225
We are now here, specifically

1091
01:06:44.075 --> 01:06:45.685
here, self attention.

1092
01:06:48.125 --> 01:06:51.005
A decoder is the component

1093
01:06:51.105 --> 01:06:53.885
or module playing a crucial role in this sequence

1094
01:06:53.885 --> 01:06:56.005
to sequence language model, like I was saying before.

1095
01:06:57.265 --> 01:07:01.285
Um, and in sequence to sequence tasks, attention mechanism,

1096
01:07:01.685 --> 01:07:06.165
decoders often incorporate this attention mechanism process

1097
01:07:06.625 --> 01:07:09.165
to selectively focus on different parts

1098
01:07:09.505 --> 01:07:12.245
of the input sequence when generating each

1099
01:07:12.245 --> 01:07:13.365
element of the output.

1100
01:07:14.475 --> 01:07:18.215
And this allows the model to capture long range dependencies

1101
01:07:19.485 --> 01:07:23.065
and improve the quality of generated sequences.

1102
01:07:29.285 --> 01:07:30.735
Alright, attention masks.

1103
01:07:33.105 --> 01:07:36.605
Um, we've established transformers,

1104
01:07:36.605 --> 01:07:38.045
like the model we're using right now,

1105
01:07:38.705 --> 01:07:42.325
use self attention mechanisms to weigh the importance

1106
01:07:42.325 --> 01:07:43.685
of different words in a sequence.

1107
01:07:44.385 --> 01:07:47.445
But here's the catch. How do we ensure

1108
01:07:48.385 --> 01:07:49.605
the model doesn't cheat

1109
01:07:50.795 --> 01:07:53.455
and look at words ahead of time that it shouldn't see?

1110
01:07:53.455 --> 01:07:56.735
During training in RNs, recurrent neural networks,

1111
01:07:56.735 --> 01:07:59.455
all we would do is have like a training set and a test set.

1112
01:08:00.115 --> 01:08:02.935
Um, here we have attention masks,

1113
01:08:06.455 --> 01:08:11.115
so we can kick off the sequence looking at special token

1114
01:08:11.505 --> 01:08:14.915
denoted as a starter token here at timestamp

1115
01:08:14.965 --> 01:08:16.205
zero, oh, let me change.

1116
01:08:17.185 --> 01:08:22.085
Here we go. So

1117
01:08:22.085 --> 01:08:24.845
at timestamp zero, when this training algorithm starts,

1118
01:08:25.145 --> 01:08:26.485
we use the starting token.

1119
01:08:28.295 --> 01:08:31.515
At timestamp one, we decide on the next token.

1120
01:08:31.655 --> 01:08:35.995
So during training, um, we know the actual

1121
01:08:36.515 --> 01:08:39.595
expected output, so we attach it to the input sequence.

1122
01:08:40.215 --> 01:08:42.915
For example, if our target phrase is,

1123
01:08:44.285 --> 01:08:47.815
you are welcome, the

1124
01:08:48.955 --> 01:08:52.975
input becomes start, which is you right there.

1125
01:08:55.145 --> 01:08:59.915
Um, but during prediction, we don't have the actual output,

1126
01:09:00.965 --> 01:09:03.435
which is, uh, generated by the model, uh,

1127
01:09:03.555 --> 01:09:04.875
P zero at timestamp zero.

1128
01:09:05.095 --> 01:09:09.715
And the output, um, when we mask will be one

1129
01:09:09.715 --> 01:09:11.075
of these selected mask words.

1130
01:09:12.495 --> 01:09:15.795
Um, at timestamp t where t is the end of the process,

1131
01:09:17.415 --> 01:09:21.475
um, we have these masks, um,

1132
01:09:21.625 --> 01:09:26.155
that the model then can guess to get the final output.

1133
01:09:27.455 --> 01:09:28.715
Uh, so a set of different words.

1134
01:09:28.765 --> 01:09:31.715
We've kind of established the sequence length

1135
01:09:31.715 --> 01:09:33.275
where the sequence is you are welcome.

1136
01:09:34.135 --> 01:09:36.135
Um, so that's sequence length is three,

1137
01:09:37.215 --> 01:09:40.335
but our actual content is T minus one.

1138
01:09:40.535 --> 01:09:43.735
'cause remember, we start at zero, um, tokens long at

1139
01:09:43.735 --> 01:09:44.855
that final timestamp.

1140
01:09:45.835 --> 01:09:50.135
So looking at future tokens from T to the final, um,

1141
01:09:50.635 --> 01:09:53.055
uh, to the final length of the vector, uh,

1142
01:09:53.155 --> 01:09:55.455
we do hide them using masking,

1143
01:09:55.515 --> 01:09:57.695
and that ensures that the model doesn't peak at future

1144
01:09:57.875 --> 01:09:59.255
tokens during the training process.

1145
01:09:59.915 --> 01:10:02.095
And then during the, uh, final prediction process,

1146
01:10:02.195 --> 01:10:04.695
that's when we, um, predict those masks.

1147
01:10:08.285 --> 01:10:10.505
And the next couple slides are gonna be really helpful

1148
01:10:10.685 --> 01:10:12.105
for looking at that process.

1149
01:10:19.515 --> 01:10:22.855
So in the self attention layer, you remember these

1150
01:10:24.505 --> 01:10:27.615
value key query very similar to what we had

1151
01:10:27.615 --> 01:10:30.135
before, value, key query and self attention.

1152
01:10:32.735 --> 01:10:36.635
Um, so the first layer of this self attention process, um,

1153
01:10:36.855 --> 01:10:40.715
we have, um, the decoder, which

1154
01:10:41.355 --> 01:10:44.035
computes its very own query key

1155
01:10:44.095 --> 01:10:47.795
and value vectors using the input it receives

1156
01:10:48.145 --> 01:10:49.995
with the objective of

1157
01:10:50.635 --> 01:10:53.715
allowing each token in the input sequence to consider others

1158
01:10:55.295 --> 01:10:57.895
establishing contextual relationships within

1159
01:10:57.915 --> 01:10:59.015
the sequence itself.

1160
01:11:00.035 --> 01:11:01.455
Uh, remember that correlation

1161
01:11:01.455 --> 01:11:02.815
matrix we were talking about before.

1162
01:11:04.275 --> 01:11:05.895
And then, um,

1163
01:11:06.435 --> 01:11:09.255
the next step is looking at the encoder

1164
01:11:09.255 --> 01:11:10.335
decoder attention layer.

1165
01:11:12.025 --> 01:11:15.605
The second type of attention is encoder decoder attention,

1166
01:11:16.325 --> 01:11:19.845
bridging that connection between the encoders, outputs,

1167
01:11:20.795 --> 01:11:21.835
encoders outputs,

1168
01:11:23.915 --> 01:11:25.975
and the decoders generation process.

1169
01:11:27.425 --> 01:11:29.685
The key in value vectors here

1170
01:11:30.275 --> 01:11:32.565
originate from the encoders final layer,

1171
01:11:33.645 --> 01:11:35.645
bringing them information from the input sequence.

1172
01:11:37.095 --> 01:11:41.055
And while this is happening, the query vector right here,

1173
01:11:42.905 --> 01:11:45.445
um, is sourced from the output

1174
01:11:45.865 --> 01:11:47.645
of the decoder self attention layer.

1175
01:11:53.705 --> 01:11:57.485
Um, and that contains insights from the partially generated

1176
01:11:57.485 --> 01:11:58.525
output sequence.

1177
01:12:00.525 --> 01:12:02.305
Um, so just to make an analogy,

1178
01:12:02.325 --> 01:12:03.545
to make things a little clearer,

1179
01:12:04.015 --> 01:12:07.845
imagine the encoder process here, um,

1180
01:12:08.145 --> 01:12:11.865
has read a nice book, the input sequence

1181
01:12:12.735 --> 01:12:13.755
and summarized it.

1182
01:12:14.775 --> 01:12:17.195
The self attention in the decoder

1183
01:12:17.815 --> 01:12:22.035
is like discussing the summary, the summary, um,

1184
01:12:22.245 --> 01:12:24.115
among various friends like a book club.

1185
01:12:25.975 --> 01:12:30.875
So here we're in the book club, um,

1186
01:12:31.055 --> 01:12:33.835
and looking at each word in the output sequence

1187
01:12:34.135 --> 01:12:35.715
to understand different perspectives.

1188
01:12:36.895 --> 01:12:41.275
The encoder decoder attention is in this analogy.

1189
01:12:41.615 --> 01:12:44.995
Um, it's continuous, continuously referring back

1190
01:12:44.995 --> 01:12:47.955
to the original book while discussing

1191
01:12:48.575 --> 01:12:51.795
and ensuring that the conversation stays relevant

1192
01:12:51.895 --> 01:12:53.515
and accurate to the source material.

1193
01:12:53.515 --> 01:12:55.115
So it's kinda like a referee in this process.

1194
01:12:57.655 --> 01:13:00.235
And the source material, again, is that input sequence.

1195
01:13:01.055 --> 01:13:05.405
So in a nutshell, self attention allows the decoder

1196
01:13:05.405 --> 01:13:07.085
to contextualize the tokens

1197
01:13:07.625 --> 01:13:10.285
within its own generated sequence.

1198
01:13:11.255 --> 01:13:14.795
And in parallel, the encoder decoder attention

1199
01:13:15.375 --> 01:13:18.875
allows this output to remain relevant to the input,

1200
01:13:18.875 --> 01:13:21.555
preserving the connection to the original message.

1201
01:13:22.895 --> 01:13:25.795
And remember how I said in parallel,

1202
01:13:26.925 --> 01:13:28.905
but then this isn't parallel.

1203
01:13:28.905 --> 01:13:33.775
This is sequential. And the reason for that is we have

1204
01:13:33.775 --> 01:13:38.285
to demonstrate this process, um, so that it's

1205
01:13:38.745 --> 01:13:40.365
as easy as possible to understand.

1206
01:13:41.585 --> 01:13:45.845
But each one of these processes

1207
01:13:46.775 --> 01:13:51.075
are happening in parallel at the same time to allow for,

1208
01:13:51.735 --> 01:13:53.595
uh, a more robust process

1209
01:13:54.535 --> 01:13:58.375
and, sorry, uh, faster.

1210
01:14:02.885 --> 01:14:05.025
All right. Remember how we talked about attention masks?

1211
01:14:06.705 --> 01:14:10.685
So this is just kind of a, a very low level demonstration of

1212
01:14:10.685 --> 01:14:12.565
what that attention masking process is.

1213
01:14:14.125 --> 01:14:17.465
Um, so if the expected output of this sentence

1214
01:14:18.905 --> 01:14:23.765
is re esan in Spanish, apologies for the pronunciation,

1215
01:14:24.785 --> 01:14:28.605
um, at times step or at timestamp zero only.

1216
01:14:28.625 --> 01:14:31.765
The first token is passed at this T zero here.

1217
01:14:31.795 --> 01:14:33.765
This is T zero right here.

1218
01:14:35.875 --> 01:14:37.535
Um, and for all the statisticians,

1219
01:14:37.535 --> 01:14:38.815
this does look quite a bit like a

1220
01:14:38.815 --> 01:14:40.095
correlation matrix, doesn't it?

1221
01:14:41.395 --> 01:14:45.175
Um, at the timestamp zero only, that first token is passed.

1222
01:14:45.955 --> 01:14:50.095
And that, looking at this visual here,

1223
01:14:51.275 --> 01:14:53.015
um, this is the first process.

1224
01:14:55.045 --> 01:14:58.715
And then at timestamp T one, this is T zero,

1225
01:14:59.185 --> 01:15:02.875
this is T one, T two, T three.

1226
01:15:04.525 --> 01:15:06.545
Um, so at t one sequence

1227
01:15:06.545 --> 01:15:08.465
with two tokens is pass the decoder,

1228
01:15:08.465 --> 01:15:10.585
and then three tokens, and then four tokens.

1229
01:15:10.585 --> 01:15:15.025
So remember, it's T minus one, T minus one, oh my gosh,

1230
01:15:20.225 --> 01:15:20.985
T minus one.

1231
01:15:23.845 --> 01:15:25.745
Um, and then in the next slide,

1232
01:15:25.745 --> 01:15:26.985
we'll talk about it a little bit further

1233
01:15:28.015 --> 01:15:29.075
in a little bit more detail.

1234
01:15:30.355 --> 01:15:32.295
Um, so that parallel concept,

1235
01:15:33.185 --> 01:15:37.375
using the attention masks with that encoder stage,

1236
01:15:38.235 --> 01:15:40.575
um, just like with the decoder,

1237
01:15:40.875 --> 01:15:44.375
the encoder also uses attention masks, ensuring

1238
01:15:44.405 --> 01:15:47.615
that self attention mechanism operates appropriately

1239
01:15:48.495 --> 01:15:53.135
recognizing and respecting the general structure, um,

1240
01:15:53.275 --> 01:15:54.935
and the intent of the input sequence.

1241
01:15:56.005 --> 01:15:58.985
So looking at like varied length sequence

1242
01:15:59.125 --> 01:16:01.825
and padding, remember we were talking about padding

1243
01:16:01.825 --> 01:16:05.495
before, um, real world scenarios.

1244
01:16:06.035 --> 01:16:09.415
Not all input sequences align to a uniform length.

1245
01:16:10.635 --> 01:16:15.015
So our model necessitates fixed size input, um,

1246
01:16:15.035 --> 01:16:16.615
and that's where we used padding.

1247
01:16:18.255 --> 01:16:21.595
Um, the way I think about it is, if you remember back to,

1248
01:16:21.695 --> 01:16:26.345
um, the, uh, calculus where we have the integral

1249
01:16:26.565 --> 01:16:28.065
and then we have like X squared,

1250
01:16:28.165 --> 01:16:30.625
and then we take the final integral, um,

1251
01:16:30.645 --> 01:16:32.825
and then we get plus constant, right?

1252
01:16:32.935 --> 01:16:34.905
Because we don't know what's after that.

1253
01:16:35.405 --> 01:16:38.785
So I think a padding kind of like this plus constant, um,

1254
01:16:38.885 --> 01:16:43.145
it just allows for extra space, um, for the,

1255
01:16:43.845 --> 01:16:46.945
um, context vector to be processed.

1256
01:16:47.575 --> 01:16:48.905
It's like a placeholder.

1257
01:16:52.205 --> 01:16:55.345
Um, so we do append that special pad tokens, um,

1258
01:16:55.965 --> 01:16:59.465
to shorter sequences to match them up, uh,

1259
01:17:00.045 --> 01:17:01.185
to a consistent length.

1260
01:17:01.425 --> 01:17:02.345
'cause that length has to be

1261
01:17:02.345 --> 01:17:03.665
consistent across the whole model.

1262
01:17:05.585 --> 01:17:08.325
Um, but the caveat is we don't want the model

1263
01:17:08.345 --> 01:17:10.845
to pay attention to those padding tokens

1264
01:17:10.845 --> 01:17:13.405
because they're not meaningful words, they're just spaces.

1265
01:17:18.185 --> 01:17:21.885
And with that, um, attention masks kind

1266
01:17:21.885 --> 01:17:23.165
of act like those guides

1267
01:17:23.545 --> 01:17:26.685
and they steer the model's focus towards the tokens

1268
01:17:26.685 --> 01:17:31.205
that carry actual value, um, and away from the pad tokens.

1269
01:17:32.365 --> 01:17:35.305
So here in this sentence we have, you are welcome.

1270
01:17:36.535 --> 01:17:40.275
And then, um, previously in the model we had, uh, a four,

1271
01:17:40.515 --> 01:17:41.595
a four word sentence.

1272
01:17:41.975 --> 01:17:44.075
So we have to now replace that with pad.

1273
01:17:44.535 --> 01:17:47.035
Uh, when we, uh, look at you are welcome vector.

1274
01:17:48.805 --> 01:17:53.585
Um, so this attention masks are going

1275
01:17:53.585 --> 01:17:54.585
to prioritize.

1276
01:17:54.685 --> 01:17:57.625
You are welcome and deprioritize pad.

1277
01:18:00.575 --> 01:18:03.155
Um, and it's kind of just telling us, Hey,

1278
01:18:03.285 --> 01:18:06.035
focus on these tokens and ignore the padding ones

1279
01:18:06.035 --> 01:18:07.875
because they're placeable, there's and not real.

1280
01:18:10.055 --> 01:18:12.795
Um, in mathematical simplicity,

1281
01:18:13.415 --> 01:18:17.235
the attention mask is often the binary sequence kind

1282
01:18:17.235 --> 01:18:20.315
of mirroring, mirroring the length of the input.

1283
01:18:21.485 --> 01:18:26.065
Um, and a signal, um, signals pay attention

1284
01:18:26.125 --> 01:18:30.225
to this token and zero it meaning ignore this one.

1285
01:18:30.365 --> 01:18:31.385
So these are all ones,

1286
01:18:37.405 --> 01:18:38.465
And these are all zeros.

1287
01:18:39.285 --> 01:18:40.835
We're gonna ignore all of these.

1288
01:18:45.335 --> 01:18:48.915
Um, so if we have a sentence like, I love NLP, uh,

1289
01:18:48.915 --> 01:18:52.305
where we assume NLP is one word I

1290
01:18:52.645 --> 01:18:56.065
and the padded sequence becomes, I love NLP

1291
01:18:56.095 --> 01:18:57.505
with a couple different pads.

1292
01:18:58.535 --> 01:19:02.555
Um, here where you have, I love NLP,

1293
01:19:03.845 --> 01:19:05.945
um, and then have the same level of pad.

1294
01:19:07.675 --> 01:19:12.295
Um, and that would have the same, uh,

1295
01:19:12.605 --> 01:19:15.495
ones and then all of the zeros in the pad areas.

1296
01:19:18.165 --> 01:19:21.985
Why is the attention mask three by three, uh, versus

1297
01:19:22.675 --> 01:19:23.945
stare in a previous one?

1298
01:19:24.445 --> 01:19:27.865
Oh, so it's similar to the, um, it's just simplified.

1299
01:19:28.605 --> 01:19:31.625
Um, so this, we have the pad, if you recall.

1300
01:19:32.285 --> 01:19:34.625
Um, so that pads out the whole vector.

1301
01:19:35.205 --> 01:19:36.545
And, uh, this also kind

1302
01:19:36.545 --> 01:19:39.345
of demonstrates the parallel processing as opposed

1303
01:19:39.345 --> 01:19:40.545
to the other, which was, um,

1304
01:19:40.545 --> 01:19:42.145
explained in a more sequential way.

1305
01:19:43.985 --> 01:19:44.805
So, good question.

1306
01:19:49.125 --> 01:19:52.185
Um, and just wrapping this up, attention masks

1307
01:19:52.205 --> 01:19:54.185
and padding together ensure that the

1308
01:19:54.705 --> 01:19:57.865
transformer model processes variable length sequences in a

1309
01:19:58.065 --> 01:20:01.545
constant manner and maintains focus on meaningful tokens

1310
01:20:01.685 --> 01:20:03.625
and, uh, disregarding the placeholders.

1311
01:20:09.895 --> 01:20:13.355
All right. Um, and next we're gonna get into, uh,

1312
01:20:13.705 --> 01:20:15.115
encoder decoder attention.

1313
01:20:16.845 --> 01:20:21.145
Um, I'm looking at the time,

1314
01:20:22.235 --> 01:20:24.705
let's take a quick, uh, 10 minute break.

1315
01:20:25.935 --> 01:20:29.915
So on Pacific time, we would get back at, uh, 10 31.

1316
01:20:30.575 --> 01:20:33.475
So all time zones would reflect 31 minutes

1317
01:20:33.485 --> 01:20:37.685
after, uh, and then reconvene

1318
01:20:38.545 --> 01:20:41.925
and yeah, uh, talk about encoder decoder retention.

1319
01:20:42.625 --> 01:20:47.565
So I'll see you back at now, 22 after, or sorry, 32

1320
01:20:47.565 --> 01:20:48.565
after it's 22 afternoon.

1321
01:20:53.955 --> 01:20:56.935
All right, everybody, let's get back,

1322
01:20:58.515 --> 01:21:03.475
get started, um, on

1323
01:21:04.225 --> 01:21:05.715
encoder decoder attention.

1324
01:21:09.115 --> 01:21:12.855
And feel free to, uh, drop questions into the question

1325
01:21:12.875 --> 01:21:17.385
and answer section as

1326
01:21:18.405 --> 01:21:19.505
we process all

1327
01:21:19.505 --> 01:21:21.585
of the incoming information that we're getting.

1328
01:21:24.815 --> 01:21:26.235
So we talked about it briefly,

1329
01:21:26.655 --> 01:21:28.755
but diving a little bit deeper

1330
01:21:29.345 --> 01:21:31.275
into the internal architecture

1331
01:21:31.495 --> 01:21:33.635
of the encoder decoder relationship.

1332
01:21:35.225 --> 01:21:38.125
Um, each of those plays a pivotal

1333
01:21:39.395 --> 01:21:42.365
role in the functionality of transformers and guides.

1334
01:21:42.365 --> 01:21:43.605
The model through, uh,

1335
01:21:43.785 --> 01:21:46.125
the distinct processing steps that we're gonna have.

1336
01:21:47.685 --> 01:21:50.785
Um, with every encoder there exists,

1337
01:21:50.915 --> 01:21:54.105
those three primary layers that we've discussed about, uh,

1338
01:21:54.685 --> 01:21:58.545
the self attention layer, the layer normalization, um,

1339
01:21:58.845 --> 01:22:01.665
and then those repeated, uh, feed forward as well.

1340
01:22:02.855 --> 01:22:05.835
Um, getting into those decoder layers we've already

1341
01:22:05.945 --> 01:22:09.525
discussed, um, just as review,

1342
01:22:10.135 --> 01:22:11.445
we've still got the layer norm,

1343
01:22:11.455 --> 01:22:13.245
we've still got the feed forward.

1344
01:22:13.785 --> 01:22:17.005
Um, encoder decoder tension, which is new.

1345
01:22:17.795 --> 01:22:19.095
Um, and then self attention.

1346
01:22:22.285 --> 01:22:25.195
Let's discuss encoder decoder attention.

1347
01:22:31.425 --> 01:22:35.645
So keep in mind here, encoder decoder attention

1348
01:22:36.585 --> 01:22:38.605
is also known as attention mechanism,

1349
01:22:39.145 --> 01:22:41.765
but that's often confused with attention masking,

1350
01:22:41.775 --> 01:22:45.125
which is like very, uh, proximal to this discussion.

1351
01:22:45.265 --> 01:22:48.085
So let's use encoder decoder attention terminology.

1352
01:22:49.145 --> 01:22:53.525
Um, it's a key component in this sequence sequence, um,

1353
01:22:54.195 --> 01:22:55.565
thought process and process.

1354
01:22:56.305 --> 01:23:00.565
Um, particularly in the tasks like machine translation, uh,

1355
01:23:00.565 --> 01:23:02.925
like translating English to Spanish or something like that.

1356
01:23:03.465 --> 01:23:06.285
Um, it enables the model to selectively focus

1357
01:23:06.825 --> 01:23:09.925
on different parts of the input sequence when generating

1358
01:23:09.955 --> 01:23:11.685
each element of the output sequence.

1359
01:23:13.145 --> 01:23:16.285
The, the attention mechanism here, um,

1360
01:23:16.515 --> 01:23:19.725
encoder decoder attention enhances the model's ability

1361
01:23:20.065 --> 01:23:23.045
to capture long range dependencies like I was saying before,

1362
01:23:23.265 --> 01:23:27.165
and align the input and output sequences effectively.

1363
01:23:28.845 --> 01:23:31.425
So again, we've discussed this part.

1364
01:23:33.515 --> 01:23:36.775
Um, and here we have their embedding output.

1365
01:23:37.995 --> 01:23:41.015
Um, and we are looking at the decoder,

1366
01:23:42.435 --> 01:23:43.835
specifically this area here.

1367
01:23:47.235 --> 01:23:51.655
Um, so the initial decoder state is initialized

1368
01:23:52.005 --> 01:23:54.695
with this context vector that we get from the out, uh,

1369
01:23:54.795 --> 01:23:55.975
the encoder process.

1370
01:23:57.365 --> 01:24:00.425
Um, often the last hidden state of that encoder.

1371
01:24:00.845 --> 01:24:01.985
Um, and

1372
01:24:02.935 --> 01:24:06.625
that context vector contains information about the entire

1373
01:24:06.795 --> 01:24:08.985
input sequence, so that, um,

1374
01:24:10.545 --> 01:24:14.895
input sequence in both the embedding and the sequence

1375
01:24:14.895 --> 01:24:18.895
and the contextual vector, um, altogether, um,

1376
01:24:18.895 --> 01:24:20.135
during the decoding process.

1377
01:24:20.635 --> 01:24:23.895
The intention mechanism, uh, encoder decoder attention

1378
01:24:24.755 --> 01:24:29.055
allows the decoder to focus on different parts

1379
01:24:29.155 --> 01:24:32.855
of the encoded input sequence dynamically, depending on

1380
01:24:32.855 --> 01:24:35.295
what is relevant for generating the next element

1381
01:24:35.395 --> 01:24:36.535
in the output sequence.

1382
01:24:38.065 --> 01:24:41.135
Um, attention scores are calculated

1383
01:24:41.235 --> 01:24:44.775
to determine the relevance of each encoded input element

1384
01:24:45.515 --> 01:24:47.375
to the current decoding step.

1385
01:24:48.695 --> 01:24:52.035
Um, and so how, good question on sequence length.

1386
01:24:52.455 --> 01:24:53.755
So there are limitations.

1387
01:24:53.815 --> 01:24:56.395
You can't drop the entire ilead into this process.

1388
01:24:56.775 --> 01:25:01.515
Um, but that's mostly gait or limited by overflow errors.

1389
01:25:01.855 --> 01:25:05.315
Um, so when you're generating your model yourself, um, it,

1390
01:25:05.895 --> 01:25:10.865
um, it's quite robust to long,

1391
01:25:11.405 --> 01:25:13.145
uh, vectors, but you can't drop

1392
01:25:13.145 --> 01:25:14.345
the entire internet in there.

1393
01:25:14.565 --> 01:25:16.105
So it's kind of just something that, um,

1394
01:25:16.255 --> 01:25:19.105
your model will have to, uh, process on its own,

1395
01:25:19.445 --> 01:25:21.745
but you can drop quite long vectors in there.

1396
01:25:24.495 --> 01:25:29.355
Um, so for attention scores, the scores are computed based

1397
01:25:30.015 --> 01:25:33.195
on the similarity between the decoders hidden, uh,

1398
01:25:33.195 --> 01:25:36.195
current state and the encoded representations

1399
01:25:36.805 --> 01:25:39.115
using a mechanism like the dot product attention.

1400
01:25:41.355 --> 01:25:45.375
Um, and we're gonna be getting a little bit more in depth,

1401
01:25:45.555 --> 01:25:47.095
uh, looking at some soft max

1402
01:25:47.115 --> 01:25:48.855
and things like that in the next couple slides.

1403
01:25:55.515 --> 01:25:58.735
And, um, here we've got the process

1404
01:25:58.835 --> 01:26:00.135
of encoder decoder attention.

1405
01:26:00.835 --> 01:26:03.135
Um, here we have the key vector.

1406
01:26:03.345 --> 01:26:05.765
We have the query vector, and we have the values vector.

1407
01:26:05.945 --> 01:26:07.605
So very similar to what we had before.

1408
01:26:08.305 --> 01:26:12.085
Uh, the, um, input array is going

1409
01:26:12.085 --> 01:26:14.125
to be I am eating an apple.

1410
01:26:16.115 --> 01:26:21.055
And at this layer only the output of the encoder here

1411
01:26:21.635 --> 01:26:23.575
is used in that decoder.

1412
01:26:25.465 --> 01:26:30.225
Um, and the, what these vectors represent,

1413
01:26:30.845 --> 01:26:33.545
uh, q may represent the query like in a Google

1414
01:26:33.545 --> 01:26:34.705
search or something like that.

1415
01:26:35.595 --> 01:26:39.215
And that, uh, comes directly from the encoder, whereas K

1416
01:26:39.215 --> 01:26:43.375
and V, those are gonna represent the already mapped input

1417
01:26:43.695 --> 01:26:48.455
sequence in a vector representation, uh, from the encoder.

1418
01:26:50.425 --> 01:26:52.565
Um, and that encoder decoder block

1419
01:26:53.845 --> 01:26:56.805
multiplies the takes the dot product, the q

1420
01:26:56.905 --> 01:27:00.845
and the K vector to find the match between the Q

1421
01:27:00.845 --> 01:27:03.165
and the K vector, and then generate the weights

1422
01:27:03.425 --> 01:27:04.645
for each value vector.

1423
01:27:07.395 --> 01:27:10.255
And then looking at the loss function, um, which is

1424
01:27:10.775 --> 01:27:11.935
a calculation in this process.

1425
01:27:13.215 --> 01:27:17.555
Um, the cross entropy loss is commonly used, uh, here,

1426
01:27:18.335 --> 01:27:21.155
uh, lots of natural language processing applications,

1427
01:27:21.495 --> 01:27:24.635
but one of its most popular application is going to be in,

1428
01:27:24.775 --> 01:27:27.115
um, the encoder decode process.

1429
01:27:29.315 --> 01:27:32.375
Um, so given the true distribution,

1430
01:27:34.405 --> 01:27:36.385
um, which is here, um,

1431
01:27:36.525 --> 01:27:41.215
and then the predicted distribution here, um, we,

1432
01:27:41.215 --> 01:27:43.175
we can calculate the cross entropy loss.

1433
01:27:43.235 --> 01:27:45.775
So you can think of this similar to kind of like residuals

1434
01:27:46.315 --> 01:27:49.255
in, um, RNN, uh, recurrent neural network

1435
01:27:49.395 --> 01:27:51.415
and, uh, logistic regression and things like that.

1436
01:27:52.375 --> 01:27:56.395
Um, so for training, um, the aim is

1437
01:27:56.395 --> 01:27:58.075
to minimize cross entropy loss.

1438
01:27:58.695 --> 01:28:01.235
And typically that's done with things like gradient based,

1439
01:28:01.455 --> 01:28:03.355
uh, optimization, um,

1440
01:28:05.075 --> 01:28:06.775
and methods that, uh,

1441
01:28:07.245 --> 01:28:09.295
improve model fit and things like that.

1442
01:28:09.845 --> 01:28:12.935
Adjusting and fine tuning these model parameters to reduce

1443
01:28:13.605 --> 01:28:14.815
this loss function.

1444
01:28:16.515 --> 01:28:19.215
Um, the model learns through sign higher probabilities

1445
01:28:19.275 --> 01:28:20.375
to the correct classes

1446
01:28:20.595 --> 01:28:23.415
or tokens, thus improving its predictions.

1447
01:28:23.595 --> 01:28:26.255
So it can, the model can calculate loss function

1448
01:28:26.275 --> 01:28:28.335
and improve as it iterates.

1449
01:28:29.555 --> 01:28:33.335
Um, it's critical to select an appropriate loss function,

1450
01:28:33.795 --> 01:28:35.495
uh, in accordance with the task that you're,

1451
01:28:35.515 --> 01:28:36.695
uh, trying to accomplish.

1452
01:28:37.315 --> 01:28:42.015
Uh, cross entropy loss is pretty well suited to, uh,

1453
01:28:42.575 --> 01:28:45.455
specifically classification algorithms, uh,

1454
01:28:45.755 --> 01:28:47.055
and generation tasks

1455
01:28:47.545 --> 01:28:50.895
where the outputs are things like probability distributions,

1456
01:28:51.595 --> 01:28:54.255
um, which is super helpful for,

1457
01:28:54.515 --> 01:28:56.135
uh, transformers specifically.

1458
01:28:56.515 --> 01:28:59.615
So we usually do use loss function to, uh,

1459
01:28:59.615 --> 01:29:00.815
measure our model success.

1460
01:29:10.785 --> 01:29:15.735
All right. So just to summarize quickly, um,

1461
01:29:16.005 --> 01:29:20.015
self attention is the key to the transformers,

1462
01:29:20.955 --> 01:29:25.695
um, and it's just simply calculating correlation

1463
01:29:25.695 --> 01:29:29.875
between each token In the input sequences, um,

1464
01:29:30.145 --> 01:29:33.315
self attention has the query key value vectors

1465
01:29:34.645 --> 01:29:38.705
and just as the dot product, uh, to of those vectors

1466
01:29:38.725 --> 01:29:40.625
to generate the self attention scores.

1467
01:29:42.845 --> 01:29:44.705
Um, we need multi-head attention

1468
01:29:45.125 --> 01:29:48.865
to capture different correlations between tokens

1469
01:29:50.085 --> 01:29:53.345
and the, uh, the sequential parallel nature.

1470
01:29:53.525 --> 01:29:56.225
So going from sequential to parallel, um,

1471
01:29:56.405 --> 01:30:00.065
of the transformer, um, really improves model speed

1472
01:30:00.565 --> 01:30:03.025
and allows for additional complexity compared

1473
01:30:03.085 --> 01:30:06.065
to the previously presented, uh, sequence to sequence model.

1474
01:30:07.365 --> 01:30:11.585
Um, there are some limitations of the transformer high comp,

1475
01:30:11.725 --> 01:30:14.625
um, which is like high computational complexity.

1476
01:30:16.145 --> 01:30:19.925
Um, but with the improvements of, uh, cloud computing

1477
01:30:20.025 --> 01:30:22.325
and improvements in computing in general, um,

1478
01:30:22.995 --> 01:30:24.805
they are less problematic than they were

1479
01:30:25.045 --> 01:30:26.205
previously in the past.

1480
01:30:27.435 --> 01:30:29.575
Um, and transformers are used in a ton

1481
01:30:29.575 --> 01:30:34.375
of different applications, um, like in trans, uh,

1482
01:30:34.685 --> 01:30:37.535
translation, uh, English to other languages

1483
01:30:37.675 --> 01:30:39.815
or any language to any other language,

1484
01:30:39.955 --> 01:30:42.015
and also in lots of domains like the computer vision

1485
01:30:42.475 --> 01:30:44.375
and classification and things like that.

1486
01:30:46.135 --> 01:30:48.195
And then, uh, I'm,

1487
01:30:48.375 --> 01:30:50.515
you want me getting these slides afterwards so you can see,

1488
01:30:50.695 --> 01:30:52.555
um, what the, where the references are,

1489
01:30:52.775 --> 01:30:54.235
but these are some great references.

1490
01:30:54.815 --> 01:30:59.435
Um, I also really like Stat Quest, um, on YouTube, uh,

1491
01:30:59.495 --> 01:31:03.435
has really good lectures if you wanna dive deeper onto a

1492
01:31:03.515 --> 01:31:05.515
different topic or things like that.

1493
01:31:06.445 --> 01:31:08.385
Um, really great resources.

1494
01:31:08.895 --> 01:31:11.465
Also, Google Scholar has, uh, lots

1495
01:31:11.465 --> 01:31:13.945
of interesting articles on state-of-the-art

1496
01:31:13.945 --> 01:31:15.105
algorithms and things like that.

1497
01:31:22.765 --> 01:31:25.915
All right, we're going next.

1498
01:31:26.825 --> 01:31:29.015
We're going to talk about Bert.

1499
01:31:30.395 --> 01:31:35.055
Um, so Bert is an acronym,

1500
01:31:35.955 --> 01:31:40.015
um, and it is build, or it's for building

1501
01:31:40.075 --> 01:31:41.775
and training, uh, the state

1502
01:31:41.775 --> 01:31:43.655
of the art natural language processing models.

1503
01:31:44.315 --> 01:31:47.095
Um, and the acronym is Bidirectional

1504
01:31:47.685 --> 01:31:50.335
Encoder Representations from Transformers.

1505
01:32:04.295 --> 01:32:07.325
So Burt Models, um,

1506
01:32:08.825 --> 01:32:12.245
in using Burt models, we can generate text like humans.

1507
01:32:13.305 --> 01:32:16.725
Um, so we know how we can use transformer models

1508
01:32:17.345 --> 01:32:19.005
for learning attention mechanisms.

1509
01:32:19.585 --> 01:32:21.045
And what we need is

1510
01:32:21.045 --> 01:32:25.285
to generate new text using encoder decoder architecture.

1511
01:32:26.815 --> 01:32:29.785
So we have all of the tools

1512
01:32:30.355 --> 01:32:33.225
after we learn Bert in, in our, um,

1513
01:32:35.015 --> 01:32:38.065
tool belts, which will help build a model, uh,

1514
01:32:38.065 --> 01:32:39.865
which will not only generate new text,

1515
01:32:40.045 --> 01:32:42.225
but understand languages like we do as humans.

1516
01:32:53.475 --> 01:32:55.775
So at the end of this section,

1517
01:32:56.755 --> 01:32:59.215
you will not only understand the Burt model,

1518
01:33:00.305 --> 01:33:05.055
but also understand popular Burt architecture

1519
01:33:06.505 --> 01:33:09.565
and lots of different fine tuning, uh,

1520
01:33:10.505 --> 01:33:11.645
for a specific task.

1521
01:33:12.785 --> 01:33:17.245
So just as a top level overview, we have the Bert model, uh,

1522
01:33:17.245 --> 01:33:20.045
which was developed by Google, uh, a few years ago

1523
01:33:20.045 --> 01:33:21.445
through a research paper.

1524
01:33:21.865 --> 01:33:23.605
Um, and it's a pre-trained model.

1525
01:33:24.265 --> 01:33:27.445
Um, and then there have been lots of improvements

1526
01:33:27.445 --> 01:33:28.965
to this model, um,

1527
01:33:29.375 --> 01:33:31.725
using things like Roberta and Electra and Albert.

1528
01:33:32.065 --> 01:33:36.225
Um, and we'll be discussing, um, how to use those,

1529
01:33:36.965 --> 01:33:41.265
um, and also why we would use that, um, model over Bert, um,

1530
01:33:41.285 --> 01:33:43.785
and why people still may use Bert for some examples.

1531
01:33:44.445 --> 01:33:47.305
Um, and we do actually have quite a few code demos.

1532
01:33:47.305 --> 01:33:51.745
We have two books, um, that will be going through that will,

1533
01:33:52.085 --> 01:33:54.625
uh, help kind of explore Bert a little bit.

1534
01:34:02.165 --> 01:34:03.225
All right, let's jump in.

1535
01:34:08.745 --> 01:34:12.725
So we're gonna get started with just a simple representation

1536
01:34:12.865 --> 01:34:14.205
of words and tokens.

1537
01:34:14.505 --> 01:34:15.885
Things like one hop lecture,

1538
01:34:15.885 --> 01:34:19.205
which we've talked about in the previous NLP uh, classes,

1539
01:34:19.985 --> 01:34:24.325
uh, TF IDF word embeddings, um, things like that.

1540
01:34:24.945 --> 01:34:28.525
But solving more complex things like language translation,

1541
01:34:28.965 --> 01:34:32.725
question answering, um, and high level performance.

1542
01:34:33.065 --> 01:34:37.545
Um, we do need model with some capacity

1543
01:34:37.625 --> 01:34:38.785
of language understanding.

1544
01:34:39.005 --> 01:34:41.905
We need syntax and semantics of each language.

1545
01:34:42.045 --> 01:34:45.985
We need context, and we need context, cognitive linguistics,

1546
01:34:46.045 --> 01:34:47.225
and a ton more.

1547
01:34:48.685 --> 01:34:52.065
And, um, the model with these capabilities, um,

1548
01:34:52.215 --> 01:34:53.425
have to be quite large.

1549
01:34:54.485 --> 01:34:57.525
And that brings us to large language models.

1550
01:34:57.935 --> 01:35:00.725
We're finally here. We're finally talking about chat, GPT.

1551
01:35:01.635 --> 01:35:05.415
Uh, so chat GPT is just kind of like a specific instance

1552
01:35:05.715 --> 01:35:07.095
of a large language model.

1553
01:35:07.265 --> 01:35:08.935
There are lots of, um,

1554
01:35:09.125 --> 01:35:10.935
less world renowned large language models

1555
01:35:10.935 --> 01:35:12.375
that we'll be talking about mostly here,

1556
01:35:12.715 --> 01:35:14.895
but mostly of those rules are going to apply to

1557
01:35:15.655 --> 01:35:17.135
whenever your roommates or friends

1558
01:35:17.155 --> 01:35:21.775
or family ask, ask you how LLM works and how she GBT works.

1559
01:35:29.575 --> 01:35:33.635
So transfer learning is a really exciting concept.

1560
01:35:34.535 --> 01:35:37.515
Um, transfer learning is machine learning paradigm where

1561
01:35:38.195 --> 01:35:42.115
a model trained on one task is leveraged

1562
01:35:42.135 --> 01:35:45.435
or adapted, uh, from or for a different

1563
01:35:45.655 --> 01:35:48.595
but kind of related task In a traditional

1564
01:35:48.865 --> 01:35:50.235
machine learning approach.

1565
01:35:51.135 --> 01:35:54.115
Uh, models are trained for a specific task from scratch

1566
01:35:55.085 --> 01:35:58.055
requiring a substantial amount of labeled data to train on.

1567
01:35:59.225 --> 01:36:02.285
Um, transfer learning on the other hand, takes advantage

1568
01:36:02.625 --> 01:36:05.365
of pre-trained models, pre-trained models.

1569
01:36:05.385 --> 01:36:06.645
That's a key concept here.

1570
01:36:07.575 --> 01:36:10.435
Um, and those model pre-trained models are trained on large

1571
01:36:10.435 --> 01:36:13.795
data sets and transfers the knowledge gained from

1572
01:36:13.795 --> 01:36:17.725
that training to perform well on a new

1573
01:36:18.405 --> 01:36:20.725
possibly smaller data set or related task.

1574
01:36:23.555 --> 01:36:26.975
So transfer learning involves two steps.

1575
01:36:27.985 --> 01:36:30.425
First step, pre-training.

1576
01:36:31.525 --> 01:36:33.185
The Bert model does this for us.

1577
01:36:34.085 --> 01:36:37.065
So a model is first trained on a huge data set

1578
01:36:37.495 --> 01:36:40.585
with a specific task, mostly in a supervised manner.

1579
01:36:41.125 --> 01:36:44.905
And that task is usually chosen, uh, to be a general

1580
01:36:45.045 --> 01:36:47.945
or related problem for which the amount

1581
01:36:48.485 --> 01:36:51.585
of labeled data, uh, is available.

1582
01:36:52.925 --> 01:36:55.505
The model learns relevant features

1583
01:36:56.125 --> 01:37:00.465
and patterns from that data, so it's trained really well on

1584
01:37:00.465 --> 01:37:01.505
that pre-training data.

1585
01:37:02.285 --> 01:37:07.025
And then, um, how we can improve it is by fine tuning.

1586
01:37:07.045 --> 01:37:08.505
That's the second step in the process.

1587
01:37:09.095 --> 01:37:10.785
Fine tuning or feature extraction.

1588
01:37:11.445 --> 01:37:14.185
Uh, and the pre-trained model is adapted

1589
01:37:14.405 --> 01:37:18.625
or fine tuned for the target task or the target data set.

1590
01:37:19.575 --> 01:37:22.595
The idea of this process is to kind of leverage the work

1591
01:37:22.595 --> 01:37:25.835
that's already been done for us, um, that knowledge

1592
01:37:25.835 --> 01:37:27.995
that was gained during the pre-training phase

1593
01:37:28.495 --> 01:37:30.275
to enhance the model's performance

1594
01:37:31.135 --> 01:37:33.235
on the specific task of interest.

1595
01:37:34.135 --> 01:37:38.595
And this adaptation can involve training the model on a

1596
01:37:38.595 --> 01:37:41.995
smaller data set related to the target task, um,

1597
01:37:42.135 --> 01:37:44.595
or just adjusting the specific layers of the model.

1598
01:37:48.755 --> 01:37:51.855
Um, there's lots of advantages of transfer learning.

1599
01:37:53.195 --> 01:37:54.775
One is data efficiency.

1600
01:37:56.035 --> 01:37:58.175
So the big lift of pre-training

1601
01:37:58.175 --> 01:37:59.735
that model has already done for you.

1602
01:38:01.185 --> 01:38:06.095
Um, and since the model originally is trained on

1603
01:38:06.095 --> 01:38:09.375
that huge dataset, it learns those generic features

1604
01:38:09.375 --> 01:38:11.215
that are applicable to a wide range of tasks.

1605
01:38:11.995 --> 01:38:15.335
And this can be especially beneficial when dealing

1606
01:38:15.405 --> 01:38:17.895
with limited label data for a specific task

1607
01:38:17.955 --> 01:38:20.215
or limited computational um, load,

1608
01:38:21.155 --> 01:38:25.255
or a situation where you, um, are putting, uh,

1609
01:38:25.315 --> 01:38:27.055
an excessive load on computations.

1610
01:38:27.375 --> 01:38:28.895
'cause a lot of that math is already done for you.

1611
01:38:30.095 --> 01:38:33.435
Um, also advantages in faster training.

1612
01:38:34.275 --> 01:38:37.515
Training a model from scratch on a large dataset can be

1613
01:38:37.775 --> 01:38:41.595
really expensive computationally and very time consuming.

1614
01:38:41.975 --> 01:38:44.595
Uh, and transfer learning allows us to start

1615
01:38:44.595 --> 01:38:48.035
with a pre-trained model, which saves a ton of time

1616
01:38:48.035 --> 01:38:51.915
and resource and improved generalization.

1617
01:38:52.455 --> 01:38:55.235
The knowledge gained from a diverse set of data

1618
01:38:55.295 --> 01:38:59.005
during pre-training, uh, usually helps us

1619
01:38:59.425 --> 01:39:01.845
and helps the model generalize really well

1620
01:39:01.845 --> 01:39:03.045
to new and unseen data.

1621
01:39:04.565 --> 01:39:06.705
Um, and that improves overall performance.

1622
01:39:08.125 --> 01:39:11.545
Um, it's really effective for related tasks.

1623
01:39:12.745 --> 01:39:14.865
Transfer learning is, um,

1624
01:39:15.235 --> 01:39:19.025
super effective when the pre-training task is related

1625
01:39:19.125 --> 01:39:23.105
to the target task and the shared features during that.

1626
01:39:23.405 --> 01:39:27.665
Um, pre-training can be beneficial when capturing relevant

1627
01:39:27.665 --> 01:39:29.585
patterns, uh, for those new tasks.

1628
01:39:30.605 --> 01:39:34.425
And for applicability across domains is another strength of,

1629
01:39:34.525 --> 01:39:35.625
uh, transfer learning.

1630
01:39:36.495 --> 01:39:39.195
Um, pre-trained models can really be transferred

1631
01:39:39.965 --> 01:39:41.265
across different domains.

1632
01:39:41.685 --> 01:39:45.465
So like a model trained on image classification may still be

1633
01:39:45.565 --> 01:39:48.785
useful, uh, for features related to computer vision,

1634
01:39:49.735 --> 01:39:51.945
even if the specific classes may differ.

1635
01:40:04.525 --> 01:40:06.785
So transfer learning can be applied

1636
01:40:06.885 --> 01:40:07.985
in lots of different ways.

1637
01:40:08.445 --> 01:40:10.505
Um, some of the ways I already kind of discussed

1638
01:40:10.505 --> 01:40:12.745
and the advantages, um,

1639
01:40:13.285 --> 01:40:15.905
but machine learning models in general dealing

1640
01:40:15.905 --> 01:40:18.385
with Latin national language processing can really be

1641
01:40:18.985 --> 01:40:20.385
strengthened with transfer learning.

1642
01:40:20.825 --> 01:40:24.225
Anything that has like a huge computational load, um,

1643
01:40:24.285 --> 01:40:26.465
if we can offset some of that load

1644
01:40:26.565 --> 01:40:27.785
by using transfer learning,

1645
01:40:27.815 --> 01:40:29.265
it's gonna be really beneficial for us.

1646
01:40:30.735 --> 01:40:32.395
Um, models for translating

1647
01:40:32.395 --> 01:40:37.075
between languages can also be modified, um, using

1648
01:40:37.595 --> 01:40:40.955
transfer learning and models that were developed.

1649
01:40:41.615 --> 01:40:45.995
Um, and trained using English language can be modified, um,

1650
01:40:46.055 --> 01:40:47.715
for tasks or languages that are similar.

1651
01:40:48.535 --> 01:40:52.795
Um, and also in, um, some basic like changing

1652
01:40:53.355 --> 01:40:55.725
from uh, one code, um, to another.

1653
01:40:55.905 --> 01:40:59.485
So like changing, uh, analysis you did from Art of Python,

1654
01:40:59.785 --> 01:41:02.285
uh, could be an area where you can use transfer learning.

1655
01:41:03.025 --> 01:41:05.805
Um, sometimes it's required for, um,

1656
01:41:06.915 --> 01:41:08.565
general code book changes.

1657
01:41:13.055 --> 01:41:16.235
Here's a, a pretty good graphical display

1658
01:41:16.455 --> 01:41:17.595
of transfer learning.

1659
01:41:18.695 --> 01:41:22.115
So in traditional machine learning,

1660
01:41:22.615 --> 01:41:26.035
we have a task and then we train a model.

1661
01:41:26.855 --> 01:41:28.715
So here's where we are traditional machine learning.

1662
01:41:29.665 --> 01:41:33.765
We have a task, we train a model, um,

1663
01:41:34.225 --> 01:41:35.885
and then we evaluate that.

1664
01:41:36.425 --> 01:41:39.045
Um, and if we have another task, we train another model

1665
01:41:39.185 --> 01:41:40.845
and then we evaluate that second model.

1666
01:41:41.725 --> 01:41:46.465
Uh, whereas with transfer learning, um, we have a task,

1667
01:41:47.315 --> 01:41:50.575
we have that model, we have the knowledge from the model,

1668
01:41:52.015 --> 01:41:56.625
um, and then that model, the model A improves

1669
01:41:57.325 --> 01:42:01.065
the task B comprehension and analysis.

1670
01:42:05.255 --> 01:42:08.755
So we get to have all of the learnings that we got

1671
01:42:09.345 --> 01:42:12.195
from the previous model analysis, um,

1672
01:42:12.415 --> 01:42:17.355
and we get to leverage that when, um, creating

1673
01:42:17.455 --> 01:42:18.995
and generating that second model.

1674
01:42:22.875 --> 01:42:27.445
Alright, um, looking at Bert as a example

1675
01:42:27.545 --> 01:42:28.605
of transfer learning.

1676
01:42:37.035 --> 01:42:40.215
So Bert, if we recall bi-directional

1677
01:42:41.045 --> 01:42:45.215
encoder representation, uh, from transformer,

1678
01:42:45.395 --> 01:42:47.775
and here is Bert from Sesame Street.

1679
01:42:50.265 --> 01:42:54.015
Um, if we have the example here,

1680
01:42:56.085 --> 01:43:00.445
sentence A and sentence B, sentence A, he got bit

1681
01:43:00.625 --> 01:43:03.195
by Python sentence B,

1682
01:43:03.775 --> 01:43:06.555
Python is my favorite programming language.

1683
01:43:07.615 --> 01:43:10.235
So we as humans are aware

1684
01:43:11.705 --> 01:43:15.515
what the two sentences differ by sentence.

1685
01:43:15.715 --> 01:43:17.875
A talks about the snake Python

1686
01:43:18.635 --> 01:43:21.555
sentence B talks about the programming language, uh, Python,

1687
01:43:22.205 --> 01:43:23.635
which we all know and love.

1688
01:43:25.265 --> 01:43:28.645
So as humans, we are aware of that difference.

1689
01:43:30.515 --> 01:43:35.125
Um, if we get the embeddings for the word python

1690
01:43:35.785 --> 01:43:39.365
in the previous two sentences using embedding models like

1691
01:43:39.365 --> 01:43:42.805
word deve, um, the embedding of the word python

1692
01:43:43.495 --> 01:43:46.405
would be the same in both sentences, which is a problem

1693
01:43:46.475 --> 01:43:48.845
because we need to know which one we're scared of, right?

1694
01:43:52.125 --> 01:43:56.705
Um, word deve is context free

1695
01:43:57.365 --> 01:43:58.945
and it will ignore that context

1696
01:43:59.565 --> 01:44:01.145
and always give us the same embedding

1697
01:44:01.145 --> 01:44:03.705
for both Python re uh, respective of the context.

1698
01:44:05.145 --> 01:44:07.065
Bert on the other hand, is context base,

1699
01:44:07.805 --> 01:44:09.705
so it will understand the context

1700
01:44:09.925 --> 01:44:12.545
and generate the embeddings, uh,

1701
01:44:12.725 --> 01:44:15.025
for the word based on the context.

1702
01:44:15.925 --> 01:44:17.825
So for the above sentence A

1703
01:44:17.825 --> 01:44:21.065
and B, um, it will give different embeddings

1704
01:44:21.205 --> 01:44:23.345
for the word Python based on the context.

1705
01:44:27.265 --> 01:44:28.445
Um, and like I was saying

1706
01:44:28.445 --> 01:44:32.045
before, it was this Bert algorithm was created initially

1707
01:44:32.385 --> 01:44:35.525
by Google, improved by some of the other big tech companies.

1708
01:44:36.785 --> 01:44:38.325
Um, and it, it was,

1709
01:44:38.325 --> 01:44:39.525
and still is considered a,

1710
01:44:39.645 --> 01:44:41.005
a great breakthrough in the field.

1711
01:44:41.225 --> 01:44:45.405
Um, there are a couple of papers, um, on Birch, uh,

1712
01:44:45.475 --> 01:44:50.245
that will link, um, in the presentation references slides.

1713
01:44:51.635 --> 01:44:53.375
Um, and like I was saying

1714
01:44:53.375 --> 01:44:56.895
before, one of the major reasons for the success, um,

1715
01:44:57.675 --> 01:45:02.255
of the Burt model is it's context based, which allows us

1716
01:45:02.315 --> 01:45:05.695
to really improve, um, the mappings here

1717
01:45:13.655 --> 01:45:18.065
and advantages of Burt by bi-directional,

1718
01:45:19.255 --> 01:45:21.635
um, and works for task specific models.

1719
01:45:22.945 --> 01:45:26.445
It is trained on these huge corpuses, so lots

1720
01:45:26.445 --> 01:45:27.725
and lots of pre-training data,

1721
01:45:28.145 --> 01:45:31.405
and that makes it easier for small, uh, more defined

1722
01:45:31.405 --> 01:45:35.765
and LP tasks, which in our day in day out work is, uh, uh,

1723
01:45:36.005 --> 01:45:37.045
frequently what we're working on.

1724
01:45:37.195 --> 01:45:39.245
Typically, we're not working on billions of records.

1725
01:45:39.335 --> 01:45:41.245
We're working on, you know, hundreds of thousands

1726
01:45:41.245 --> 01:45:42.905
or millions, um,

1727
01:45:43.005 --> 01:45:47.625
or even smaller, um, metrics can be fine tuned,

1728
01:45:49.755 --> 01:45:52.895
um, and used almost immediately, which is super great.

1729
01:45:54.015 --> 01:45:55.875
Um, and we'll see some of that in the demo.

1730
01:45:56.575 --> 01:45:59.555
Um, the accuracy of the model is really high, um,

1731
01:45:59.555 --> 01:46:01.555
because it is frequently updated.

1732
01:46:02.745 --> 01:46:06.045
Um, and that can be, uh, really beneficial

1733
01:46:06.385 --> 01:46:08.565
for successful fine tuning and training.

1734
01:46:09.305 --> 01:46:11.405
And, um, Bert model is available

1735
01:46:11.745 --> 01:46:15.405
and pre-trained in lots of different, um,

1736
01:46:15.615 --> 01:46:18.005
which is really helpful for a global scale.

1737
01:46:28.185 --> 01:46:31.645
Um, answering the question from, um, Baskar, uh,

1738
01:46:31.665 --> 01:46:33.325
for Python vector embeddings,

1739
01:46:33.325 --> 01:46:36.765
we will have different embeddings for the two sentences be,

1740
01:46:36.905 --> 01:46:38.445
um, that is because of the words

1741
01:46:38.805 --> 01:46:39.845
surrounding the word Python.

1742
01:46:41.025 --> 01:46:45.125
Um, so that in some cases is true,

1743
01:46:46.215 --> 01:46:50.045
um, but it is much more accurate

1744
01:46:50.225 --> 01:46:53.805
to use a more advanced model like, uh, Bert

1745
01:46:53.985 --> 01:46:58.445
or one of the other, um, more contextually rich models.

1746
01:46:59.225 --> 01:47:00.525
So great clarification.

1747
01:47:10.575 --> 01:47:11.065
Alright,

1748
01:47:17.305 --> 01:47:19.605
so Bert, I'm gonna say the acronym again,

1749
01:47:19.635 --> 01:47:22.405
it's always helpful to keep reiterating the acronym,

1750
01:47:22.955 --> 01:47:25.965
bidirectional Encoder Representations from Transformers.

1751
01:47:31.165 --> 01:47:34.185
Um, architecture is fundamentally based

1752
01:47:35.085 --> 01:47:39.145
on the encoder component of the original transformer.

1753
01:47:42.005 --> 01:47:46.755
Um, and if you get bit by a python

1754
01:47:47.415 --> 01:47:49.035
and you be interpreted as the love

1755
01:47:49.035 --> 01:47:51.905
of the Python language, I'm not sure.

1756
01:47:52.345 --> 01:47:54.745
I think that really depends on, uh,

1757
01:47:55.125 --> 01:47:57.685
how you feel about getting bit, uh,

1758
01:47:58.685 --> 01:47:59.925
I had snakes when I was a kid

1759
01:47:59.925 --> 01:48:01.565
and I was always a little bit afraid of 'em.

1760
01:48:03.775 --> 01:48:07.815
Um, okay, so Bert,

1761
01:48:07.835 --> 01:48:10.175
as an encoder marvel, it really is a state

1762
01:48:10.175 --> 01:48:12.335
of the art algorithm when it was initially created.

1763
01:48:13.195 --> 01:48:15.255
Um, contextualized representations.

1764
01:48:16.015 --> 01:48:19.575
Bert uses, uh, the transformer encoder process input data,

1765
01:48:19.705 --> 01:48:24.185
which is a lot of jargon, transformer encoder

1766
01:48:25.245 --> 01:48:26.905
to process input data.

1767
01:48:28.005 --> 01:48:30.825
And it uses that process to, uh,

1768
01:48:30.875 --> 01:48:33.705
forge high dimensional context rich.

1769
01:48:34.365 --> 01:48:37.105
Um, so while we do have some context in our previous

1770
01:48:37.105 --> 01:48:40.105
methodologies, this really improves the context.

1771
01:48:41.345 --> 01:48:46.325
Um, the handling these embeddings do not merely mirror

1772
01:48:46.425 --> 01:48:47.965
the input words or phrases,

1773
01:48:48.305 --> 01:48:51.405
but they also encapsulate their meanings in light

1774
01:48:51.405 --> 01:48:54.645
of the surrounding context, offering a robust, uh,

1775
01:48:54.695 --> 01:48:56.685
foundation for various downstream

1776
01:48:56.685 --> 01:48:58.045
natural language processing tasks.

1777
01:48:59.785 --> 01:49:03.665
Um, so pre-training

1778
01:49:04.765 --> 01:49:09.475
is going to be the main benefit here.

1779
01:49:10.455 --> 01:49:12.975
Masked language modeling,

1780
01:49:13.065 --> 01:49:14.735
which we're gonna be talking a little bit

1781
01:49:14.735 --> 01:49:15.895
about in the next coming slides.

1782
01:49:16.975 --> 01:49:19.915
Um, Bert went through huge pre-training regimen

1783
01:49:20.795 --> 01:49:24.365
involving tasks, um, like that mass language modeling,

1784
01:49:24.775 --> 01:49:27.085
where random words in a sentence are masked

1785
01:49:27.465 --> 01:49:28.645
and the model is trained

1786
01:49:28.645 --> 01:49:30.605
to predict them based on surrounding context.

1787
01:49:31.425 --> 01:49:34.765
Um, and we're also going to be doing, um,

1788
01:49:35.635 --> 01:49:39.025
next sentence prediction, um, which is another way

1789
01:49:39.175 --> 01:49:40.745
that the Burt model was trained.

1790
01:49:41.805 --> 01:49:44.785
Um, Bert will, in next language

1791
01:49:45.045 --> 01:49:47.565
or next sentence prediction, uh, Bert attempts

1792
01:49:47.565 --> 01:49:50.205
to predict whether the two sentences are likely

1793
01:49:50.265 --> 01:49:54.325
to appear contextually or, uh, consecutively in a text.

1794
01:49:55.775 --> 01:49:59.475
Um, and that training helps it cultivate an understanding

1795
01:49:59.895 --> 01:50:03.915
of relationships and coherence between sentences.

1796
01:50:05.895 --> 01:50:09.805
These pre-training steps serve as a crucial playground,

1797
01:50:10.725 --> 01:50:13.405
allowing Burt to learn dynamic languages,

1798
01:50:14.075 --> 01:50:16.805
syntactical properties, and semantic nuances

1799
01:50:17.025 --> 01:50:19.005
before it embarks on specific tasks.

1800
01:50:22.085 --> 01:50:26.345
Um, another section that we're gonna be discussing is

1801
01:50:27.095 --> 01:50:28.865
word positional information.

1802
01:50:30.065 --> 01:50:33.365
Um, unlike some earlier models that might overlook, uh,

1803
01:50:33.505 --> 01:50:35.765
the sequential nature of language,

1804
01:50:36.685 --> 01:50:40.765
Bert will meticulously incorporate word positions

1805
01:50:41.735 --> 01:50:43.665
information into its embeddings.

1806
01:50:45.065 --> 01:50:47.125
Um, and when forming those embeddings,

1807
01:50:47.885 --> 01:50:50.965
Bert encodes positional information to make sure

1808
01:50:51.035 --> 01:50:52.125
that the sequential

1809
01:50:52.425 --> 01:50:54.125
and, uh, structural aspects

1810
01:50:54.125 --> 01:50:57.725
of the language which are super pivotal in understanding

1811
01:50:57.785 --> 01:51:00.005
and meaning and context are preserved

1812
01:51:00.005 --> 01:51:01.365
and utilized in the training phase.

1813
01:51:02.065 --> 01:51:05.605
Um, Bert's ability

1814
01:51:05.605 --> 01:51:09.125
to create deeply contextualize word representation

1815
01:51:09.665 --> 01:51:12.205
has really pioneered the advancements in lots

1816
01:51:12.205 --> 01:51:13.965
of natural language processing algorithms.

1817
01:51:14.505 --> 01:51:18.765
Um, providing a powerful pre-trained starting point for lots

1818
01:51:18.765 --> 01:51:22.685
of tasks, uh, like tech summarization and question answering

1819
01:51:23.025 --> 01:51:24.325
and a ton of other stuff.

1820
01:51:24.775 --> 01:51:26.965
There would not be Chad GBT without Bert.

1821
01:51:28.975 --> 01:51:33.195
Um, and as we embrace the elegance of Bert,

1822
01:51:33.895 --> 01:51:36.835
we observe how it's architecture, uh,

1823
01:51:36.835 --> 01:51:39.195
founded on the transformer encoder

1824
01:51:39.215 --> 01:51:41.355
and enriched through some pre-training, um,

1825
01:51:41.375 --> 01:51:46.065
how it offers robust context aware base for

1826
01:51:46.815 --> 01:51:49.105
tons of different NLP uh, applications.

1827
01:51:50.145 --> 01:51:54.685
And moving forward, uh, let's look at how Bert is adapted

1828
01:51:55.025 --> 01:51:57.765
and fine tuned, uh, for specific use cases

1829
01:52:08.725 --> 01:52:12.825
and Bert input representation.

1830
01:52:13.845 --> 01:52:17.025
Uh, we can see how Bert manages all

1831
01:52:17.025 --> 01:52:18.145
of these different inputs

1832
01:52:18.525 --> 01:52:20.945
and how that's going to be different from, um, all

1833
01:52:20.945 --> 01:52:22.505
of the improvements on Bert

1834
01:52:22.505 --> 01:52:23.785
that we're gonna be talking about later.

1835
01:52:25.835 --> 01:52:30.535
Um, and we are now at the halfway point of the lecture time,

1836
01:52:31.075 --> 01:52:35.255
um, and this is the point where I know I can get monotone

1837
01:52:35.595 --> 01:52:37.015
and sometimes like, oh my gosh,

1838
01:52:37.155 --> 01:52:39.095
Ben is gonna be talking for another two hours.

1839
01:52:39.875 --> 01:52:44.455
But, uh, I really appreciate everybody's, um, understanding

1840
01:52:45.145 --> 01:52:48.785
and, um, attention that you're paying to the class.

1841
01:52:49.295 --> 01:52:51.865
This is really important to,

1842
01:52:51.865 --> 01:52:53.145
these are really important topics

1843
01:52:54.005 --> 01:52:56.345
and I'm sure that it will be helpful for

1844
01:52:56.345 --> 01:52:57.385
to your career in the future.

1845
01:52:57.725 --> 01:52:58.985
Having this knowledge in natural,

1846
01:52:58.985 --> 01:53:01.265
natural language processing is going to be,

1847
01:53:01.525 --> 01:53:02.745
uh, really beneficial.

1848
01:53:03.255 --> 01:53:05.585
Even if you're not a natural language processing engineer,

1849
01:53:05.615 --> 01:53:06.425
it's always important

1850
01:53:06.445 --> 01:53:07.865
to remember all of these, these things.

1851
01:53:08.765 --> 01:53:11.465
Um, so a four hour lecture is a really long time,

1852
01:53:11.725 --> 01:53:13.545
so I really appreciate everybody's time

1853
01:53:13.685 --> 01:53:17.105
and willingness to have their wrapped attention, uh,

1854
01:53:17.125 --> 01:53:19.145
for these whole four-ish hours.

1855
01:53:22.675 --> 01:53:27.255
So looking at Howbert manages these embeddings

1856
01:53:27.255 --> 01:53:29.775
and understanding their, uh, words

1857
01:53:30.035 --> 01:53:31.815
and their order in the sentences.

1858
01:53:32.155 --> 01:53:36.965
So if we have the input sentence, my dog is cute, um,

1859
01:53:37.185 --> 01:53:40.035
he likes playing, um,

1860
01:53:41.295 --> 01:53:44.195
and see I as a human immediately combined those.

1861
01:53:44.935 --> 01:53:48.625
Um, but Bert, um,

1862
01:53:50.645 --> 01:53:52.985
can, might not necessarily do that.

1863
01:53:53.285 --> 01:53:57.145
Um, and we'll talk about how Bert is different, uh,

1864
01:53:57.335 --> 01:53:58.625
when combining these

1865
01:53:59.085 --> 01:54:02.905
or these words versus the improvements on it in the future

1866
01:54:04.885 --> 01:54:05.945
or in the future slides.

1867
01:54:06.685 --> 01:54:09.785
So for token embedding, so remember, each

1868
01:54:09.785 --> 01:54:11.425
of these are tokens that are embedded.

1869
01:54:12.385 --> 01:54:14.605
Uh, this is the embedded token of each one of these.

1870
01:54:15.145 --> 01:54:16.725
We have an entire embedding for,

1871
01:54:17.145 --> 01:54:19.685
or we have an entire embedding for just the, um,

1872
01:54:20.125 --> 01:54:21.405
ING at ending.

1873
01:54:22.785 --> 01:54:26.925
Um, so this is a map

1874
01:54:27.595 --> 01:54:32.165
from words or word pieces to vectors of numbers.

1875
01:54:33.415 --> 01:54:37.785
Imagine like each word here is getting a unique fingerprint

1876
01:54:38.045 --> 01:54:41.885
that's effectively just a bunch of numbers of numbers.

1877
01:54:41.885 --> 01:54:43.205
These are looked up from a table.

1878
01:54:43.865 --> 01:54:47.965
And yes, these vectors are learned during training.

1879
01:54:49.125 --> 01:54:52.045
Bert will tweak them bit by bit as it learns

1880
01:54:52.465 --> 01:54:55.565
to reduce prediction errors in, uh, the training tasks.

1881
01:54:58.045 --> 01:55:01.365
And that's token embeddings

1882
01:55:01.945 --> 01:55:03.165
in segment embeddings.

1883
01:55:04.005 --> 01:55:06.765
Once we finish token embedding, um,

1884
01:55:07.095 --> 01:55:09.125
birth could be reading a sentence or two,

1885
01:55:09.665 --> 01:55:11.485
and if it's two, it needs

1886
01:55:11.485 --> 01:55:13.845
to know which word belongs to which sentence.

1887
01:55:15.805 --> 01:55:19.575
Um, so words from the first sentence

1888
01:55:19.575 --> 01:55:21.895
here, my dog is cute.

1889
01:55:26.385 --> 01:55:29.085
And the words from the second sentence he likes playing,

1890
01:55:36.105 --> 01:55:38.125
um, have separators in between.

1891
01:55:38.275 --> 01:55:41.365
They're segment embedded or segment embedded,

1892
01:55:41.985 --> 01:55:45.365
and they are added to the token embeddings of all

1893
01:55:45.365 --> 01:55:47.525
of the words in each of their respective sentences.

1894
01:55:48.105 --> 01:55:49.525
Um, they're also learned

1895
01:55:49.745 --> 01:55:51.525
and refined during the training process.

1896
01:55:55.735 --> 01:56:00.195
So, um, vasan answering your first question,

1897
01:56:01.545 --> 01:56:02.685
what's up with the split?

1898
01:56:03.025 --> 01:56:05.245
Uh, let me just discuss that live real quick.

1899
01:56:06.075 --> 01:56:06.895
Um, so

1900
01:56:10.925 --> 01:56:13.345
if you consider the English language, um,

1901
01:56:13.345 --> 01:56:14.625
there's lots of tenses.

1902
01:56:15.285 --> 01:56:19.515
Uh, so we have like, play played,

1903
01:56:22.675 --> 01:56:26.745
playing, et cetera, right?

1904
01:56:27.825 --> 01:56:31.765
Um, so here is just play here is play ed,

1905
01:56:31.985 --> 01:56:33.325
and here's play, ING.

1906
01:56:33.985 --> 01:56:37.765
So for the concept of play, we have

1907
01:56:39.455 --> 01:56:44.035
at least three different interpretations, uh, replicated by

1908
01:56:44.775 --> 01:56:49.685
the different, uh, the different text, the different,

1909
01:56:49.705 --> 01:56:54.365
uh, sorry, I'm blanking on the, the sub

1910
01:56:54.625 --> 01:56:58.755
or the, um, following words, the tense.

1911
01:56:59.015 --> 01:57:02.475
So Ed or uh, um, ING.

1912
01:57:03.265 --> 01:57:06.245
So we have to store each one of those as opposed

1913
01:57:06.305 --> 01:57:09.285
to if we just have ING stored,

1914
01:57:10.505 --> 01:57:14.555
then we can use that embedding, uh, to apply

1915
01:57:14.655 --> 01:57:16.635
to just the base word.

1916
01:57:17.575 --> 01:57:20.315
So like, if you think of play, um,

1917
01:57:20.455 --> 01:57:22.635
and we have playing, um,

1918
01:57:23.295 --> 01:57:26.875
and we can think of something like eat, uh,

1919
01:57:26.895 --> 01:57:31.195
we can apply this same contextual embedding of, uh,

1920
01:57:31.385 --> 01:57:36.315
hash ING to all verbs, um, playing,

1921
01:57:36.495 --> 01:57:40.505
eating, um, looking things like that.

1922
01:57:41.205 --> 01:57:43.625
So that really saves space

1923
01:57:44.135 --> 01:57:47.505
because we can have each of these sections

1924
01:57:48.125 --> 01:57:49.145
stored separately.

1925
01:58:01.645 --> 01:58:04.015
Alright, so covering token, embedding segment,

1926
01:58:04.015 --> 01:58:05.455
embedding, position embedding.

1927
01:58:06.035 --> 01:58:10.875
Um, so for position embedding right here, um,

1928
01:58:11.155 --> 01:58:12.515
Bert will need to know the position

1929
01:58:12.515 --> 01:58:13.755
of a word within a sequence.

1930
01:58:14.615 --> 01:58:17.675
So I think that gets a little bit into how's question.

1931
01:58:18.585 --> 01:58:21.325
Um, but ta please feel free to answer that as well.

1932
01:58:21.505 --> 01:58:23.445
Uh, your answers are very complete, which is awesome.

1933
01:58:25.555 --> 01:58:28.895
Um, so to achieve that, each position up

1934
01:58:28.895 --> 01:58:30.615
to the max sequence length gets its own.

1935
01:58:30.615 --> 01:58:33.815
Embedding another bunch of numbers, um,

1936
01:58:35.585 --> 01:58:38.445
is the embedding words in the first position,

1937
01:58:38.945 --> 01:58:39.965
get the first positional,

1938
01:58:39.965 --> 01:58:41.925
embedding words within the second position,

1939
01:58:42.105 --> 01:58:44.605
get the second positional embedding, and so on and so forth.

1940
01:58:45.455 --> 01:58:50.005
These positional embeddings, um, are also learned during

1941
01:58:50.005 --> 01:58:53.925
that training phase and

1942
01:58:54.565 --> 01:58:55.685
bringing everything together.

1943
01:58:56.595 --> 01:59:00.925
Each word is represented by a sum of three vectors

1944
01:59:01.785 --> 01:59:04.765
has its own token embedding has its own segment embedding,

1945
01:59:05.145 --> 01:59:07.325
um, and has its own positional embedding.

1946
01:59:08.075 --> 01:59:10.375
Um, all of these numbers are crunched together

1947
01:59:10.995 --> 01:59:13.855
to give us a pretty rich cocktail of information

1948
01:59:13.955 --> 01:59:15.095
for each word to Bert.

1949
01:59:17.495 --> 01:59:19.155
Um, and how are they learned?

1950
01:59:21.215 --> 01:59:23.955
How does Bert, uh, educate itself?

1951
01:59:24.055 --> 01:59:25.595
How is it machine learning?

1952
01:59:27.275 --> 01:59:31.095
Um, Bert tries to get better at all of these tasks.

1953
01:59:32.075 --> 01:59:35.415
Um, and if it makes a mistake, it slightly adjusts all

1954
01:59:35.415 --> 01:59:37.415
of these embeddings to reduce the chance

1955
01:59:37.675 --> 01:59:39.255
of making the same mistake.

1956
01:59:39.255 --> 01:59:39.695
Next time.

1957
01:59:44.215 --> 01:59:49.145
Bert gradually fine tunes this understanding of words

1958
01:59:49.525 --> 01:59:53.425
as it learns the meanings of those words, the order of the,

1959
01:59:53.425 --> 01:59:56.465
those words and the sentence, uh, membership

1960
01:59:56.605 --> 01:59:59.785
to get a better understanding at text took

1961
02:00:03.925 --> 02:00:08.645
me quite a few years to learn English, uh, and how to speak.

1962
02:00:09.225 --> 02:00:11.685
Um, Bert is only a couple years old,

1963
02:00:11.685 --> 02:00:13.005
so is doing a great job so far.

1964
02:00:21.375 --> 02:00:23.075
And like we discussed,

1965
02:00:23.835 --> 02:00:26.195
remember we've got positional embedding, segment embedding,

1966
02:00:26.195 --> 02:00:30.875
token embedding, and from all of those three embeddings,

1967
02:00:31.885 --> 02:00:36.465
um, we can have the sentence, um, Paris is a beautiful city,

1968
02:00:37.455 --> 02:00:39.795
and sentence BI love Paris.

1969
02:00:42.055 --> 02:00:45.435
So those tokenizing those different sentences, we can get,

1970
02:00:45.855 --> 02:00:47.475
um, lots of different tokens,

1971
02:00:47.615 --> 02:00:49.035
uh, distinct for each sentence.

1972
02:00:49.895 --> 02:00:53.755
Um, so we have, Paris is a beautiful city,

1973
02:00:54.335 --> 02:00:55.355
and then a separator.

1974
02:00:55.595 --> 02:00:59.615
I love Paris. So two different sentences, uh,

1975
02:00:59.675 --> 02:01:01.495
and we'll be talking about that in the next slide.

1976
02:01:04.075 --> 02:01:07.495
So here we have those sentences. Paris is a beautiful city.

1977
02:01:08.895 --> 02:01:11.335
I love Paris. I personally have never been there,

1978
02:01:11.335 --> 02:01:15.905
but the lecture slides have, um, position embedding

1979
02:01:16.645 --> 02:01:21.185
is used to incorporate information about the position

1980
02:01:21.565 --> 02:01:24.345
of words or tokens within a sequence.

1981
02:01:26.085 --> 02:01:30.635
So how position embedding works, um, in

1982
02:01:31.275 --> 02:01:32.915
transformer models, like what we're talking about today.

1983
02:01:33.675 --> 02:01:38.595
Position embedding is added to the embedding vectors

1984
02:01:39.145 --> 02:01:40.365
of the input tokens.

1985
02:01:41.705 --> 02:01:44.125
Um, and that provides information about

1986
02:01:44.125 --> 02:01:46.285
how their positions are in this sequence

1987
02:01:46.955 --> 02:01:50.085
that allows the Burt model to distinguish between, um,

1988
02:01:50.085 --> 02:01:52.525
different positions in a sequence without relying

1989
02:01:52.625 --> 02:01:54.165
solely on the token order.

1990
02:01:55.855 --> 02:01:59.705
Um, our encoding function, um,

1991
02:01:59.845 --> 02:02:02.265
the positional encoding here is usually created

1992
02:02:02.335 --> 02:02:03.945
with a mathematical function.

1993
02:02:04.525 --> 02:02:08.505
Um, and that generates a unique vector in, um,

1994
02:02:08.735 --> 02:02:10.465
each position for each sequence.

1995
02:02:12.295 --> 02:02:14.275
Um, most common approach here is

1996
02:02:14.275 --> 02:02:16.035
to use like trigonometry functions,

1997
02:02:16.035 --> 02:02:18.795
like we were talking about earlier in the slide, like sign

1998
02:02:18.795 --> 02:02:21.355
and code is signed to create those embeddings, um,

1999
02:02:21.825 --> 02:02:24.275
that capture the positional information there.

2000
02:02:26.025 --> 02:02:29.005
Um, and Bert is essentially the

2001
02:02:29.285 --> 02:02:30.525
Transformers encoder in general.

2002
02:02:31.145 --> 02:02:34.445
Um, and we need to give Bert information about the position

2003
02:02:34.745 --> 02:02:37.125
of those words in our sentence

2004
02:02:37.205 --> 02:02:39.125
before feeding them directly into Burt.

2005
02:02:40.095 --> 02:02:41.915
Um, so kind of similar to

2006
02:02:41.915 --> 02:02:43.755
what we had in the previous slides, just at a,

2007
02:02:43.795 --> 02:02:47.595
a more advanced level, um, positional embedding

2008
02:02:48.255 --> 02:02:51.475
is used to get the position, uh, the,

2009
02:02:51.815 --> 02:02:56.635
or provide that position, uh, vector to Burke or to Burt.

2010
02:02:57.335 --> 02:02:59.395
Um, and Bert specifically used

2011
02:02:59.425 --> 02:03:00.995
trained positional embeddings.

2012
02:03:01.535 --> 02:03:04.635
So like what we're looking at in this example, each one

2013
02:03:04.635 --> 02:03:07.445
of these, um, embeddings

2014
02:03:08.385 --> 02:03:10.085
is translated here.

2015
02:03:10.585 --> 02:03:15.405
So 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, where separators are also,

2016
02:03:15.625 --> 02:03:18.765
and, uh, start statement at close statement are also

2017
02:03:19.045 --> 02:03:20.885
considered in those embeddings

2018
02:03:22.825 --> 02:03:25.205
and explaining it further in the coming slides

2019
02:03:27.845 --> 02:03:31.115
For segment embeddings, um,

2020
02:03:32.385 --> 02:03:36.285
we use segment embeddings to distinguish two sentences.

2021
02:03:36.985 --> 02:03:41.685
So we have this one segment here. Paris is a beautiful city.

2022
02:03:42.325 --> 02:03:44.705
And then we have the next segment here. I love Paris.

2023
02:03:45.285 --> 02:03:46.585
See how the segment embedding.

2024
02:03:46.585 --> 02:03:49.705
This is segment one or segment A.

2025
02:03:50.945 --> 02:03:52.365
Um, and then this is segment B.

2026
02:03:54.665 --> 02:03:58.765
Um, so apart from that separator token, we kind of sort out,

2027
02:03:59.305 --> 02:04:02.725
um, the indicator of our model to distinguish

2028
02:04:02.755 --> 02:04:04.285
between those two sentences.

2029
02:04:05.025 --> 02:04:09.005
Um, so we feed in both input tokens, each segment, um,

2030
02:04:09.065 --> 02:04:12.445
and then we output the segment embeddings to, uh,

2031
02:04:12.585 --> 02:04:13.685
enrich that context.

2032
02:04:19.795 --> 02:04:23.375
And then for the final phase, uh, token embedding

2033
02:04:24.065 --> 02:04:26.085
in this diagram, we have all the

2034
02:04:26.085 --> 02:04:27.405
embeddings for all the tokens.

2035
02:04:28.105 --> 02:04:31.085
And, um, CLS

2036
02:04:32.925 --> 02:04:35.095
indicates the embedding of that token.

2037
02:04:36.305 --> 02:04:41.055
Um, and e Paris, um,

2038
02:04:41.125 --> 02:04:43.215
indicates the embedding of the Paris token

2039
02:04:43.435 --> 02:04:44.495
and so on and so forth.

2040
02:04:47.905 --> 02:04:51.605
Uh, so the token embedding is a representation of each

2041
02:04:51.605 --> 02:04:54.365
of those words, or sub words or strings

2042
02:04:54.365 --> 02:04:55.725
or sentences in

2043
02:04:55.725 --> 02:04:58.885
that language converted into a continuous vector space.

2044
02:05:00.765 --> 02:05:03.825
Um, these embeddings capture semantic relationships

2045
02:05:04.015 --> 02:05:05.265
between the words.

2046
02:05:06.295 --> 02:05:09.395
Um, so that allows us to really have rich context

2047
02:05:10.335 --> 02:05:13.315
and for our models to understand the, uh,

2048
02:05:13.345 --> 02:05:14.755
process of national language.

2049
02:05:21.025 --> 02:05:22.885
So like I said

2050
02:05:22.885 --> 02:05:25.565
before, all of the separators, um, that's

2051
02:05:25.565 --> 02:05:27.205
how we separate sentence to sentence.

2052
02:05:27.825 --> 02:05:32.525
And CLS is for classification. Um, separators used to.

2053
02:05:32.825 --> 02:05:36.565
So, uh, that's kind of the input token is we're classifying.

2054
02:05:37.455 --> 02:05:39.715
Um, and before feeding all those tokens in divert,

2055
02:05:39.975 --> 02:05:42.115
we convert those tokens into embeddings

2056
02:05:43.165 --> 02:05:47.545
and using the embeddings, um, we have that final layer

2057
02:05:47.565 --> 02:05:48.585
of token embedding.

2058
02:05:50.225 --> 02:05:53.925
Um, and all of these token embeddings will be learned

2059
02:05:54.025 --> 02:05:55.405
during that training process.

2060
02:06:01.885 --> 02:06:03.585
So getting even further,

2061
02:06:05.525 --> 02:06:08.155
we've got the Bert tokenization mechanism.

2062
02:06:10.025 --> 02:06:12.395
And, um, Bert

2063
02:06:13.585 --> 02:06:16.155
uses a very special kind of tokenize,

2064
02:06:16.445 --> 02:06:19.275
which is called the word piece, tokenize, not to be confused

2065
02:06:19.305 --> 02:06:20.715
with one piece tokenize.

2066
02:06:21.695 --> 02:06:23.795
Um, let's start

2067
02:06:24.735 --> 02:06:27.075
by pre-training the model here in this example.

2068
02:06:28.855 --> 02:06:31.835
Um, so we have that sentence.

2069
02:06:33.355 --> 02:06:34.655
Let us start pre-training.

2070
02:06:35.275 --> 02:06:38.315
Uh, the model, remember, uh,

2071
02:06:38.655 --> 02:06:41.475
pre is a prefix.

2072
02:06:41.475 --> 02:06:43.435
That's what I was looking before, prefix and suffix.

2073
02:06:44.055 --> 02:06:46.315
Um, so if we take the prefix

2074
02:06:46.815 --> 02:06:50.075
and suffix of all base words, um, then

2075
02:06:50.075 --> 02:06:51.395
that saves us a ton of space.

2076
02:06:52.525 --> 02:06:54.965
So pre-train, um,

2077
02:06:58.215 --> 02:07:01.615
And pre-training

2078
02:07:06.765 --> 02:07:10.275
becomes train

2079
02:07:11.505 --> 02:07:16.365
with the suffix, pre or prefix pre,

2080
02:07:17.985 --> 02:07:22.545
and then the suffix, um, in,

2081
02:07:28.845 --> 02:07:31.705
Um, and that is the tokenized vector here.

2082
02:07:32.365 --> 02:07:35.285
So one thing that you can notice is if, if

2083
02:07:35.285 --> 02:07:38.045
that token is not presi, uh, present in the vocabulary,

2084
02:07:38.535 --> 02:07:41.045
it'll split it into sub tokens, like I was saying,

2085
02:07:41.045 --> 02:07:42.405
with the prefix and the suffix.

2086
02:07:43.305 --> 02:07:46.125
Um, and we do that until we can find all

2087
02:07:46.125 --> 02:07:47.565
of the tokens in the vocabulary.

2088
02:07:47.565 --> 02:07:51.765
So we have this unique, uh, corpus bank of tokens,

2089
02:07:52.825 --> 02:07:55.885
um, that we end in a unigram.

2090
02:07:56.745 --> 02:08:00.085
Um, and in the above example, we can see that, uh,

2091
02:08:00.125 --> 02:08:04.485
pre-training is split into those three segments.

2092
02:08:06.325 --> 02:08:11.205
Um, every sub token, um, which is a part of a word, is, um,

2093
02:08:11.205 --> 02:08:15.085
given that hash mark to differentiate it from other tokens.

2094
02:08:16.305 --> 02:08:19.605
Um, and this strategy better handles the unseen token

2095
02:08:19.605 --> 02:08:20.645
during the inference phase.

2096
02:08:21.265 --> 02:08:22.325
And like we were saying

2097
02:08:22.325 --> 02:08:23.645
before, really helps us

2098
02:08:23.995 --> 02:08:26.565
with reducing the size complexity of our model.

2099
02:08:32.725 --> 02:08:36.995
All right, so now we're gonna get a little bit deeper

2100
02:08:37.465 --> 02:08:39.355
into how Burt was trained.

2101
02:08:42.995 --> 02:08:46.015
So the Burt methodology, um,

2102
02:08:46.405 --> 02:08:50.175
uses two general sections, uh, for training.

2103
02:08:51.395 --> 02:08:53.775
Um, and that is, we kind of discussed it a little bit

2104
02:08:53.775 --> 02:08:56.495
before, mast language model,

2105
02:08:57.855 --> 02:09:01.335
MLM, not to be confused with LLM.

2106
02:09:02.235 --> 02:09:05.375
Um, and then next sentence prediction.

2107
02:09:14.685 --> 02:09:16.105
Uh, questions have been relatively

2108
02:09:16.105 --> 02:09:17.385
quiet for the last little bit.

2109
02:09:18.315 --> 02:09:22.975
Um, we are going to take a break in a couple of minutes,

2110
02:09:24.475 --> 02:09:29.295
um, at 1130, let's take a break.

2111
02:09:29.675 --> 02:09:32.495
Um, usually I'll watch questions come in,

2112
02:09:32.595 --> 02:09:35.495
and if we like have a low volume, um, that means

2113
02:09:35.495 --> 02:09:36.575
that everybody's really tired

2114
02:09:37.435 --> 02:09:40.095
and I believe in you, we can do it together.

2115
02:09:40.385 --> 02:09:42.975
We've got just over 90 minutes remaining.

2116
02:09:43.315 --> 02:09:46.055
And, um, this has been a lot of lecture slides.

2117
02:09:46.655 --> 02:09:49.615
I think we've got some code demos, which are always fun

2118
02:09:49.615 --> 02:09:52.775
to watch in, um, three or four slides.

2119
02:09:53.635 --> 02:09:56.335
So we'll take a break once we get to the code demo.

2120
02:09:56.865 --> 02:10:00.775
We've got 1, 2, 3 slides including this one.

2121
02:10:02.005 --> 02:10:03.665
Um, and then we'll get to the code demo,

2122
02:10:04.205 --> 02:10:06.665
and then we'll do the code demo, and then we'll start fresh

2123
02:10:06.925 --> 02:10:09.785
and be excited and everybody is gonna learn

2124
02:10:09.795 --> 02:10:11.305
today and it's gonna be awesome.

2125
02:10:14.285 --> 02:10:18.875
Okay? Uh, mass language modeling is used,

2126
02:10:19.855 --> 02:10:24.155
um, for the pre-training phase of models like Bert.

2127
02:10:25.375 --> 02:10:29.475
Um, who remembers what Bert stands for?

2128
02:10:32.435 --> 02:10:33.455
Drop it into the chat.

2129
02:10:40.435 --> 02:10:41.855
All right, five seconds until I

2130
02:10:41.855 --> 02:10:42.975
give you the first word as a hint.

2131
02:10:48.655 --> 02:10:50.595
Bi-directional is the first word,

2132
02:10:52.865 --> 02:10:54.845
second word we talked a lot about earlier.

2133
02:10:58.985 --> 02:11:01.765
Oh, look at that. Great work. Great work. Benjamin.

2134
02:11:01.765 --> 02:11:05.115
Great work. Have great work. Fassan crushed it.

2135
02:11:06.155 --> 02:11:10.295
Bi-directional encoder representations from transformers.

2136
02:11:11.075 --> 02:11:12.335
All right, good work team.

2137
02:11:15.815 --> 02:11:19.465
Um, okay, so Bert, the goal

2138
02:11:19.645 --> 02:11:23.025
of MLM is to train a model to predict missing

2139
02:11:23.285 --> 02:11:27.105
or mass words with a given context, promoting

2140
02:11:27.665 --> 02:11:31.105
a deep language, or sorry, promoting a deep understanding

2141
02:11:31.125 --> 02:11:34.505
of language, um, semantics and context.

2142
02:11:36.705 --> 02:11:38.445
Um, here's how it works.

2143
02:11:41.105 --> 02:11:45.575
So our input, how are you doing today?

2144
02:11:46.805 --> 02:11:49.125
We put it into this black box,

2145
02:11:49.835 --> 02:11:52.365
Burt mass language model and the output.

2146
02:11:53.265 --> 02:11:58.085
How are you doing today, um, with this mask?

2147
02:12:00.135 --> 02:12:04.435
See, I naturally filled that in as a human.

2148
02:12:05.275 --> 02:12:08.075
I read, how are you today doing today?

2149
02:12:08.815 --> 02:12:13.115
And I just implicitly put that mask in as you,

2150
02:12:14.015 --> 02:12:17.355
we know that that value is likely you as humans.

2151
02:12:18.445 --> 02:12:21.145
Um, but we're not letting Bert know that

2152
02:12:21.675 --> 02:12:22.865
we're testing Bert on that.

2153
02:12:23.615 --> 02:12:25.075
So we have that masked.

2154
02:12:25.495 --> 02:12:29.995
Um, and then the output is a probability vector

2155
02:12:30.525 --> 02:12:33.575
where we have you,

2156
02:12:36.505 --> 02:12:39.965
uh, they or your,

2157
02:12:42.045 --> 02:12:44.665
all of those are potential options for the output, whether

2158
02:12:44.665 --> 02:12:46.625
or not they're grammatically correct, they're still options.

2159
02:12:47.945 --> 02:12:51.995
And then, uh, the Bert output process, uh,

2160
02:12:51.995 --> 02:12:53.235
will assign probabilities.

2161
02:12:54.015 --> 02:12:56.315
So probability, that's statistics, terminology

2162
02:12:56.655 --> 02:13:00.325
or statistics, um, diction.

2163
02:13:01.065 --> 02:13:05.845
Um, so they may have probability of you probably like,

2164
02:13:05.925 --> 02:13:07.125
I don't know, maybe 80%.

2165
02:13:08.345 --> 02:13:12.325
Uh, how are they doing today? Maybe that's like 10%.

2166
02:13:14.065 --> 02:13:15.405
How are your doing today?

2167
02:13:16.145 --> 02:13:20.605
Uh, maybe that's like 10% or, um, like 5%.

2168
02:13:22.085 --> 02:13:26.915
And then, um, it will provide enough

2169
02:13:27.295 --> 02:13:31.965
to, uh, give options to, uh, some to one typically.

2170
02:13:32.955 --> 02:13:37.335
Um, but the model would choose the mask here as

2171
02:13:37.555 --> 02:13:38.855
how are you doing today?

2172
02:13:38.925 --> 02:13:40.015
Just like you and I would.

2173
02:13:45.115 --> 02:13:49.535
Um, so said in different words, the input sequence,

2174
02:13:49.535 --> 02:13:52.455
which is usually some type of sentence or text.

2175
02:13:52.515 --> 02:13:54.415
In this case, it's how are you doing today?

2176
02:13:54.435 --> 02:13:56.015
Or how are masks doing today?

2177
02:13:58.105 --> 02:14:01.285
Um, the impex has a certain percentage of the words

2178
02:14:01.465 --> 02:14:04.125
or tokens randomly selected and replaced with a mask.

2179
02:14:04.255 --> 02:14:06.085
Token does not always have

2180
02:14:06.085 --> 02:14:07.205
to be just the one, it could be many.

2181
02:14:07.625 --> 02:14:10.805
Um, this masking is done to create a situation

2182
02:14:10.805 --> 02:14:12.685
where the model must predict mask words

2183
02:14:12.695 --> 02:14:14.045
based on the surrounding context.

2184
02:14:15.455 --> 02:14:19.675
Um, that objective of this process is

2185
02:14:19.935 --> 02:14:21.475
to model, um,

2186
02:14:22.215 --> 02:14:25.395
and predict the original identities of that mask.

2187
02:14:25.485 --> 02:14:29.395
Token is the original identity you, is it they, is it your,

2188
02:14:30.095 --> 02:14:31.675
that's what Bert's job is.

2189
02:14:33.485 --> 02:14:36.105
Um, this task inherently is bidirectional

2190
02:14:36.105 --> 02:14:38.945
because, uh, the model must consider both the left

2191
02:14:39.005 --> 02:14:42.545
and the right context to accurately predict the, um,

2192
02:14:43.925 --> 02:14:45.785
uh, the output, the missing words.

2193
02:14:46.205 --> 02:14:48.385
So that's why Bert is bidirectional,

2194
02:14:50.445 --> 02:14:51.545
uh, for the training.

2195
02:14:52.245 --> 02:14:53.945
The model is presented

2196
02:14:53.945 --> 02:14:56.705
with sequences containing mask tokens,

2197
02:14:56.725 --> 02:14:59.345
and it learns to generate the correct tokens

2198
02:14:59.685 --> 02:15:01.305
by optimizing a loss function.

2199
02:15:01.635 --> 02:15:05.185
Y'all remember loss functions, we talked about it eons ago

2200
02:15:05.645 --> 02:15:07.745
and, you know, 50 or so slides ago.

2201
02:15:10.045 --> 02:15:14.145
Um, so using that loss function, it will optimize, uh,

2202
02:15:14.445 --> 02:15:18.225
the training process and, uh, identify the discrepancy

2203
02:15:18.225 --> 02:15:20.945
of the predicted probabilities, uh, for the mask tokens

2204
02:15:20.945 --> 02:15:21.945
and the actual tokens.

2205
02:15:22.805 --> 02:15:25.145
Um, and then for fine tuning downstream tasks,

2206
02:15:26.365 --> 02:15:29.275
after that pre-training process on, um,

2207
02:15:29.695 --> 02:15:33.715
the mass language model task, a model can be fine tuned

2208
02:15:35.115 --> 02:15:38.015
on a specific downstream task like text classification,

2209
02:15:38.145 --> 02:15:40.735
named entity recognition, uh, things like that

2210
02:15:41.195 --> 02:15:42.615
to leverage knowledge gained

2211
02:15:42.795 --> 02:15:45.375
during the mass language, um, modeling phase.

2212
02:15:46.385 --> 02:15:50.245
Um, so we start with this kind

2213
02:15:50.245 --> 02:15:51.485
of general Bert model,

2214
02:15:52.105 --> 02:15:54.245
and then we leverage, um,

2215
02:15:55.875 --> 02:15:58.865
the specifics.

2216
02:15:59.525 --> 02:16:01.145
Uh, when we're tasked with,

2217
02:16:01.525 --> 02:16:03.345
are we interested in text classification?

2218
02:16:03.365 --> 02:16:05.745
Are we interested in named entity recognition, uh,

2219
02:16:05.925 --> 02:16:09.545
et cetera, et cetera, to improve and fine tune the model.

2220
02:16:14.555 --> 02:16:18.445
Alright, now what's the other side of Bert?

2221
02:16:18.995 --> 02:16:21.885
Next sentence prediction. NSP.

2222
02:16:23.775 --> 02:16:27.435
So in this example, Assan likes cookies.

2223
02:16:27.975 --> 02:16:31.925
Do you like them? And Bert

2224
02:16:32.965 --> 02:16:34.485
d uh, determines classification.

2225
02:16:34.865 --> 02:16:37.165
Yes. Sentence B follows sentence A.

2226
02:16:38.235 --> 02:16:39.855
So, next sentence prediction.

2227
02:16:41.145 --> 02:16:43.725
Um, the goal in general is to train a model

2228
02:16:44.065 --> 02:16:46.125
to predict whether a given pair

2229
02:16:46.125 --> 02:16:48.245
of consecutive sentences in a corpus

2230
02:16:48.875 --> 02:16:52.165
follows each other logically or if they're randomly paired.

2231
02:16:53.565 --> 02:16:56.745
Um, so for a sentence, pair construction

2232
02:16:57.985 --> 02:17:01.535
pairs in a consecutive sentence are randomly sampled

2233
02:17:02.175 --> 02:17:03.305
from a large corpus.

2234
02:17:04.125 --> 02:17:08.025
For each pair, there's at least 50% chance that the next,

2235
02:17:08.605 --> 02:17:10.265
uh, that the second sentence in

2236
02:17:10.265 --> 02:17:13.105
that pair is actually the next sentence, um,

2237
02:17:13.105 --> 02:17:14.465
that follows the first one.

2238
02:17:15.775 --> 02:17:18.155
Um, and there's a 50% chance that it doesn't,

2239
02:17:19.845 --> 02:17:21.745
um, for labeling.

2240
02:17:22.945 --> 02:17:25.475
Once that process is the sentence for construction is done,

2241
02:17:25.895 --> 02:17:28.995
um, the model is then trained to predict whether

2242
02:17:28.995 --> 02:17:31.755
or not the next sentence is indeed the next sentence,

2243
02:17:32.335 --> 02:17:34.355
or that it logically follows the first one.

2244
02:17:34.775 --> 02:17:38.115
If it is the next sentence, then it's labeled as is next,

2245
02:17:38.415 --> 02:17:39.595
or binary one.

2246
02:17:40.215 --> 02:17:43.235
Um, or if it's a random sentence, it's not next

2247
02:17:43.295 --> 02:17:44.635
or binary zero.

2248
02:17:46.265 --> 02:17:50.965
Um, and that objective is training the model.

2249
02:17:51.465 --> 02:17:55.485
Um, it's optimized to correctly classify, uh, yes

2250
02:17:55.505 --> 02:17:57.125
or no to next sentence.

2251
02:17:57.465 --> 02:17:59.605
And that loss function that is calculated

2252
02:17:59.705 --> 02:18:02.965
by the correct yes no prediction, uh,

2253
02:18:02.965 --> 02:18:06.005
measures the discrepancy between the predicted probabilities

2254
02:18:06.145 --> 02:18:07.565
and the true false labels.

2255
02:18:09.075 --> 02:18:11.095
Um, learning contextual representations

2256
02:18:11.675 --> 02:18:14.775
by training on the next sentence, prediction task.

2257
02:18:15.635 --> 02:18:18.135
The model learns to understand the relationships

2258
02:18:18.135 --> 02:18:20.615
between the sentences, um,

2259
02:18:22.115 --> 02:18:23.365
between the sentences

2260
02:18:26.165 --> 02:18:29.345
and the, um, rest of the model.

2261
02:18:36.255 --> 02:18:39.915
And for fine tuning as we get further down, um,

2262
02:18:40.205 --> 02:18:43.935
after pre-training on tasks like next sentence,

2263
02:18:43.935 --> 02:18:47.055
pre prediction, um, we can fine tune

2264
02:18:47.795 --> 02:18:52.615
on specific downstream te tasks, uh, like in MLM,

2265
02:18:53.235 --> 02:18:56.735
tax test, classification, named entity recognition,

2266
02:18:57.035 --> 02:18:58.295
and lots of other stuff.

2267
02:19:04.515 --> 02:19:06.685
Alright, let's get through this last slide

2268
02:19:06.685 --> 02:19:07.725
and then we'll take the break.

2269
02:19:08.315 --> 02:19:09.725
This slide should be pretty quick actually.

2270
02:19:10.265 --> 02:19:12.285
So, um, this just kind of demonstrates

2271
02:19:12.945 --> 02:19:15.005
how huge Bert is in general.

2272
02:19:16.585 --> 02:19:20.445
Uh, so Bert oftentimes is fit on.

2273
02:19:20.505 --> 02:19:23.005
Um, I think our download in the code, um,

2274
02:19:23.945 --> 02:19:26.645
is like one over one gigabyte.

2275
02:19:27.145 --> 02:19:30.605
Uh, so for a model that's largely string based, um,

2276
02:19:30.745 --> 02:19:35.205
and just general parameters and vectors, um, that's huge.

2277
02:19:35.685 --> 02:19:37.365
A gigabyte that's massive.

2278
02:19:37.415 --> 02:19:39.445
Think about an Excel sheet that's a gigabyte.

2279
02:19:40.655 --> 02:19:43.235
Um, lots and lots and lots of rows and columns.

2280
02:19:44.885 --> 02:19:48.545
Um, and this is much more compressed than a Excel sheet.

2281
02:19:50.045 --> 02:19:53.265
Um, so there's Burt base that's about 10, uh,

2282
02:19:53.365 --> 02:19:55.585
or 110 million parameters.

2283
02:19:55.605 --> 02:19:57.505
You can think of parameters as like columns.

2284
02:19:58.325 --> 02:20:02.265
Um, Burt Large has three times that.

2285
02:20:03.595 --> 02:20:07.095
Um, and there's Burt, tiny, Burt, mini, Burt, small, Burt,

2286
02:20:07.095 --> 02:20:08.415
medium, lots of different Burts.

2287
02:20:08.795 --> 02:20:11.735
Um, and this is the size complexity of each one of those.

2288
02:20:12.155 --> 02:20:15.665
So l here, oh, lemme get the pen out.

2289
02:20:15.845 --> 02:20:20.805
Um, l here is, uh, the number of encoder layers,

2290
02:20:21.625 --> 02:20:24.565
and h is the number of embedding dimensions.

2291
02:20:25.935 --> 02:20:30.215
Um, so as we go from tiny

2292
02:20:30.915 --> 02:20:33.735
to base, uh, we increase in size.

2293
02:20:35.745 --> 02:20:40.285
Um, so the, the main goal here is, um,

2294
02:20:40.625 --> 02:20:41.805
use the right tool for the job.

2295
02:20:42.535 --> 02:20:45.525
Don't use a huge one when you have a small problem

2296
02:20:45.865 --> 02:20:48.085
or don't use a tiny one when you have a huge problem.

2297
02:20:48.665 --> 02:20:51.205
Um, and you can apply many different ways to,

2298
02:20:51.305 --> 02:20:52.645
um, optimize your model.

2299
02:20:52.705 --> 02:20:55.125
And I recommend doing that for each, uh, model.

2300
02:21:01.105 --> 02:21:02.445
Uh, sorry, I'm just reading the question.

2301
02:21:02.865 --> 02:21:04.525
Uh, correct in drawing a correlation

2302
02:21:04.525 --> 02:21:06.125
between the continuous back of words

2303
02:21:06.125 --> 02:21:08.685
and the way we use, uh, bird utilize maps.

2304
02:21:09.085 --> 02:21:10.565
I, I think there's certainly a correlation.

2305
02:21:11.265 --> 02:21:15.045
Um, looks like Rosh is, uh, also answering that question.

2306
02:21:15.825 --> 02:21:19.975
Um, so that may be more helpful, uh,

2307
02:21:20.105 --> 02:21:23.295
while we get towards our break.

2308
02:21:24.175 --> 02:21:25.595
Uh, so good work everybody.

2309
02:21:26.175 --> 02:21:28.475
Uh, we made it to the next break.

2310
02:21:29.415 --> 02:21:33.355
Um, it is 1133 Pacific.

2311
02:21:34.665 --> 02:21:39.125
Um, so that will put us back at, uh,

2312
02:21:39.185 --> 02:21:41.205
1143 Pacific.

2313
02:21:42.435 --> 02:21:44.295
Um, so I will see you all in 10 minutes.

2314
02:21:55.765 --> 02:21:59.625
All right, everybody, uh, let's get started again.

2315
02:22:01.885 --> 02:22:05.665
Um, so we are back together.

2316
02:22:06.405 --> 02:22:09.105
We just finished talking about Burt in theory.

2317
02:22:09.445 --> 02:22:12.585
Now we are going to talk about Burt in practice.

2318
02:22:14.065 --> 02:22:16.685
Um, and here I am now standing.

2319
02:22:19.625 --> 02:22:22.745
Um, okay, here's the code book.

2320
02:22:27.145 --> 02:22:28.515
Everybody should be able to see.

2321
02:22:29.495 --> 02:22:32.915
Um, actually it looks like I need to reshare

2322
02:22:41.265 --> 02:22:42.445
screen four.

2323
02:22:47.745 --> 02:22:49.805
All right, can everybody see the, uh,

2324
02:22:50.325 --> 02:22:51.885
Bert examples, pre-trained model?

2325
02:23:00.375 --> 02:23:04.425
All right, thank you, Benjamin. Uh,

2326
02:23:05.045 --> 02:23:08.855
the thank you, uh, that, like, if I made it all the way

2327
02:23:08.855 --> 02:23:12.335
through this and then nobody could see the code.

2328
02:23:13.165 --> 02:23:16.055
Okay, so this is a really great code book to demonstrate.

2329
02:23:16.315 --> 02:23:20.975
Uh, Bert Bert does take, uh, quite a bit of time to run.

2330
02:23:21.435 --> 02:23:24.215
So I've, um, done the run beforehand.

2331
02:23:24.795 --> 02:23:26.775
Um, and if we have some time, we can, um,

2332
02:23:27.205 --> 02:23:30.615
look at each individually, um, and rerun.

2333
02:23:30.875 --> 02:23:32.935
But, um, Bert is a huge model,

2334
02:23:33.115 --> 02:23:37.495
so it does actually have some, um, computational time, uh,

2335
02:23:37.565 --> 02:23:39.535
that would not be great during the demo.

2336
02:23:41.955 --> 02:23:44.535
Um, all right, let's zoom in a little bit.

2337
02:23:48.155 --> 02:23:50.575
Thanks for, uh, asking. Is that better?

2338
02:23:56.955 --> 02:23:59.255
All right, cool. Thank you for prompting.

2339
02:23:59.435 --> 02:24:04.055
Um, this is actually displayed on a huge screen, uh, for me,

2340
02:24:04.275 --> 02:24:07.765
so, um, it might show up small on the screen.

2341
02:24:07.765 --> 02:24:09.885
Share. I, okay, cool.

2342
02:24:10.865 --> 02:24:14.355
So just going

2343
02:24:14.355 --> 02:24:15.915
through our favorite acronym,

2344
02:24:15.975 --> 02:24:19.115
Bi-Directional Encoder Representations from Transformers.

2345
02:24:19.305 --> 02:24:21.435
It's based on Transformers deep learning model,

2346
02:24:21.435 --> 02:24:24.435
which we've learned about, um, and deep learning

2347
02:24:24.435 --> 02:24:27.795
because it does have that, uh, feedback loop, uh,

2348
02:24:27.795 --> 02:24:31.915
pre-training, um, to, um, the next application model.

2349
02:24:32.975 --> 02:24:36.235
Uh, each output element is connected into the input element,

2350
02:24:36.575 --> 02:24:37.635
and the weightings

2351
02:24:37.635 --> 02:24:39.835
between them are dynamically calculated

2352
02:24:39.965 --> 02:24:41.075
based on their connection.

2353
02:24:41.695 --> 02:24:43.075
Um, it's designed to have

2354
02:24:43.075 --> 02:24:45.635
that pre-trained deep bi-directional representation

2355
02:24:46.185 --> 02:24:49.935
from unlabeled text jointly, uh,

2356
02:24:49.935 --> 02:24:51.735
conditioning the left and the right context.

2357
02:24:54.015 --> 02:24:56.745
Uh, all right, let's zoom in a little bit more.

2358
02:25:01.955 --> 02:25:05.095
Um, so implementing the pre-trained Bert model

2359
02:25:05.115 --> 02:25:07.335
for the following tasks, quest

2360
02:25:07.675 --> 02:25:09.815
or text summarization is what we're gonna talk about first,

2361
02:25:09.915 --> 02:25:12.215
and then it's gonna follow by question and answering system.

2362
02:25:13.245 --> 02:25:14.905
Um, definitely prompt me in the chat.

2363
02:25:15.125 --> 02:25:17.745
I'm looking at it in another screen if you can't see.

2364
02:25:18.325 --> 02:25:21.745
Um, let me f 11 this, so it's a little bit better.

2365
02:25:26.055 --> 02:25:29.035
Uh, can I minimize this? Yes.

2366
02:25:30.955 --> 02:25:34.475
All right. Um, so

2367
02:25:35.395 --> 02:25:37.875
remember this is pre-run to avoid delays.

2368
02:25:38.215 --> 02:25:42.795
Um, so here we are installing the, uh, required libraries.

2369
02:25:43.175 --> 02:25:44.595
And if you haven't used CoLab

2370
02:25:44.595 --> 02:25:46.835
before, it's fairly new, super great.

2371
02:25:47.215 --> 02:25:50.555
Um, but it uses a lot of, uh, terminal commands.

2372
02:25:50.775 --> 02:25:54.115
So here we just have PIP install, um, Bert, uh,

2373
02:25:54.205 --> 02:25:55.675
extractive summarizer.

2374
02:25:55.695 --> 02:25:57.075
So this is that Burt Library.

2375
02:25:58.915 --> 02:26:01.415
Um, and you can see, um, it checks a lot

2376
02:26:01.415 --> 02:26:03.735
of Burt different requirements for previous packages.

2377
02:26:04.875 --> 02:26:08.895
Um, and then we import the Bert Summarizer.

2378
02:26:09.235 --> 02:26:12.335
Um, and that actually takes quite a bit of time, um,

2379
02:26:12.555 --> 02:26:14.415
has nice progress bars that you can see.

2380
02:26:15.535 --> 02:26:16.995
Um, but we use that.

2381
02:26:17.575 --> 02:26:21.035
Um, again, Bert conveys core information of original text

2382
02:26:21.035 --> 02:26:23.195
through the shortest text possible.

2383
02:26:24.015 --> 02:26:26.235
Um, here we're gonna do some sentence summary,

2384
02:26:26.335 --> 02:26:29.715
and they're generated by the model, not just extracted.

2385
02:26:29.735 --> 02:26:30.995
So this is generative.

2386
02:26:32.215 --> 02:26:36.315
Um, so now let's summarize text with Bert.

2387
02:26:37.655 --> 02:26:41.155
So first, load the model, um, kind of similar

2388
02:26:41.215 --> 02:26:42.635
to like linear regression

2389
02:26:42.635 --> 02:26:44.835
and all of the other psychic learn packages.

2390
02:26:44.855 --> 02:26:47.395
We just declare a summarizer, uh,

2391
02:26:47.455 --> 02:26:49.355
to burnt model, uh, variable.

2392
02:26:51.655 --> 02:26:55.695
Um, and second step, uh,

2393
02:26:55.795 --> 02:26:57.375
to generate and print out the summary.

2394
02:26:58.075 --> 02:26:59.775
Uh, we have the original text.

2395
02:27:00.755 --> 02:27:04.485
Um, for parameters, we have the model, um, which is

2396
02:27:04.485 --> 02:27:06.835
that Bert summary model, uh, text.

2397
02:27:06.895 --> 02:27:10.625
We have the original text, which is down here, Um,

2398
02:27:10.925 --> 02:27:13.865
min length notes, minimum length of the summary.

2399
02:27:14.235 --> 02:27:18.745
Great variable name, number of sentences is number

2400
02:27:18.745 --> 02:27:20.745
of sentences that'll be available in the summary.

2401
02:27:22.995 --> 02:27:27.495
Uh, so we have text, we have min length, um,

2402
02:27:27.995 --> 02:27:29.055
number sentences.

2403
02:27:29.595 --> 02:27:32.495
Um, and then that goes into the Bert model function,

2404
02:27:32.495 --> 02:27:36.545
which is, uh, the summarizer, um, that

2405
02:27:37.085 --> 02:27:39.305
we then print, uh, Bert summary.

2406
02:27:40.655 --> 02:27:44.375
Um, and that's run. We have, uh, so that's just the model.

2407
02:27:44.795 --> 02:27:47.815
Um, notice, so this is just a, a python thing.

2408
02:27:47.875 --> 02:27:49.735
Uh, if you're not super aware of Python.

2409
02:27:50.355 --> 02:27:53.255
So here we're, uh, just declaring the model.

2410
02:27:53.955 --> 02:27:55.695
Um, we are not calling the model.

2411
02:27:56.805 --> 02:28:00.465
So, um, once we declare the model here, um,

2412
02:28:00.575 --> 02:28:03.745
then we declare the text, natural language processing,

2413
02:28:03.765 --> 02:28:04.945
not gonna read that whole thing.

2414
02:28:05.565 --> 02:28:08.785
Um, but we have this, uh, whole block of text.

2415
02:28:09.915 --> 02:28:12.895
And then, um, we pass that through

2416
02:28:13.405 --> 02:28:16.775
this previously declared Jen text summary function.

2417
02:28:18.495 --> 02:28:21.885
Um, so we have model which we declared text,

2418
02:28:21.895 --> 02:28:25.925
which we declared minimum length is, uh, 60 number

2419
02:28:25.925 --> 02:28:29.045
of sentences is three, so we call that.

2420
02:28:29.745 --> 02:28:31.965
Um, and then it's just avoid function, uh,

2421
02:28:31.965 --> 02:28:34.045
which means it doesn't return anything, it just prints,

2422
02:28:34.505 --> 02:28:35.565
uh, or does the task.

2423
02:28:36.145 --> 02:28:40.125
Um, and you can see that it prints the summary.

2424
02:28:40.865 --> 02:28:45.605
Um, so it has this whole text, um, here

2425
02:28:46.555 --> 02:28:48.085
that I didn't wanna read at all,

2426
02:28:48.705 --> 02:28:52.625
and this whole summary, uh, with, um,

2427
02:28:52.985 --> 02:28:56.145
a significantly shorter, um, text

2428
02:28:56.215 --> 02:28:58.265
that is a little bit more approachable for reading.

2429
02:29:00.305 --> 02:29:02.965
Uh, so text summarization is, uh, a great use

2430
02:29:02.965 --> 02:29:03.965
of the burn model as well.

2431
02:29:04.905 --> 02:29:08.885
Um, so here we can summarize it into

2432
02:29:08.885 --> 02:29:10.285
two sentences, so it's even shorter.

2433
02:29:10.425 --> 02:29:12.525
And I actually might read this one. Here we go.

2434
02:29:12.595 --> 02:29:15.405
Natural language processing is an area of computer science

2435
02:29:15.405 --> 02:29:17.845
and artificial intelligence concerned with interaction

2436
02:29:17.845 --> 02:29:20.005
between human, uh, between computers

2437
02:29:20.345 --> 02:29:21.645
and humans in natural language.

2438
02:29:21.795 --> 02:29:24.445
Natural language processing has, uh, existed

2439
02:29:24.465 --> 02:29:27.245
for more than 50 years and has roots in the field of,

2440
02:29:27.625 --> 02:29:28.765
uh, linguistics.

2441
02:29:29.465 --> 02:29:32.325
Um, so that is a great summary.

2442
02:29:33.265 --> 02:29:36.205
Uh, from this, you can see it's pulled out some sentences,

2443
02:29:36.245 --> 02:29:38.445
like natural language processing has, uh, existed

2444
02:29:38.505 --> 02:29:40.805
for more than 50 years in brute field of linguistics.

2445
02:29:41.465 --> 02:29:45.005
Um, but like all of this kind of stuff in the middle here,

2446
02:29:45.585 --> 02:29:47.685
not super critical as defined by bird.

2447
02:29:48.225 --> 02:29:52.485
Um, and that's actually something really important, um,

2448
02:29:52.825 --> 02:29:55.365
in the work that I'm doing now, um,

2449
02:29:56.435 --> 02:29:58.485
with just summarizing models.

2450
02:29:58.945 --> 02:30:02.485
Um, so if we have like an org leader, for example,

2451
02:30:03.235 --> 02:30:06.165
that um, is talking to thousands

2452
02:30:06.165 --> 02:30:09.325
and thousands of their direct reports, their employees,

2453
02:30:10.255 --> 02:30:14.755
and they don't want to audit a million different emails, um,

2454
02:30:14.845 --> 02:30:15.995
about different opinions.

2455
02:30:16.215 --> 02:30:19.875
So we can take this overall text

2456
02:30:21.445 --> 02:30:22.545
of a million emails

2457
02:30:23.165 --> 02:30:26.825
and distill it into this much more readable

2458
02:30:26.845 --> 02:30:28.905
by somebody who's time limited, um,

2459
02:30:29.445 --> 02:30:30.985
interpretation and summary.

2460
02:30:31.845 --> 02:30:33.505
And that org leader

2461
02:30:33.505 --> 02:30:36.745
with a thousand direct reports can get general sentiment

2462
02:30:37.445 --> 02:30:40.465
of their overall employee experience.

2463
02:30:40.845 --> 02:30:44.745
And that's something that is, um, really valuable to a lot

2464
02:30:44.745 --> 02:30:47.385
of different org leaders and, uh, connected to a lot

2465
02:30:47.385 --> 02:30:49.385
of the work I'm doing currently at Microsoft.

2466
02:30:52.095 --> 02:30:56.945
Um, so that's summarization for Bert.

2467
02:30:57.505 --> 02:31:00.655
Question answering tasks, um,

2468
02:31:00.795 --> 02:31:02.575
that's a whole separate topic.

2469
02:31:03.755 --> 02:31:07.255
Um, here, input to the Bert model

2470
02:31:08.275 --> 02:31:10.615
is typically in question and paragraph form.

2471
02:31:11.395 --> 02:31:14.015
Um, Bert has to extract the answer

2472
02:31:14.195 --> 02:31:16.855
of the given question from that paragraph.

2473
02:31:17.705 --> 02:31:20.805
The input embeddings of that are, uh, the sum

2474
02:31:20.865 --> 02:31:22.125
of the token embeddings

2475
02:31:22.425 --> 02:31:25.485
and, um, involve some of the segment embeddings as well.

2476
02:31:26.705 --> 02:31:30.845
Um, token embeddings, the context token is added

2477
02:31:30.905 --> 02:31:33.325
to the input words at, uh,

2478
02:31:33.385 --> 02:31:34.805
the token at the beginning of the question.

2479
02:31:34.865 --> 02:31:38.245
And that separator, um, so that, um, period

2480
02:31:38.245 --> 02:31:42.845
between sentences token is inserted at the end of,

2481
02:31:43.065 --> 02:31:45.285
uh, both the question and the paragraph.

2482
02:31:46.545 --> 02:31:51.305
For segment embeddings, A marker indicating sequence A

2483
02:31:51.485 --> 02:31:54.465
or sequence B is added to each token.

2484
02:31:55.415 --> 02:31:59.315
Um, this allows the model to distinguish between sentences.

2485
02:32:00.215 --> 02:32:02.235
Um, and we can look at the diagram below.

2486
02:32:02.935 --> 02:32:05.995
So all those tokens are marked as a, uh,

2487
02:32:06.135 --> 02:32:08.795
for the first sentence and b for the second sentence.

2488
02:32:09.775 --> 02:32:11.915
So for the input question of

2489
02:32:12.055 --> 02:32:16.715
how many parameter does Bert large have, um, we have,

2490
02:32:19.005 --> 02:32:21.265
um, the question here

2491
02:32:21.405 --> 02:32:23.985
and then the reference with separator here.

2492
02:32:24.045 --> 02:32:26.505
So we have the separator preloaded if, uh,

2493
02:32:26.505 --> 02:32:29.105
there's a feature question or anything like that.

2494
02:32:29.765 --> 02:32:32.025
Um, and then the reference text,

2495
02:32:32.075 --> 02:32:34.145
which has been pre-trained on the model

2496
02:32:35.465 --> 02:32:36.845
is bur large is really big.

2497
02:32:36.845 --> 02:32:37.925
It has 24 layers

2498
02:32:37.985 --> 02:32:42.125
and embedding size of over a thousand for a total of, uh,

2499
02:32:42.495 --> 02:32:45.165
340 million parameters.

2500
02:32:46.195 --> 02:32:48.775
And that's over a gigabyte for just that model.

2501
02:32:49.435 --> 02:32:51.935
Um, so it's gonna take a couple minutes to download

2502
02:32:51.955 --> 02:32:54.815
to your collab instance, which is why we've done it already.

2503
02:32:57.255 --> 02:33:02.115
Um, so first of all, we, uh, to do the rest

2504
02:33:02.115 --> 02:33:04.955
of this demo, we're installing all the required libraries

2505
02:33:05.095 --> 02:33:06.835
for, uh, question answering tasks.

2506
02:33:07.935 --> 02:33:12.275
Um, and then we, um, upgrade those in informers

2507
02:33:12.415 --> 02:33:14.675
and, uh, all of the earth transformers.

2508
02:33:15.465 --> 02:33:17.285
So we have all these packages installed,

2509
02:33:17.705 --> 02:33:20.565
and I believe you're provided this, uh, demo afterwards

2510
02:33:20.585 --> 02:33:22.885
so you can, uh, do it on your own instance.

2511
02:33:23.705 --> 02:33:27.025
Um, importing the required libraries,

2512
02:33:27.025 --> 02:33:28.545
we have Bert for question answer.

2513
02:33:28.895 --> 02:33:30.385
This is a nice feature of CoLab.

2514
02:33:30.385 --> 02:33:33.185
We get, um, the, um,

2515
02:33:33.585 --> 02:33:35.705
documentation right in the tool tip there when you

2516
02:33:35.705 --> 02:33:36.745
hover over that text.

2517
02:33:37.715 --> 02:33:39.935
And then we have Burt Tokenize, um,

2518
02:33:40.035 --> 02:33:42.495
and then Torch, which is a great package.

2519
02:33:43.975 --> 02:33:48.325
Uh, and then the model in general, um, Burt,

2520
02:33:48.325 --> 02:33:52.525
large Uncased word masking fine tuned squad, uh,

2521
02:33:52.675 --> 02:33:57.075
that whole large, um, descriptor, uh,

2522
02:33:57.105 --> 02:33:59.595
it's a pre-trained on the English language using

2523
02:34:00.095 --> 02:34:01.675
masked, uh, language modeling.

2524
02:34:02.015 --> 02:34:03.955
So everybody remember mask language modeling.

2525
02:34:04.705 --> 02:34:07.995
That was an exciting, uh, uh, slide.

2526
02:34:09.195 --> 02:34:12.535
Um, so this model is not cased

2527
02:34:12.755 --> 02:34:14.935
and it does not make difference between, uh,

2528
02:34:14.935 --> 02:34:16.375
English and English.

2529
02:34:16.985 --> 02:34:21.685
Um, and surprisingly that has caused me a lot of headache.

2530
02:34:21.985 --> 02:34:26.245
Um, so it's good that that's already done for you.

2531
02:34:26.665 --> 02:34:28.325
Um, you know, you haven't been in code

2532
02:34:28.325 --> 02:34:31.285
unless you've, uh, wasted an entire day

2533
02:34:31.285 --> 02:34:34.725
because of, uh, bad case, uh, management.

2534
02:34:34.985 --> 02:34:36.685
So it's good that it's already done it for you.

2535
02:34:37.995 --> 02:34:40.905
Um, so differently to other Burt models.

2536
02:34:41.335 --> 02:34:43.225
This was trained using a new technique,

2537
02:34:43.225 --> 02:34:45.265
which is whole word masking.

2538
02:34:46.365 --> 02:34:49.425
Um, so in that case, all the tokens corresponding

2539
02:34:49.425 --> 02:34:50.585
to a word are master all at once.

2540
02:34:51.365 --> 02:34:55.085
Um, the overall masking rate remains the same,

2541
02:34:55.825 --> 02:34:57.245
but in this case,

2542
02:34:57.675 --> 02:35:01.325
each mask word piece token is predicted independently.

2543
02:35:02.305 --> 02:35:07.005
Um, and then after pre-training, um, it has been fine tuned.

2544
02:35:07.545 --> 02:35:11.925
Um, this specific, uh, bur large case on hold bird masking,

2545
02:35:13.595 --> 02:35:14.815
uh, it's been fine tuned

2546
02:35:14.875 --> 02:35:17.455
by the squad dataset on the squad dataset.

2547
02:35:17.835 --> 02:35:20.855
Uh, so we have this specific squad dataset that we have used

2548
02:35:20.855 --> 02:35:22.935
to improve and fine tune our model.

2549
02:35:23.995 --> 02:35:25.565
Um, and

2550
02:35:25.775 --> 02:35:28.565
after that fine tuning, we've got 24 layers

2551
02:35:29.115 --> 02:35:32.725
over a thousand hidden dimensions, 16 attention heads, um,

2552
02:35:33.225 --> 02:35:37.405
and just over 300 million parameters, um,

2553
02:35:37.535 --> 02:35:39.525
which is slightly different than what we had

2554
02:35:39.525 --> 02:35:44.045
before in the base model, which is, uh, uh, 340 million.

2555
02:35:48.395 --> 02:35:50.055
So slightly simplified here.

2556
02:35:56.325 --> 02:36:00.755
Um, so loading the model Bert,

2557
02:36:02.185 --> 02:36:05.735
uh, model here, uh, from pre-trained

2558
02:36:06.395 --> 02:36:09.615
the uncased forward masking fine tuned squad.

2559
02:36:10.235 --> 02:36:11.695
See how long this takes to run.

2560
02:36:14.815 --> 02:36:15.835
All right, already run, look at that.

2561
02:36:18.505 --> 02:36:20.735
So we can see the output here.

2562
02:36:22.305 --> 02:36:24.565
Um, this is mostly just initializing it,

2563
02:36:25.195 --> 02:36:27.405
loading the tokenize, which we've already done.

2564
02:36:29.305 --> 02:36:30.965
Um, and then declaring the function.

2565
02:36:31.185 --> 02:36:33.285
So what this function will do is it'll be helpful

2566
02:36:33.385 --> 02:36:35.365
for finding the answer to a given question

2567
02:36:35.955 --> 02:36:37.245
from a given paragraph.

2568
02:36:38.875 --> 02:36:41.255
Um, so remember that example in code

2569
02:36:41.675 --> 02:36:44.295
or in, uh, the lecture slides where we're talking about

2570
02:36:44.625 --> 02:36:47.095
where does a question start, where does a question end?

2571
02:36:48.565 --> 02:36:52.105
Um, so we use the function in code plus

2572
02:36:53.015 --> 02:36:54.315
to encode that sequence.

2573
02:36:55.065 --> 02:36:59.515
That function returns a dictionary that contains input IDs,

2574
02:36:59.565 --> 02:37:02.595
token type IDs, uh, and the attention mask.

2575
02:37:03.175 --> 02:37:05.875
Um, but we only need input IDs

2576
02:37:06.175 --> 02:37:09.275
and token type IDs for, uh, the quality assurance task.

2577
02:37:11.635 --> 02:37:14.895
So here we're declaring the encoding variable

2578
02:37:15.435 --> 02:37:17.285
with this function in code plus.

2579
02:37:18.145 --> 02:37:20.045
Um, and then we have the question,

2580
02:37:20.305 --> 02:37:22.885
and then, um, the text pair, uh, question

2581
02:37:23.065 --> 02:37:25.555
or text pair text, um,

2582
02:37:25.575 --> 02:37:28.555
and then we have our inputs in the token embedding.

2583
02:37:28.555 --> 02:37:29.435
And then we have our segment

2584
02:37:29.435 --> 02:37:30.835
embeddings, like we were talking about before.

2585
02:37:31.535 --> 02:37:33.555
And then we have our input tokens.

2586
02:37:35.205 --> 02:37:39.105
Um, so we have, from there, we declare start scores

2587
02:37:39.125 --> 02:37:42.305
and end scores using the, uh, model input.

2588
02:37:44.125 --> 02:37:48.585
Um, and we use that to pass parameters, uh,

2589
02:37:48.695 --> 02:37:50.545
like the token embedding, segment embedding

2590
02:37:50.735 --> 02:37:52.105
that are declared up here.

2591
02:37:53.835 --> 02:37:56.895
Um, and then we use the arg max function

2592
02:37:56.895 --> 02:37:58.735
to get the start index of the answer

2593
02:37:59.855 --> 02:38:01.635
and then the end index of the answer.

2594
02:38:01.975 --> 02:38:05.075
So remember, um, if you have,

2595
02:38:05.805 --> 02:38:10.715
let's find an example here, um, in this big block of text,

2596
02:38:11.495 --> 02:38:13.915
the question is, what is machine learning?

2597
02:38:14.775 --> 02:38:18.715
And then this is the return, uh, text.

2598
02:38:19.615 --> 02:38:22.595
Um, so it's important for us to be able to identify

2599
02:38:23.145 --> 02:38:27.835
what section of a user prompt is the actual question.

2600
02:38:28.375 --> 02:38:31.475
So that's what we mean when we have the start index

2601
02:38:31.655 --> 02:38:34.515
and the end index of the question

2602
02:38:34.615 --> 02:38:36.915
and the answer you ever talking about the answer.

2603
02:38:38.335 --> 02:38:42.355
Um, and then we have finding the answer from a given

2604
02:38:42.355 --> 02:38:44.195
paragraph using the start and end index.

2605
02:38:45.055 --> 02:38:48.075
Um, and then here we have recover any potential words

2606
02:38:48.075 --> 02:38:50.915
that were broken down into subs, um,

2607
02:38:50.915 --> 02:38:54.015
finally storing the correct answer, um,

2608
02:38:54.395 --> 02:38:55.735
and then printing it out.

2609
02:38:56.575 --> 02:38:58.315
Um, this is just a, a bucketing thing.

2610
02:38:58.735 --> 02:39:01.595
Um, and then we print out the final, uh,

2611
02:39:01.595 --> 02:39:03.035
correct answer at the end.

2612
02:39:04.535 --> 02:39:07.635
Um, so here at the end of the day,

2613
02:39:07.635 --> 02:39:09.355
we just have this simple output, right?

2614
02:39:10.375 --> 02:39:14.635
So the simple output, once we, uh, call the function up here

2615
02:39:15.265 --> 02:39:19.035
with the input of question and the output of paragraph

2616
02:39:20.295 --> 02:39:24.505
or the, uh, reference text paragraph, it's just, um,

2617
02:39:25.555 --> 02:39:28.365
user asks, what is machine

2618
02:39:28.525 --> 02:39:31.315
learning question is question here.

2619
02:39:32.185 --> 02:39:36.445
And paragraph is, um, that's the reference text.

2620
02:39:37.255 --> 02:39:41.995
Um, so the final corrected answer is going to be

2621
02:39:42.655 --> 02:39:45.755
the scientific study of algorithms and statistical models.

2622
02:39:48.105 --> 02:39:51.605
Um, so let's see if we can find this text

2623
02:39:53.735 --> 02:39:55.435
or this answer in that text.

2624
02:39:56.435 --> 02:40:01.005
So there we go. Machine learning is the scientific study

2625
02:40:01.065 --> 02:40:03.005
of algorithms and statistical models.

2626
02:40:03.465 --> 02:40:07.005
So considering the org leader example, um,

2627
02:40:07.055 --> 02:40:09.485
where we have this super important person

2628
02:40:09.635 --> 02:40:12.245
with a million direct reports, um,

2629
02:40:12.585 --> 02:40:15.855
and they are a very busy person

2630
02:40:16.355 --> 02:40:19.015
and they don't wanna read this entire block of text

2631
02:40:19.045 --> 02:40:20.215
because they look at it

2632
02:40:20.215 --> 02:40:22.055
and they're like, Ugh, that seems like a lot of work.

2633
02:40:22.755 --> 02:40:25.975
So you can deliver them something that is this, um,

2634
02:40:28.205 --> 02:40:30.195
model that simplifies.

2635
02:40:30.255 --> 02:40:32.555
So all they have to do is ask a question

2636
02:40:32.655 --> 02:40:36.235
and they get the answer from this overall corpus of text.

2637
02:40:36.975 --> 02:40:41.275
So this, uh, when scaled becomes something like chat two PT

2638
02:40:41.275 --> 02:40:42.955
where you can ask it pretty much anything

2639
02:40:42.955 --> 02:40:44.315
because it's paragraph

2640
02:40:44.495 --> 02:40:48.075
or corpus that it references is the entire internet.

2641
02:40:54.835 --> 02:40:57.135
So that is that code book.

2642
02:40:57.355 --> 02:41:01.135
Um, one thing that you can do to kind of like test case,

2643
02:41:01.165 --> 02:41:03.535
this is, uh, replace question

2644
02:41:03.595 --> 02:41:05.615
and paragraph with different prompts

2645
02:41:05.615 --> 02:41:07.655
and different response, uh, books.

2646
02:41:07.715 --> 02:41:09.655
So you can see how it changes over time

2647
02:41:10.155 --> 02:41:11.695
and how it changes with different inputs.

2648
02:41:12.395 --> 02:41:15.925
Um, but let me switch back over

2649
02:41:16.065 --> 02:41:17.565
to the lecture slides quickly.

2650
02:41:20.185 --> 02:41:21.405
Any questions on the code book

2651
02:41:21.405 --> 02:41:24.405
before we, uh, do the final switch back,

2652
02:41:24.825 --> 02:41:27.525
or not final switch until the next code book?

2653
02:41:41.465 --> 02:41:45.775
All right, thank you, Benjamin. Oh, look at that.

2654
02:41:45.815 --> 02:41:47.995
I didn't even have to resume.

2655
02:41:48.895 --> 02:41:53.115
Um, all right, let's keep rolling.

2656
02:42:01.615 --> 02:42:05.135
Let me just get the notes here.

2657
02:42:08.105 --> 02:42:10.885
Um, and here we're gonna look at

2658
02:42:12.735 --> 02:42:15.995
in addition to Bert, which has been, um,

2659
02:42:17.305 --> 02:42:18.685
really popular for a long time,

2660
02:42:19.375 --> 02:42:21.005
there have been improvements.

2661
02:42:22.225 --> 02:42:26.005
So, so on top of Bert, we now have Roberta

2662
02:42:26.875 --> 02:42:29.445
developed by Facebook, uh,

2663
02:42:29.565 --> 02:42:32.805
robustly optimized Bert pre-training approach, um,

2664
02:42:32.855 --> 02:42:35.605
which is simply, uh, put in statistic terms,

2665
02:42:35.915 --> 02:42:40.725
it's a bootstrapping, uh, improvement of, um, Bert.

2666
02:42:41.515 --> 02:42:43.775
And then we have Albert, which is a

2667
02:42:44.015 --> 02:42:45.655
simplified version of Bert.

2668
02:42:46.155 --> 02:42:49.815
Um, it takes effectively all of the good with minimal

2669
02:42:49.915 --> 02:42:52.045
of the bad, um,

2670
02:42:52.425 --> 02:42:55.085
and makes it a much more lightweight algorithm.

2671
02:42:55.465 --> 02:42:58.165
And then we have electro, which is efficiently learning

2672
02:42:58.265 --> 02:43:01.045
and encoder that classifies tokens, uh,

2673
02:43:01.045 --> 02:43:02.325
replacements accurately.

2674
02:43:02.745 --> 02:43:07.445
So said simply it, um, determines, uh, correctness, um,

2675
02:43:07.705 --> 02:43:10.365
and whether or not an item was replaced or original.

2676
02:43:11.225 --> 02:43:14.045
Um, but in a practical sense,

2677
02:43:14.605 --> 02:43:17.645
I would say we're 90% of the time going

2678
02:43:17.645 --> 02:43:19.765
to be using in our day-to-day work.

2679
02:43:20.105 --> 02:43:23.245
Uh, Roberta and Albert, um,

2680
02:43:24.685 --> 02:43:26.005
Electra could be used occasionally,

2681
02:43:26.265 --> 02:43:29.005
but it's a much more, uh, advanced nuance algorithm.

2682
02:43:29.905 --> 02:43:33.925
Um, in my personal work, uh, I would choose probably Albert

2683
02:43:34.145 --> 02:43:35.485
for most of the use cases,

2684
02:43:35.785 --> 02:43:37.005
and if I'm not getting good fitting,

2685
02:43:37.225 --> 02:43:39.285
I'd probably hit Albert or Roberta.

2686
02:43:39.745 --> 02:43:40.925
Um, and then maybe Bert

2687
02:43:40.925 --> 02:43:44.405
after that, uh, depending on a couple of cases that we're,

2688
02:43:44.405 --> 02:43:46.685
we'll talk about a in a second here.

2689
02:43:53.855 --> 02:43:57.795
All right. Uh, so for Roberta, um,

2690
02:43:58.265 --> 02:44:02.815
Roberta is an important variation of Bert, um,

2691
02:44:02.915 --> 02:44:07.815
in a short for robust optimized Bert pre-training approach.

2692
02:44:09.155 --> 02:44:11.455
So this variant of Bert, while

2693
02:44:11.975 --> 02:44:16.175
maintaining the foundational architecture of Bert, um,

2694
02:44:16.225 --> 02:44:18.535
introduces notable modifications

2695
02:44:19.275 --> 02:44:20.895
to the pre-training procedure

2696
02:44:21.435 --> 02:44:23.255
and enhances the model's capabilities.

2697
02:44:24.925 --> 02:44:28.105
Um, Bert does have some disadvantages with under training,

2698
02:44:29.465 --> 02:44:31.565
and that's been identified through lots

2699
02:44:31.565 --> 02:44:33.645
of different researchers, um,

2700
02:44:33.905 --> 02:44:37.005
and data scientists looking at the Burt model, uh, in depth.

2701
02:44:37.745 --> 02:44:42.565
Um, and they discerned a potential suboptimal

2702
02:44:42.805 --> 02:44:44.285
training issue, um,

2703
02:44:44.345 --> 02:44:47.085
and hypothesized that Burt could be Undertrained.

2704
02:44:47.545 --> 02:44:50.485
Uh, Roberta was, uh, formulated

2705
02:44:50.485 --> 02:44:53.605
to further exploit births potential, um,

2706
02:44:53.665 --> 02:44:56.645
by altering its pre-training regimen, usually by, um,

2707
02:44:56.825 --> 02:44:59.245
or mostly by implementing bootstrapping,

2708
02:44:59.245 --> 02:45:02.045
which is statistical sampling with repetition, um,

2709
02:45:02.225 --> 02:45:04.125
to improve the size of the dataset.

2710
02:45:06.285 --> 02:45:11.185
Um, and another way they did that is, uh, dynamic masking.

2711
02:45:11.485 --> 02:45:14.265
So remember the masking concept where we take a specific,

2712
02:45:14.805 --> 02:45:16.545
um, word and redact it.

2713
02:45:17.485 --> 02:45:19.895
Roberta introduces, uh, lots

2714
02:45:19.895 --> 02:45:24.375
of improvement using the masked language model task

2715
02:45:25.185 --> 02:45:26.615
using that dynamic masking.

2716
02:45:27.115 --> 02:45:30.415
So as opposed to static masking where the same word is, uh,

2717
02:45:31.635 --> 02:45:35.665
muted or masked each time, um, dynamic masking implies

2718
02:45:35.665 --> 02:45:38.985
that the words selected to be masked are different across

2719
02:45:38.985 --> 02:45:41.345
different aspects of training, different times we, uh,

2720
02:45:41.705 --> 02:45:42.905
iterate through, uh, training.

2721
02:45:44.545 --> 02:45:49.445
Um, so therefore, Roberta training is, um, only on the

2722
02:45:49.965 --> 02:45:52.405
MLM task and does not

2723
02:45:52.955 --> 02:45:55.325
rely on predicting the sequential relation

2724
02:45:55.325 --> 02:45:56.605
between the two sentences.

2725
02:45:57.185 --> 02:46:01.485
So Bert Algorithm uses both, um,

2726
02:46:01.925 --> 02:46:05.455
MLM mask language model, um, methodology to train

2727
02:46:05.915 --> 02:46:07.935
and next sentence prediction.

2728
02:46:08.355 --> 02:46:09.525
So uses of those.

2729
02:46:09.525 --> 02:46:13.565
Whereas Roberta just relies on masked language modeling.

2730
02:46:16.975 --> 02:46:20.955
Um, that adjustment that Roberta does, uh,

2731
02:46:20.955 --> 02:46:25.235
prevents the model from memorization of mask words, uh, due

2732
02:46:25.235 --> 02:46:27.555
to the continual change of the masked instances.

2733
02:46:28.655 --> 02:46:31.555
Um, so it can't just get good, um,

2734
02:46:31.975 --> 02:46:33.675
at memorizing things.

2735
02:46:34.015 --> 02:46:36.395
It actually has to learn where, uh,

2736
02:46:36.455 --> 02:46:37.715
the context could be used.

2737
02:46:38.995 --> 02:46:41.975
Um, so why would we drop NSP?

2738
02:46:42.355 --> 02:46:44.215
Why would we drop next sentence prediction?

2739
02:46:45.235 --> 02:46:50.215
Um, so it was initially present in Bert's training, um,

2740
02:46:50.955 --> 02:46:55.315
but we found that it was not as valuable

2741
02:46:55.535 --> 02:46:56.795
as we initially considered,

2742
02:46:57.375 --> 02:47:01.795
and, um, we removed it to allow for, um,

2743
02:47:02.035 --> 02:47:03.875
a more optimized model.

2744
02:47:05.445 --> 02:47:07.025
Um, so

2745
02:47:07.055 --> 02:47:10.265
because of that, we're able to use larger batch sizes

2746
02:47:10.265 --> 02:47:14.505
during training, and that has a more substantially, um,

2747
02:47:15.415 --> 02:47:19.225
increased number of examples, uh, for training.

2748
02:47:20.105 --> 02:47:23.645
Um, and it can happen concurrently during iteration.

2749
02:47:24.945 --> 02:47:29.515
Um, and then also Roberta utilizes bite level

2750
02:47:29.795 --> 02:47:33.395
BPE tokenize, uh, which is a lot of words,

2751
02:47:34.155 --> 02:47:38.315
BPE bite level pair encoding,

2752
02:47:38.575 --> 02:47:40.835
so BPE bite pair encoding.

2753
02:47:42.265 --> 02:47:47.125
Um, so it uses, Roberta uses BPE as a tokenize,

2754
02:47:47.625 --> 02:47:50.765
uh, providing it with a lot of advantages

2755
02:47:51.065 --> 02:47:55.365
and advantageous capability to handle text

2756
02:47:55.365 --> 02:47:56.405
and multiple languages

2757
02:47:56.865 --> 02:48:00.165
and effectively process larger text corpuses.

2758
02:48:02.575 --> 02:48:05.915
Um, so through all those changes in pre-training,

2759
02:48:06.785 --> 02:48:10.835
Roberta tries to navigate the challenges identified in Burt

2760
02:48:11.305 --> 02:48:13.235
achieving refined performance

2761
02:48:13.465 --> 02:48:16.395
through various natural language processing tasks

2762
02:48:17.095 --> 02:48:19.035
by fully leveraging the potential

2763
02:48:19.035 --> 02:48:20.475
of the transformer architecture.

2764
02:48:34.155 --> 02:48:36.895
Oh, my pen's. There we go. It was off.

2765
02:48:38.075 --> 02:48:38.815
All right, here we go.

2766
02:48:43.935 --> 02:48:47.155
So, Roberta, using dynamic masking instead

2767
02:48:47.155 --> 02:48:48.595
of static masking, remember that's

2768
02:48:48.595 --> 02:48:49.875
what we talked about the last slide.

2769
02:48:50.135 --> 02:48:53.155
Um, here we are going to, um, use an example.

2770
02:48:55.205 --> 02:48:57.545
So, um, static masking

2771
02:48:58.115 --> 02:49:01.705
again means we're hiding the same words every time.

2772
02:49:02.525 --> 02:49:04.825
Um, we show a sentence to the model during training,

2773
02:49:05.605 --> 02:49:09.125
so it's easier to predict sometimes, uh, that makes it,

2774
02:49:09.345 --> 02:49:10.845
um, under fit.

2775
02:49:12.375 --> 02:49:15.275
So dynamic masking, we, uh,

2776
02:49:15.385 --> 02:49:17.915
hide different words every time during training.

2777
02:49:18.805 --> 02:49:22.185
Um, so imagine a sentence, we make 10 copies of it.

2778
02:49:23.815 --> 02:49:26.075
Now we hide different words in each copy.

2779
02:49:27.675 --> 02:49:30.215
Like here, it's the same sentence, right?

2780
02:49:30.795 --> 02:49:33.135
We arrived at the airport in time.

2781
02:49:34.355 --> 02:49:36.335
Um, so we have sentence one, sentence two,

2782
02:49:36.435 --> 02:49:37.575
all the way to sentence 10.

2783
02:49:38.275 --> 02:49:41.815
So in sentence one, we're masking arrived

2784
02:49:43.005 --> 02:49:44.985
and we're masking time.

2785
02:49:46.485 --> 02:49:51.425
Sentence two, we're masking at, uh, we're masking airport

2786
02:49:52.465 --> 02:49:56.995
sentence 10, we're masking the, and we're masking

2787
02:50:02.215 --> 02:50:02.435
in.

2788
02:50:03.985 --> 02:50:08.525
Um, and we repeat those, uh, different processes,

2789
02:50:08.525 --> 02:50:10.645
randomization over time.

2790
02:50:11.265 --> 02:50:15.805
Um, and if you're interested, that concept of expanding

2791
02:50:16.065 --> 02:50:21.045
and randomizing, um, is just statistical bootstrapping.

2792
02:50:21.785 --> 02:50:23.045
Um, so let me write

2793
02:50:23.045 --> 02:50:25.405
that down if you're interested in researching it further.

2794
02:50:29.405 --> 02:50:29.625
So

2795
02:50:34.145 --> 02:50:35.955
It's bootstrapping applied in a different way.

2796
02:50:36.015 --> 02:50:37.995
So there's actually a lot of literature on it.

2797
02:50:38.215 --> 02:50:41.515
Um, and it's a very robust algorithm to improve.

2798
02:50:41.815 --> 02:50:44.715
Um, actually recently used it work, uh,

2799
02:50:44.865 --> 02:50:47.155
like just the other day to improve one of my analysis.

2800
02:50:47.935 --> 02:50:52.835
Um, and one thing that it does, it is it, uh,

2801
02:50:53.185 --> 02:50:57.435
creates a statistically kind of like a, a sample, um,

2802
02:50:58.295 --> 02:51:00.195
uh, a sample mean distribution, uh,

2803
02:51:00.195 --> 02:51:01.555
or sample normal distribution.

2804
02:51:02.295 --> 02:51:06.915
Um, and that really helps, um, improve algorithms

2805
02:51:07.055 --> 02:51:10.635
and allows you to play nice with, um, analyses.

2806
02:51:11.495 --> 02:51:14.315
Um, because instead of just having one record,

2807
02:51:14.735 --> 02:51:15.875
you have many records

2808
02:51:16.255 --> 02:51:20.275
and it, uh, leverages the central limit theorem to allow

2809
02:51:20.295 --> 02:51:22.035
for potential normal distributions

2810
02:51:22.035 --> 02:51:23.515
and sample means statistically.

2811
02:51:24.015 --> 02:51:27.755
Um, that's not exactly how we're using it in this context,

2812
02:51:28.295 --> 02:51:30.115
but it's in that same vein.

2813
02:51:30.695 --> 02:51:33.635
So it does, um, allow for improvements.

2814
02:51:37.975 --> 02:51:38.465
Alright,

2815
02:51:44.095 --> 02:51:48.195
Um, so training Roberta using dynamic masking.

2816
02:51:49.855 --> 02:51:53.555
So here we are training this model

2817
02:51:53.695 --> 02:51:55.795
for 40 epics, right?

2818
02:51:56.095 --> 02:52:00.395
So there are a ton of different replicates, um,

2819
02:52:00.495 --> 02:52:03.555
and we are, uh, replicating it many times.

2820
02:52:04.215 --> 02:52:06.765
So we are randomizing, randomizing, randomizing

2821
02:52:06.765 --> 02:52:08.405
and bootstrapping, et cetera, et cetera.

2822
02:52:08.785 --> 02:52:12.245
So for each epic we feed the sentence with, uh,

2823
02:52:12.355 --> 02:52:13.485
many different tokens

2824
02:52:13.945 --> 02:52:15.805
or with different tokens that are masked.

2825
02:52:15.945 --> 02:52:20.425
So same sentence, different tokens, mask each time, um,

2826
02:52:20.645 --> 02:52:24.145
the model will see that sentence, um,

2827
02:52:25.065 --> 02:52:27.185
repeated over and over again with different masks,

2828
02:52:27.645 --> 02:52:29.585
and that will help improve the model.

2829
02:52:31.065 --> 02:52:35.205
Um, so that is generally, um,

2830
02:52:35.385 --> 02:52:39.125
at a very high level how Roberta HA is trained

2831
02:52:39.785 --> 02:52:42.085
and how, um, it is different from Bert.

2832
02:52:43.635 --> 02:52:47.135
Um, but the top level that you have to remember of

2833
02:52:47.155 --> 02:52:48.375
how it's different Roberta

2834
02:52:48.375 --> 02:52:52.335
and Bert is Roberta does not use NSP.

2835
02:52:52.915 --> 02:52:56.895
Um, and it relies heavily on, uh, MLM, um,

2836
02:52:57.355 --> 02:52:59.995
and how it changes MLM

2837
02:52:59.995 --> 02:53:02.715
to improve it is dynamic masking.

2838
02:53:12.585 --> 02:53:14.645
So why would we use each of 'em?

2839
02:53:15.695 --> 02:53:18.755
Um, Roberta model more extension

2840
02:53:18.755 --> 02:53:20.915
of training dataset than Bert, uh,

2841
02:53:20.915 --> 02:53:23.955
consisting over 160 million sentences,

2842
02:53:23.965 --> 02:53:25.395
which is a lot of sentences.

2843
02:53:26.215 --> 02:53:29.355
Um, deeper transformer model than Bert.

2844
02:53:30.095 --> 02:53:33.445
Um, lots of transformer layers, double, uh,

2845
02:53:33.745 --> 02:53:35.445
max language modeling objective.

2846
02:53:35.505 --> 02:53:38.325
If that's what you're intending to research, then

2847
02:53:38.345 --> 02:53:40.205
that's obviously the best case.

2848
02:53:40.405 --> 02:53:44.005
'cause Roberta has a more robust, uh, MLM, um,

2849
02:53:44.185 --> 02:53:46.205
it has a much larger vocabulary than Bert

2850
02:53:46.395 --> 02:53:47.485
over a million words.

2851
02:53:49.165 --> 02:53:51.305
Um, and Roberta training has

2852
02:53:51.925 --> 02:53:55.225
sub word tokenization in general, uh, dynamic masking,

2853
02:53:55.225 --> 02:53:59.185
like I was saying before, and can take longer sequences up

2854
02:53:59.185 --> 02:54:02.865
to about 500 tokens, a little over 500 tokens, um, compared

2855
02:54:02.865 --> 02:54:04.065
to Bert who is much less.

2856
02:54:04.845 --> 02:54:09.375
Um, and just general pros

2857
02:54:09.375 --> 02:54:11.415
and cons, uh, Roberta

2858
02:54:12.945 --> 02:54:14.675
removes next sentence prediction.

2859
02:54:15.835 --> 02:54:17.455
Uh, significantly increases.

2860
02:54:18.195 --> 02:54:20.535
Oh, looks like there's a poll on

2861
02:54:20.675 --> 02:54:22.855
screen, uh, for you to take.

2862
02:54:22.995 --> 02:54:26.055
Now. Uh, we've got about 45 minutes left in the class,

2863
02:54:27.195 --> 02:54:29.175
so we always appreciate your feedback.

2864
02:54:31.315 --> 02:54:33.615
Um, and I promise right

2865
02:54:33.615 --> 02:54:37.755
after that, we're gonna get back into the code.

2866
02:54:38.055 --> 02:54:40.235
Um, I think probably in about 10, 15 minutes

2867
02:54:40.445 --> 02:54:42.515
after the next code session, uh,

2868
02:54:42.515 --> 02:54:43.995
we're gonna take just a five minute break

2869
02:54:43.995 --> 02:54:46.915
because that will put us at 30 minutes to go,

2870
02:54:46.925 --> 02:54:48.595
which is the hardest 30 minutes,

2871
02:54:49.735 --> 02:54:52.275
but we know, um, we're almost there.

2872
02:54:58.965 --> 02:55:00.745
Really appreciate everybody's feedback.

2873
02:55:05.755 --> 02:55:07.735
All right, closing the poll in 10 seconds.

2874
02:55:13.215 --> 02:55:15.835
Um, and then after this, also feel free to connect

2875
02:55:15.835 --> 02:55:19.035
with me on LinkedIn, um, and ask any questions.

2876
02:55:19.575 --> 02:55:21.795
Um, I think I'm the only tankas on there.

2877
02:55:22.035 --> 02:55:23.555
I think it's like me and like my dad.

2878
02:55:23.895 --> 02:55:25.355
So I'm the one that's not a photographer.

2879
02:55:29.245 --> 02:55:31.145
Um, always happy to, uh,

2880
02:55:31.185 --> 02:55:32.305
I think we've also got a career

2881
02:55:32.705 --> 02:55:33.905
coaching session on Wednesday.

2882
02:55:34.045 --> 02:55:35.985
I'm not exactly sure how to sign up for that.

2883
02:55:36.685 --> 02:55:41.645
Um, but I think you've been communicated, uh, how

2884
02:55:41.645 --> 02:55:43.125
to sign up for the career coaching session,

2885
02:55:43.425 --> 02:55:45.085
but always happy to talk about that stuff

2886
02:55:45.145 --> 02:55:47.915
and stoked to talk about data.

2887
02:55:48.695 --> 02:55:51.235
Um, speaking of which, let's talk about data.

2888
02:55:55.495 --> 02:55:57.805
Thank you for the feedback. Let's keep rolling.

2889
02:55:59.625 --> 02:56:02.155
Um, okay,

2890
02:56:03.095 --> 02:56:05.195
so we talked about the main thing,

2891
02:56:05.305 --> 02:56:07.655
Roberto versus Bert pre-training.

2892
02:56:07.655 --> 02:56:09.975
Objective sentence piece. I don't think we've talked. Yeah.

2893
02:56:09.975 --> 02:56:11.975
Okay. Sentence piece tokenization.

2894
02:56:13.055 --> 02:56:16.105
Roberta uses a sentence piece tokenize, which allows

2895
02:56:16.105 --> 02:56:19.065
for dynamic vocabulary and that adapts to training data.

2896
02:56:19.885 --> 02:56:22.985
Um, that's different than birch, uh,

2897
02:56:23.035 --> 02:56:25.425
which usually uses word piece,

2898
02:56:25.465 --> 02:56:28.025
tokenize sentence piece tokenize.

2899
02:56:28.205 --> 02:56:30.745
So at the sentence level, tokenizing instead of word level,

2900
02:56:31.325 --> 02:56:35.785
um, that is typically, um, advantageous for languages

2901
02:56:35.885 --> 02:56:36.905
and rich morphology.

2902
02:56:37.245 --> 02:56:41.345
Um, so like when more context is required, you usually have

2903
02:56:41.345 --> 02:56:42.425
to cast a wider net.

2904
02:56:42.565 --> 02:56:44.425
So look at sentence tokenization as opposed

2905
02:56:44.425 --> 02:56:45.505
to word tokenization.

2906
02:56:47.135 --> 02:56:50.545
Um, so Roberta handles that a little bit better.

2907
02:56:52.345 --> 02:56:53.885
Uh, learning rate schedules,

2908
02:56:54.235 --> 02:56:56.685
Roberta employees a more aggressive training setup

2909
02:56:56.715 --> 02:56:59.885
with larger batch size and a linear learning rate.

2910
02:57:00.945 --> 02:57:05.565
Um, and this may result in faster convergence,

2911
02:57:05.825 --> 02:57:07.405
so getting to an answer a little quicker.

2912
02:57:07.785 --> 02:57:10.325
Um, and improved performance in some cases.

2913
02:57:12.175 --> 02:57:14.355
Uh, for model architecture, both Bert

2914
02:57:14.375 --> 02:57:17.155
and Roberta are based on transformer architecture.

2915
02:57:17.505 --> 02:57:18.595
What are transformers?

2916
02:57:19.145 --> 02:57:21.795
That was what we talked about earlier in the lecture, um,

2917
02:57:21.825 --> 02:57:23.595
when we're talking about encoders and decoders.

2918
02:57:23.595 --> 02:57:27.635
So both Bert and Roberta are, um, uh, based on those models,

2919
02:57:29.885 --> 02:57:32.225
um, they normalization both used.

2920
02:57:32.765 --> 02:57:35.345
Um, so depending on the specific task

2921
02:57:35.345 --> 02:57:37.425
and desired model behavior, uh, bur

2922
02:57:37.425 --> 02:57:41.065
and Roberta, uh, differences may be relevant on whether

2923
02:57:41.065 --> 02:57:43.105
or not, um, NSP is used.

2924
02:57:43.295 --> 02:57:44.465
Next sentence prediction.

2925
02:57:45.355 --> 02:57:50.175
Um, and fine tuning, um, choice between bur

2926
02:57:50.175 --> 02:57:52.415
and Roberta when fine tuning is pretty

2927
02:57:52.615 --> 02:57:53.855
dependent on performance of the model.

2928
02:57:53.855 --> 02:57:54.975
On the downstream tasks.

2929
02:57:55.145 --> 02:57:58.495
After fine tuning, it's usually advisable to experiment

2930
02:57:58.995 --> 02:58:01.685
and evaluate both models on target tasks

2931
02:58:01.825 --> 02:58:03.325
to determine which one performs better.

2932
02:58:03.945 --> 02:58:08.525
Um, so when I am, uh, doing analyses,

2933
02:58:09.545 --> 02:58:14.105
I don't just run one thing because it's 100% the right case.

2934
02:58:14.805 --> 02:58:16.825
Um, sometimes Roberta performs better,

2935
02:58:16.825 --> 02:58:18.185
sometimes Bert performs better

2936
02:58:18.725 --> 02:58:23.685
and they're pretty computationally, um, trivial to run,

2937
02:58:24.425 --> 02:58:26.565
um, at the same time.

2938
02:58:27.305 --> 02:58:29.925
So when I'm trying to answer a research question,

2939
02:58:30.165 --> 02:58:33.125
I usually do just do all algorithms

2940
02:58:33.625 --> 02:58:38.165
and, um, if they are, uh, performing similarly,

2941
02:58:38.675 --> 02:58:41.565
I'll probably lean towards the one that does it faster.

2942
02:58:41.755 --> 02:58:46.205
Does that same, um, analysis faster, um, or more robust?

2943
02:58:46.985 --> 02:58:48.685
So fastest one, um,

2944
02:58:48.685 --> 02:58:50.445
that we're gonna talk about in a little bit is, Albert

2945
02:58:51.035 --> 02:58:52.685
most robust is Roberta,

2946
02:58:52.685 --> 02:58:55.605
and then Birch is the one that most people know, which

2947
02:58:56.125 --> 02:58:58.005
honestly is something valid as well.

2948
02:58:58.745 --> 02:59:01.995
Um, so all of those things are important to consider.

2949
02:59:03.515 --> 02:59:07.275
Alright, code demo, we're gonna do the code demo,

2950
02:59:07.375 --> 02:59:08.835
and then we're gonna take a five minute break

2951
02:59:09.255 --> 02:59:11.315
and we're gonna be happy and it's gonna be awesome.

2952
02:59:13.665 --> 02:59:17.245
Um, so here we have the code demo.

2953
02:59:18.395 --> 02:59:20.095
Um, let me just start.

2954
02:59:22.385 --> 02:59:26.285
All right, yeah, so this should be already run.

2955
02:59:27.185 --> 02:59:30.125
So again, unfortunately I can't run it live just

2956
02:59:30.125 --> 02:59:33.685
because these, uh, code books are ra rather large.

2957
02:59:34.845 --> 02:59:38.385
Um, so it would take all of the time we have remaining, uh,

2958
02:59:38.385 --> 02:59:40.625
just to run a couple of these, uh,

2959
02:59:40.765 --> 02:59:42.785
slides, like five minutes.

2960
02:59:43.285 --> 02:59:45.145
But we don't wanna watch, uh,

2961
02:59:46.005 --> 02:59:48.465
the corgis run across the screen for five minutes.

2962
02:59:50.525 --> 02:59:52.665
All right, uh, introduction.

2963
02:59:53.525 --> 02:59:55.625
Oh, actually, uh, let's zoom in.

2964
03:00:00.595 --> 03:00:03.855
Um, all right, that should match what we had

2965
03:00:03.855 --> 03:00:06.375
before, maybe a little bit more zoomed in.

2966
03:00:07.035 --> 03:00:09.815
Um, let me know in the chat if this is,

2967
03:00:09.915 --> 03:00:11.295
if we need to zoom in a little bit more.

2968
03:00:13.275 --> 03:00:16.815
Um, so Roberta, like we were talking about in the slides,

2969
03:00:17.155 --> 03:00:19.815
implementation of Bert with some key changes.

2970
03:00:20.635 --> 03:00:24.935
Um, and those changes include hyper parameterization, um,

2971
03:00:25.205 --> 03:00:29.295
lots of embedding tweaks, uses bite level, uh, BPE

2972
03:00:29.835 --> 03:00:34.215
as a tokenize, and that is a different pre-training scheme.

2973
03:00:35.365 --> 03:00:39.505
Um, Roberta is trained for longer sequences, so bigger, um,

2974
03:00:40.585 --> 03:00:44.545
capacity and number of iterations is increased, uh,

2975
03:00:44.695 --> 03:00:48.545
from a hundred K to 300 K and sometimes even 500 K as well.

2976
03:00:49.615 --> 03:00:52.995
Um, uses larger bite level vocab, um,

2977
03:00:53.015 --> 03:00:56.235
and has 50 K sub-board units instead of character level.

2978
03:00:56.695 --> 03:01:00.165
Uh, BPE, um, the size of 30 K and Bert.

2979
03:01:01.795 --> 03:01:05.855
Um, so all of its stock points into MLM, uh, none

2980
03:01:05.855 --> 03:01:07.735
of its stock points into, uh, NSP.

2981
03:01:08.915 --> 03:01:12.375
Um, Roberta doesn't use, uh, token type IDs

2982
03:01:12.755 --> 03:01:14.815
and we don't need to define tokens, which is pretty cool.

2983
03:01:15.645 --> 03:01:18.145
Um, only separate segments, uh,

2984
03:01:18.535 --> 03:01:21.225
with the separation or separation.

2985
03:01:21.925 --> 03:01:25.965
Um, in the token, um, larger mini batches,

2986
03:01:26.965 --> 03:01:31.365
oh, geez, sorry, uh, markdown,

2987
03:01:31.745 --> 03:01:33.605
uh, larger mini batches

2988
03:01:33.665 --> 03:01:36.725
and learning rates, uh, are used in Thea training.

2989
03:01:36.985 --> 03:01:39.205
Um, NSPs removed, like I was saying before.

2990
03:01:39.705 --> 03:01:42.125
But, um, let's get into the code.

2991
03:01:44.205 --> 03:01:46.585
Um, so this is connecting it to Google Drive.

2992
03:01:46.725 --> 03:01:49.505
If you have not used, um, the CoLab,

2993
03:01:49.725 --> 03:01:52.585
that's really the only negative I have with, uh, CoLab.

2994
03:01:52.845 --> 03:01:55.065
Um, it's a little bit difficult to, um,

2995
03:01:55.245 --> 03:01:56.865
manage your route directory.

2996
03:01:56.925 --> 03:02:01.905
So I usually just save, um, the code book locally, um,

2997
03:02:02.205 --> 03:02:06.425
in my Google Drive folder, and then have, um, an abso

2998
03:02:06.485 --> 03:02:10.065
or a, um, reference mapping, uh, for the actual path.

2999
03:02:10.645 --> 03:02:15.185
Um, and you can in, uh, Google find the location,

3000
03:02:15.845 --> 03:02:18.865
um, of each, uh, file by going to properties.

3001
03:02:19.245 --> 03:02:20.265
And that helped me out a lot.

3002
03:02:22.095 --> 03:02:24.955
Uh, so we install the Transformers torch,

3003
03:02:28.175 --> 03:02:30.795
um, install the accelerate package,

3004
03:02:33.595 --> 03:02:37.175
um, and then we clone this, uh,

3005
03:02:38.775 --> 03:02:43.375
repo, uh, dataset from the, um, get page here.

3006
03:02:43.875 --> 03:02:45.295
So tweet dataset.

3007
03:02:48.115 --> 03:02:51.845
Um, so here we are getting into the analysis,

3008
03:02:52.845 --> 03:02:56.125
defining the Roberta base model from Transformers.

3009
03:02:56.125 --> 03:02:57.965
We import the pipeline, um,

3010
03:02:58.025 --> 03:03:00.525
and we choose the model Roberta base.

3011
03:03:02.265 --> 03:03:04.365
And our token is also Roberta base.

3012
03:03:06.825 --> 03:03:08.565
Um, so this is the load here.

3013
03:03:10.715 --> 03:03:14.695
And after that we declare a function to, uh,

3014
03:03:14.695 --> 03:03:17.295
predict the masked token in the sentence.

3015
03:03:19.375 --> 03:03:22.595
Um, so our function here,

3016
03:03:22.685 --> 03:03:24.195
we're gonna predict the mass token.

3017
03:03:24.615 --> 03:03:25.955
We have the input model,

3018
03:03:27.305 --> 03:03:29.125
and then we have, uh, the input sentence.

3019
03:03:29.745 --> 03:03:31.485
Um, and then this is just going to be

3020
03:03:31.745 --> 03:03:33.165
for everything that we predict.

3021
03:03:33.705 --> 03:03:37.845
Um, we print the sequence with, uh, some formatting, um,

3022
03:03:37.865 --> 03:03:40.565
and then we print the, uh, confidence score.

3023
03:03:43.805 --> 03:03:48.245
Um, so here you see we have this mask

3024
03:03:48.855 --> 03:03:53.515
input send the, uh, mask back,

3025
03:03:54.255 --> 03:03:56.635
uh, calling that function with the Roberta model.

3026
03:03:56.655 --> 03:03:58.725
So we, we have the Roberta model input

3027
03:03:59.355 --> 03:04:00.725
into the predict mask token.

3028
03:04:01.345 --> 03:04:05.205
And then our goal is to predict what this is.

3029
03:04:06.385 --> 03:04:09.165
And see, we haven't really trained it on anything.

3030
03:04:09.665 --> 03:04:11.645
Um, we are just calling the pre-trained model

3031
03:04:12.265 --> 03:04:15.245
and we manually masking this.

3032
03:04:17.305 --> 03:04:22.115
Um, it looks like there's another poll.

3033
03:04:24.835 --> 03:04:26.255
Uh, I think we already answered this.

3034
03:04:28.305 --> 03:04:30.775
Looks like there's some slightly different questions.

3035
03:04:31.395 --> 03:04:33.455
Uh, we're gonna take about 15 seconds to answer this,

3036
03:04:33.695 --> 03:04:36.655
'cause I think we're running pretty short on time.

3037
03:04:42.145 --> 03:04:44.205
Oh, okay. Uh, thank you Benjamin.

3038
03:04:58.675 --> 03:05:00.935
Uh, so this one is for me.

3039
03:05:01.955 --> 03:05:04.215
Um, the last one was for the ta.

3040
03:05:05.035 --> 03:05:06.935
So really appreciate everybody's feedback.

3041
03:05:23.815 --> 03:05:24.875
All right, let's give it 10 more seconds.

3042
03:05:50.275 --> 03:05:52.535
All right, thanks everybody for the responses.

3043
03:05:58.945 --> 03:06:01.895
We've got about, uh, 30 minutes left in the class,

3044
03:06:01.995 --> 03:06:03.415
so we'll finish this code book

3045
03:06:03.435 --> 03:06:05.655
and then take five minutes, not 10 minutes.

3046
03:06:06.675 --> 03:06:10.695
Um, and then we will finish the last, I think,

3047
03:06:10.845 --> 03:06:12.575
15, 20 slides.

3048
03:06:15.275 --> 03:06:18.655
Um, okay, here we go.

3049
03:06:19.685 --> 03:06:23.625
So, predict the mask token, um, send the mask back.

3050
03:06:24.325 --> 03:06:27.185
Um, so Roberta is trying to figure out

3051
03:06:27.455 --> 03:06:28.505
what we're trying to say.

3052
03:06:29.295 --> 03:06:32.545
Send the pictures back, send the photos back,

3053
03:06:32.655 --> 03:06:33.945
send the emails back so we're

3054
03:06:33.945 --> 03:06:35.065
all kind of in the same region.

3055
03:06:35.845 --> 03:06:37.305
Um, and you can see here

3056
03:06:37.845 --> 03:06:39.985
it has confidence score with each one of these.

3057
03:06:40.485 --> 03:06:44.465
So Roberta is pretty sure that it'd send the pictures back,

3058
03:06:44.765 --> 03:06:47.025
but it also might be send the photos back.

3059
03:06:47.765 --> 03:06:52.505
So, um, all these three here are pretty tied.

3060
03:06:53.325 --> 03:06:57.765
Um, and what's interesting is those are all things

3061
03:06:57.765 --> 03:06:59.325
that humans would say, right?

3062
03:06:59.475 --> 03:07:00.725
Send the emails back, send the

3063
03:07:00.725 --> 03:07:01.965
pictures back, send the photos back.

3064
03:07:02.155 --> 03:07:04.365
It's pretty uncommon, uh, for somebody to say,

3065
03:07:04.365 --> 03:07:06.005
send the image back or send the letters

3066
03:07:06.115 --> 03:07:07.285
back, especially nowadays.

3067
03:07:07.545 --> 03:07:09.925
And there is a fairly significant confidence drop in those.

3068
03:07:09.985 --> 03:07:11.645
So that is really interesting

3069
03:07:11.645 --> 03:07:13.805
that it captures natural language sentiment

3070
03:07:14.505 --> 03:07:15.765
or natural language performance.

3071
03:07:16.385 --> 03:07:20.205
Um, so one thing that is important is while, uh,

3072
03:07:20.395 --> 03:07:22.885
Roberta models are updated, um,

3073
03:07:23.025 --> 03:07:25.165
and the pre-train models are continually trained,

3074
03:07:26.145 --> 03:07:29.725
it may not, um, have the most UpToDate information,

3075
03:07:30.015 --> 03:07:32.805
especially when talking about like, uh, culture

3076
03:07:33.305 --> 03:07:36.885
and, um, societal, um, leanings and things like that.

3077
03:07:37.665 --> 03:07:41.535
Uh, so when we're putting in e

3078
03:07:41.535 --> 03:07:44.975
or then when the input is, Elon Musk is the founder of x,

3079
03:07:46.045 --> 03:07:49.425
uh, when X is mask, um, Elon Musk is the founder

3080
03:07:49.425 --> 03:07:51.705
of Tesla, uh, SpaceX.

3081
03:07:51.805 --> 03:07:53.065
So we're pretty confident in that.

3082
03:07:53.765 --> 03:07:57.785
Um, and I'm, one thing

3083
03:07:57.935 --> 03:08:00.265
that we should consider is Elon

3084
03:08:00.265 --> 03:08:01.505
Musk is the founder of Twitter.

3085
03:08:02.125 --> 03:08:05.745
Um, while he's not the founder, I would expect this, uh,

3086
03:08:05.745 --> 03:08:06.945
confidence score to be a little bit higher

3087
03:08:06.945 --> 03:08:08.385
because he's the current CEO.

3088
03:08:08.925 --> 03:08:12.465
So this may not be the, um,

3089
03:08:14.255 --> 03:08:16.785
like we can tune this model to improve

3090
03:08:16.785 --> 03:08:18.105
that prediction a little bit better.

3091
03:08:19.375 --> 03:08:22.315
Um, especially because there is no Twitter anymore.

3092
03:08:22.315 --> 03:08:26.625
It's X, right? Um, okay, so configuring,

3093
03:08:26.625 --> 03:08:28.585
tokenizing training and saving the model.

3094
03:08:29.285 --> 03:08:31.225
Uh, so prep, or prep,

3095
03:08:31.225 --> 03:08:33.265
preparing the data set is what we're gonna do first.

3096
03:08:34.215 --> 03:08:38.755
So we build the, uh, token here, um, the BPE tokenize,

3097
03:08:39.415 --> 03:08:43.035
uh, and we have token on Roberta based token

3098
03:08:43.655 --> 03:08:45.315
or the model on Roberta base as well.

3099
03:08:46.805 --> 03:08:49.625
Um, and then here we are preparing the transformers,

3100
03:08:51.625 --> 03:08:53.925
um, based on this training text.

3101
03:08:57.605 --> 03:09:00.665
Um, and then from there we'll load some

3102
03:09:00.665 --> 03:09:01.745
libraries in a little bit.

3103
03:09:01.765 --> 03:09:03.865
But let's just get into some context quickly.

3104
03:09:05.805 --> 03:09:08.745
Uh, so what is the data collator and what does it do?

3105
03:09:09.885 --> 03:09:14.495
Data correlators are objects that will form a branch using

3106
03:09:15.215 --> 03:09:17.695
a list of data sets and elements as input.

3107
03:09:18.515 --> 03:09:22.295
Um, those elements are the same type as the elements

3108
03:09:22.295 --> 03:09:24.775
of the training dataset or ev eval dataset.

3109
03:09:25.155 --> 03:09:26.855
And to be able to build branches.

3110
03:09:27.445 --> 03:09:30.695
Data collators, um, may apply some processing

3111
03:09:30.835 --> 03:09:34.415
or padding, um, from, uh,

3112
03:09:34.415 --> 03:09:37.775
things like this data collator, uh, for language modeling.

3113
03:09:38.515 --> 03:09:41.815
And they also apply some random data augmentation like

3114
03:09:41.845 --> 03:09:45.535
masking, um, random masking specifically on

3115
03:09:45.535 --> 03:09:46.575
that formed branch.

3116
03:09:47.885 --> 03:09:49.745
Um, and here we're gonna use that function

3117
03:09:50.005 --> 03:09:52.825
for randomly masking 15% of the tokens.

3118
03:09:53.025 --> 03:09:57.485
For MLM tests, uh, parameters are, uh,

3119
03:09:57.705 --> 03:10:01.125
the token, and the token is typically used for,

3120
03:10:01.185 --> 03:10:02.725
uh, encoding the data.

3121
03:10:04.545 --> 03:10:07.445
Um, the MLM process, um,

3122
03:10:08.225 --> 03:10:10.325
or the MLM um, input is whether

3123
03:10:10.325 --> 03:10:12.125
or not to use mask language modeling.

3124
03:10:12.625 --> 03:10:16.655
If we set that to false, um, the labels

3125
03:10:16.995 --> 03:10:20.495
and the inputs, um, are ignored.

3126
03:10:21.545 --> 03:10:25.165
Um, otherwise the labels are, um, set to true

3127
03:10:26.425 --> 03:10:27.525
or not ignored, I should say.

3128
03:10:28.185 --> 03:10:32.805
Um, and for MLM probability, um, like I was saying

3129
03:10:32.805 --> 03:10:34.445
before, if we want to include

3130
03:10:34.445 --> 03:10:36.005
that in our model, then we set to true.

3131
03:10:41.415 --> 03:10:45.715
All right, so finally, um, importing

3132
03:10:45.715 --> 03:10:48.555
that final model data co, uh, data collator

3133
03:10:48.555 --> 03:10:50.955
for large language model, big functioning.

3134
03:10:52.145 --> 03:10:54.325
Um, so we declare it, we declare the tokenize

3135
03:10:54.325 --> 03:10:56.165
that we're using that we declared up above.

3136
03:10:56.665 --> 03:10:59.085
Um, we are using MLM, um,

3137
03:10:59.705 --> 03:11:01.525
and we have, uh,

3138
03:11:01.525 --> 03:11:05.045
15% masking randomly 15% of the tokens.

3139
03:11:05.585 --> 03:11:10.085
Um, so in a sentence of 100 words, 15

3140
03:11:10.105 --> 03:11:11.645
of those will be masked.

3141
03:11:17.585 --> 03:11:21.555
Oh, um, shout out from, uh, Reja

3142
03:11:21.895 --> 03:11:23.875
and, uh, Benjamin and a couple other people.

3143
03:11:24.015 --> 03:11:25.875
Um, really appreciate the feedback.

3144
03:11:26.655 --> 03:11:28.995
Um, always good to hear, um,

3145
03:11:29.255 --> 03:11:33.835
and stoked to contribute to the knowledge of new,

3146
03:11:34.215 --> 03:11:37.435
um, data science people and data engineers

3147
03:11:37.815 --> 03:11:40.995
and us other engineers or whoever you may be.

3148
03:11:44.205 --> 03:11:48.845
Um, okay, so difference between Roberta model base

3149
03:11:49.025 --> 03:11:50.645
and a Roberta model retrained.

3150
03:11:51.745 --> 03:11:54.285
Um, Roberta model base is a pretty generic model.

3151
03:11:55.065 --> 03:11:59.025
Um, and we have that pre-training step versus

3152
03:11:59.055 --> 03:12:03.185
that fine tuning to get Roberta a retrained model.

3153
03:12:06.815 --> 03:12:09.195
Um, so here we are training the model.

3154
03:12:09.535 --> 03:12:10.675
We have transformer

3155
03:12:10.815 --> 03:12:12.835
or from transformers, we're importing the trainer

3156
03:12:12.895 --> 03:12:16.395
and the training arguments, um, we're initializing here.

3157
03:12:17.075 --> 03:12:18.875
Training arguments are this function.

3158
03:12:19.495 --> 03:12:21.795
Um, I know when it gets to code, it's pretty difficult

3159
03:12:21.795 --> 03:12:25.035
to interpret all the time, but just like go line by line

3160
03:12:25.055 --> 03:12:26.595
and we will read it together.

3161
03:12:27.925 --> 03:12:31.865
Um, the output directory is where our model is going to go,

3162
03:12:33.185 --> 03:12:35.125
um, and whether

3163
03:12:35.125 --> 03:12:37.845
or not we're overriding our current, um, model,

3164
03:12:37.845 --> 03:12:39.325
which is kind of that learning process.

3165
03:12:40.025 --> 03:12:41.925
Um, and how many epics we're gonna train on.

3166
03:12:42.305 --> 03:12:44.565
Uh, one is just so that we kind

3167
03:12:44.565 --> 03:12:46.485
of reduce the computational load.

3168
03:12:46.785 --> 03:12:48.285
You can go higher and higher and higher.

3169
03:12:48.865 --> 03:12:51.485
Um, this is the batch side or batch size.

3170
03:12:51.675 --> 03:12:54.325
Also consider that for, um, computational load.

3171
03:12:55.225 --> 03:12:56.645
Um, and then this is seed.

3172
03:12:56.785 --> 03:12:59.885
So, uh, randomness, um, there's lots.

3173
03:13:00.185 --> 03:13:04.285
So seed in randomness is, it's actually really difficult

3174
03:13:04.285 --> 03:13:07.325
to get a truly random process, uh,

3175
03:13:07.985 --> 03:13:10.045
by generating numbers randomly.

3176
03:13:10.465 --> 03:13:14.055
You have to have, um, numbers

3177
03:13:14.445 --> 03:13:16.295
that are truly random.

3178
03:13:17.275 --> 03:13:20.015
And for that you have to set a seed.

3179
03:13:20.015 --> 03:13:22.615
There's an approved list of randomness, um,

3180
03:13:22.745 --> 03:13:24.615
which is hilarious to say,

3181
03:13:24.795 --> 03:13:29.495
but, um, that approved randomness will allow us

3182
03:13:30.035 --> 03:13:33.895
to make sure that our model is the most effective.

3183
03:13:34.355 --> 03:13:38.615
So seed selects one of those approved random lists.

3184
03:13:40.165 --> 03:13:41.625
Um, so seed one here.

3185
03:13:43.985 --> 03:13:47.365
Um, and then passing the model training arguments

3186
03:13:47.365 --> 03:13:50.285
to the data correlator, uh, the data set, uh,

3187
03:13:50.305 --> 03:13:52.445
to the trainer function and training the model.

3188
03:13:53.245 --> 03:13:56.185
Um, so here we take the training function,

3189
03:13:56.285 --> 03:13:58.185
we declare the model that we've discussed.

3190
03:13:58.565 --> 03:14:00.305
Uh, we have the training arguments,

3191
03:14:00.305 --> 03:14:02.505
and then we have the data collator and the data set.

3192
03:14:04.055 --> 03:14:06.595
Um, and then we train, uh, see,

3193
03:14:06.595 --> 03:14:07.635
this took about three minutes.

3194
03:14:08.905 --> 03:14:11.925
Um, so this is going to be a little bit, uh,

3195
03:14:11.925 --> 03:14:13.125
computationally expensive.

3196
03:14:13.185 --> 03:14:14.605
And this is a simplified model.

3197
03:14:14.785 --> 03:14:18.165
So, um, if you're training a more advanced model, just, uh,

3198
03:14:18.525 --> 03:14:20.165
remember to plan in advance.

3199
03:14:22.205 --> 03:14:24.385
Um, and then we're loading

3200
03:14:24.385 --> 03:14:25.985
that trained model, that retrained model.

3201
03:14:27.365 --> 03:14:29.505
Um, and now we're using again.

3202
03:14:30.675 --> 03:14:33.975
So here we have a lot of calls and a lot of outputs.

3203
03:14:34.475 --> 03:14:37.855
Um, what we're going to be looking at is

3204
03:14:38.645 --> 03:14:40.815
this right here, Roberta retrained.

3205
03:14:41.405 --> 03:14:43.655
Roberta retrained, Roberta base.

3206
03:14:44.915 --> 03:14:48.115
Um, so look at

3207
03:14:49.285 --> 03:14:50.475
these two differences.

3208
03:14:51.415 --> 03:14:54.475
Uh, first one is retrained, second one is base.

3209
03:14:56.405 --> 03:14:59.705
Um, I hate watching mask sports.

3210
03:15:01.145 --> 03:15:05.125
And the next one is the same. I hate watching mask sports.

3211
03:15:05.795 --> 03:15:10.365
This one is Roberta retrained. This one is Roberta Bass.

3212
03:15:10.545 --> 03:15:11.725
So let's look at the differences.

3213
03:15:13.145 --> 03:15:16.315
So Roberta retrained, um,

3214
03:15:16.915 --> 03:15:19.275
I hate watching fantasy sport.

3215
03:15:20.075 --> 03:15:22.395
I hate watching stupid sport college sport,

3216
03:15:22.625 --> 03:15:23.715
live sport, pro sport.

3217
03:15:24.435 --> 03:15:26.355
I hate watching fantasy sport.

3218
03:15:26.575 --> 03:15:28.635
So that's the same college sport, live sport,

3219
03:15:28.635 --> 03:15:30.115
professional sport, pro sport.

3220
03:15:30.895 --> 03:15:33.675
So the difference is, we can see here, uh,

3221
03:15:33.775 --> 03:15:37.235
stupid sport is not available in the base model.

3222
03:15:38.295 --> 03:15:42.435
Um, and college is a little bit higher in the base model.

3223
03:15:43.055 --> 03:15:47.915
Um, professional sport is also, um, duplicated here.

3224
03:15:47.935 --> 03:15:49.405
So pro sport, professional sport.

3225
03:15:50.385 --> 03:15:51.885
So this, um,

3226
03:15:52.225 --> 03:15:54.325
may be a little bit more applicable in our setting.

3227
03:15:55.105 --> 03:15:58.765
Um, so I hate watching fantasy sport. Um, stupid sport.

3228
03:15:58.765 --> 03:16:00.685
College sport may be more applicable for us

3229
03:16:00.685 --> 03:16:02.365
because we have tuned it that way.

3230
03:16:03.515 --> 03:16:07.495
Um, so we can see the confidence is lower

3231
03:16:08.355 --> 03:16:10.535
10% versus 24%.

3232
03:16:11.075 --> 03:16:14.575
Um, but in fine tuning, you will have

3233
03:16:14.575 --> 03:16:16.815
to take a little bit more ownership over the model

3234
03:16:17.035 --> 03:16:18.455
and how it predicts your things.

3235
03:16:18.795 --> 03:16:21.255
Um, so just make sure that, um,

3236
03:16:21.565 --> 03:16:23.535
when you are fitting algorithms, um,

3237
03:16:24.605 --> 03:16:26.615
just try different things when you're tuning it,

3238
03:16:26.615 --> 03:16:28.935
you're retraining it, um, to fit your purposes.

3239
03:16:30.295 --> 03:16:34.115
Um, and then here we can just see, uh, in the demo slides,

3240
03:16:34.295 --> 03:16:36.035
I'm gonna kind of gloss over these next

3241
03:16:36.055 --> 03:16:37.315
few 'cause it's about the same.

3242
03:16:38.015 --> 03:16:41.075
Um, I'm a male model fashion female, Russian young.

3243
03:16:41.615 --> 03:16:43.435
Um, this is the retrained.

3244
03:16:43.455 --> 03:16:45.475
And the next, I'm a male female,

3245
03:16:45.475 --> 03:16:46.835
professional fashion Russian.

3246
03:16:47.135 --> 03:16:48.755
So very similar. Um,

3247
03:16:48.855 --> 03:16:51.195
and then here we have a little bit lower, uh,

3248
03:16:51.285 --> 03:16:52.755
confidence for male.

3249
03:16:53.655 --> 03:16:57.715
Um, and then, uh, female is significantly lower on the list,

3250
03:16:57.815 --> 03:16:59.395
but about the same confidence level.

3251
03:17:00.055 --> 03:17:04.605
Um, so it is up to you

3252
03:17:04.625 --> 03:17:06.925
to determine, um, your cases.

3253
03:17:07.865 --> 03:17:10.485
Um, which one you would recommend retrain

3254
03:17:10.585 --> 03:17:12.365
or base model here.

3255
03:17:12.665 --> 03:17:15.365
Um, I think for this circumstance,

3256
03:17:15.585 --> 03:17:17.005
I'd probably recommend base model

3257
03:17:17.005 --> 03:17:20.845
because there is a higher confidence, um, on male model.

3258
03:17:21.505 --> 03:17:25.525
Um, and if that is what we're looking at, um, then

3259
03:17:25.755 --> 03:17:27.445
that is the model I'd recommend.

3260
03:17:27.515 --> 03:17:30.965
Usually when, um, we are in this state, uh,

3261
03:17:30.965 --> 03:17:33.885
where we have such a low difference, uh,

3262
03:17:33.915 --> 03:17:37.565
only four percentage points, um, I would probably

3263
03:17:38.385 --> 03:17:41.005
not consider those two different from each other.

3264
03:17:41.415 --> 03:17:43.485
Other, um, I do like in the base model,

3265
03:17:43.545 --> 03:17:46.085
how there's this big gap between the leading one

3266
03:17:46.225 --> 03:17:48.125
and the, uh, a second level

3267
03:17:48.315 --> 03:17:51.565
because that allows me to make a confident decision that,

3268
03:17:51.745 --> 03:17:54.285
uh, we are predicting, hello, I'm a male model

3269
03:17:54.385 --> 03:17:55.925
to be the number one, um,

3270
03:17:58.655 --> 03:18:00.515
return output value.

3271
03:18:01.865 --> 03:18:03.525
All right, let's get back to the lecture slides.

3272
03:18:04.565 --> 03:18:05.985
Oh, shoot, uh, five minute break.

3273
03:18:06.525 --> 03:18:08.585
So we are back at, uh,

3274
03:18:08.645 --> 03:18:13.335
1245, uh, PST.

3275
03:18:14.035 --> 03:18:16.615
Um, and I do want to take this break instead of just power

3276
03:18:16.615 --> 03:18:18.695
through because I really want to pay attention

3277
03:18:18.695 --> 03:18:20.135
to the last couple slides.

3278
03:18:20.395 --> 03:18:23.375
And at the end of four hours, we're all toast.

3279
03:18:23.875 --> 03:18:25.175
So just five minutes

3280
03:18:25.675 --> 03:18:27.855
and then we will power through the next few slides.

3281
03:18:37.345 --> 03:18:41.885
All right, let's get back, uh, rolling on Albert.

3282
03:18:45.745 --> 03:18:50.305
So Albert is a like version of Bert.

3283
03:18:51.275 --> 03:18:53.095
So Bert is that base model.

3284
03:18:53.905 --> 03:18:57.165
Um, and then we have reduced it in size,

3285
03:18:57.985 --> 03:19:01.845
but maintained the robustness of the analysis.

3286
03:19:02.585 --> 03:19:05.435
Um, so similar to Bert,

3287
03:19:05.435 --> 03:19:08.435
Albert uses self supervised learning

3288
03:19:08.785 --> 03:19:11.835
with mass language modeling and MLM, um,

3289
03:19:12.495 --> 03:19:16.555
and again, uh, NSP is not used in Albert.

3290
03:19:16.895 --> 03:19:19.995
Uh, Albert uses sentence order prediction instead.

3291
03:19:20.375 --> 03:19:24.815
Uh, SOP umbert consists

3292
03:19:24.815 --> 03:19:26.895
of just over a million parameters,

3293
03:19:27.235 --> 03:19:29.095
and Albert is significantly smaller.

3294
03:19:29.225 --> 03:19:33.135
We'll be, uh, looking at a really nice visual presenting the

3295
03:19:33.135 --> 03:19:34.455
difference between, uh, bird

3296
03:19:34.475 --> 03:19:36.135
and Albert in the next couple of slides.

3297
03:19:37.395 --> 03:19:40.295
So, um, it does a few different,

3298
03:19:40.315 --> 03:19:42.655
or it, um, simplifies a few different ways.

3299
03:19:43.145 --> 03:19:47.335
Cross layer parameter sharing, factored embedding, um,

3300
03:19:47.395 --> 03:19:50.055
and then inner sentence, uh, coherence prediction.

3301
03:19:55.755 --> 03:20:00.375
Um, so Albert first we're getting into cross

3302
03:20:00.545 --> 03:20:02.055
layer parameter sharing.

3303
03:20:04.025 --> 03:20:07.605
Um, so in cross layer parameter sharing,

3304
03:20:08.825 --> 03:20:10.605
um, it's an interesting method

3305
03:20:10.625 --> 03:20:12.325
for reducing the number of parameters.

3306
03:20:12.385 --> 03:20:15.565
In the Burt model, Burt consists of n number

3307
03:20:15.625 --> 03:20:18.965
of encoder layers, uh, usually a ton, um,

3308
03:20:19.505 --> 03:20:22.525
and sometimes it could be 12, sometimes more.

3309
03:20:22.945 --> 03:20:24.125
Uh, remember those different types

3310
03:20:24.145 --> 03:20:25.885
of encodings we talked about at the beginning

3311
03:20:25.885 --> 03:20:30.515
of the lecture, um, all shared where,

3312
03:20:30.895 --> 03:20:32.435
uh, we share all the parameters

3313
03:20:32.435 --> 03:20:34.155
and the sublayers of the first encoder

3314
03:20:34.155 --> 03:20:36.075
with all the sublayers of the other encoders

3315
03:20:36.735 --> 03:20:38.315
shared Feedforward network.

3316
03:20:39.475 --> 03:20:42.255
Um, and in this, we only share the parameters

3317
03:20:42.255 --> 03:20:45.335
of the feedforward network of the first layer encoder

3318
03:20:45.405 --> 03:20:46.975
with the Feedforward net network

3319
03:20:47.035 --> 03:20:48.295
of the other encoder layers.

3320
03:20:48.315 --> 03:20:49.375
So we're simplifying that.

3321
03:20:49.955 --> 03:20:54.015
Um, and also another option would be shared attention.

3322
03:20:54.795 --> 03:20:57.855
And in this layer, um, we only share the parameters

3323
03:20:57.855 --> 03:21:01.055
of multi-head attention, um, of the first encoder layer

3324
03:21:01.125 --> 03:21:03.335
with multi-head attention of the other encoder layers.

3325
03:21:12.785 --> 03:21:15.965
And then this is kind of just a, a visual description.

3326
03:21:16.585 --> 03:21:20.165
Um, so we can kind of see how Albert, uh,

3327
03:21:20.165 --> 03:21:24.245
potentially shortcuts some of those encoder steps to, um,

3328
03:21:25.695 --> 03:21:27.355
reduce model complexity.

3329
03:21:28.305 --> 03:21:29.525
Uh, we have multiple layers,

3330
03:21:29.745 --> 03:21:33.125
and if we share the weights of those multiple layers, um,

3331
03:21:33.345 --> 03:21:37.165
as opposed to, uh, have each individual weights replicated

3332
03:21:37.185 --> 03:21:39.645
for each of those layers, then the model size will decrease.

3333
03:21:40.385 --> 03:21:43.085
So we may have factor weights in multi-head attrition

3334
03:21:43.275 --> 03:21:45.965
that are duplicates of add and normalized

3335
03:21:46.105 --> 03:21:48.245
and, uh, fee forward, et cetera, et cetera.

3336
03:21:48.505 --> 03:21:50.245
So we remove those duplicates,

3337
03:21:50.745 --> 03:21:54.205
and that is how Albert is faster by the removing

3338
03:21:54.225 --> 03:21:55.525
of duplicate weights.

3339
03:22:03.985 --> 03:22:07.325
Um, so here's another option in Albert

3340
03:22:07.585 --> 03:22:09.685
or another way that it reduces complexity.

3341
03:22:10.145 --> 03:22:12.485
Uh, factorized embedding parameterization.

3342
03:22:13.185 --> 03:22:14.725
Um, in the Burt model,

3343
03:22:15.305 --> 03:22:17.885
its improvements like Excel net and Roberta.

3344
03:22:18.505 --> 03:22:20.885
Um, the input layer embedding

3345
03:22:20.905 --> 03:22:23.605
and the hidden layer embedding have the same size roughly.

3346
03:22:24.305 --> 03:22:26.085
Um, so remember that multi-layer visual

3347
03:22:26.085 --> 03:22:29.805
that we talked about earlier in the, uh, lecture, um,

3348
03:22:30.355 --> 03:22:32.485
both term, both in terms of modeling

3349
03:22:32.585 --> 03:22:36.125
and an application, um, that's pretty suboptimal.

3350
03:22:36.705 --> 03:22:38.965
Um, while it is effective,

3351
03:22:38.965 --> 03:22:40.885
in some cases it could be improved,

3352
03:22:41.065 --> 03:22:42.645
and that is improved in Albert.

3353
03:22:43.595 --> 03:22:46.935
Um, in this model, the two embedding matrices are separated

3354
03:22:46.935 --> 03:22:50.535
at the input level, and they only need to refine, uh,

3355
03:22:50.535 --> 03:22:51.895
the context independent learning.

3356
03:22:52.115 --> 03:22:56.175
Um, and the hidden level H requires some more

3357
03:22:56.175 --> 03:22:57.255
context dependent learning.

3358
03:22:58.115 --> 03:23:02.855
Um, and that leads to a pretty huge reduction in size,

3359
03:23:03.595 --> 03:23:07.295
um, up to about 80% with only a minor, uh,

3360
03:23:07.395 --> 03:23:09.255
per performance drop when compared to Bert.

3361
03:23:09.595 --> 03:23:12.095
So again, this is why I usually select Albert,

3362
03:23:12.365 --> 03:23:13.455
because, um,

3363
03:23:13.595 --> 03:23:16.655
it is significantly less computationally expensive, uh,

3364
03:23:16.895 --> 03:23:17.975
compared to Bert or Roberta.

3365
03:23:18.435 --> 03:23:21.985
So start with Albert, um,

3366
03:23:21.985 --> 03:23:22.905
unless you're explaining

3367
03:23:22.905 --> 03:23:23.945
something, and then start with the base.

3368
03:23:24.505 --> 03:23:27.705
Bert. Um, so

3369
03:23:28.275 --> 03:23:32.135
inter sentence co coherence prediction, uh, similar

3370
03:23:32.275 --> 03:23:37.175
to Bert Albert used the MLM mass language model in training.

3371
03:23:38.225 --> 03:23:41.045
Um, instead of using next sentence prediction,

3372
03:23:41.055 --> 03:23:42.205
we're using SOP.

3373
03:23:42.865 --> 03:23:45.015
Um, so sentence order prediction,

3374
03:23:45.015 --> 03:23:47.375
which we'll discuss here in a little bit.

3375
03:23:48.635 --> 03:23:53.455
Um, so it's a binary classification loss is what it uses.

3376
03:23:53.875 --> 03:23:57.055
Uh, remember loss, uh, function calculation.

3377
03:23:57.475 --> 03:24:00.055
Uh, it's kind of like the residuals of, um,

3378
03:24:00.415 --> 03:24:01.695
transformer um, models.

3379
03:24:03.355 --> 03:24:06.855
Um, so it uses that to predict whether two segments occur,

3380
03:24:07.275 --> 03:24:10.375
uh, consecutively, um, in original text.

3381
03:24:10.955 --> 03:24:13.975
Um, and this checks the loss for coherence as well as,

3382
03:24:14.275 --> 03:24:16.175
um, next sentence.

3383
03:24:16.555 --> 03:24:19.015
But SOP only looks for sentence coherence.

3384
03:24:19.675 --> 03:24:24.475
So SOP is only looking for sentence coherence, not, uh,

3385
03:24:24.505 --> 03:24:25.795
what the next sentence may be.

3386
03:24:30.425 --> 03:24:34.405
Um, so this is interesting here.

3387
03:24:34.875 --> 03:24:39.405
Burt Base has nine times as many parameters as Albert base.

3388
03:24:39.845 --> 03:24:41.045
Remember, there's different models sizes.

3389
03:24:41.625 --> 03:24:44.765
Um, so Burt Base has nine times

3390
03:24:45.825 --> 03:24:47.125
as many parameters as Albert.

3391
03:24:47.945 --> 03:24:52.285
Um, Burt large has 18 times as many parameters

3392
03:24:52.385 --> 03:24:54.805
as Albert large, so it's way bigger.

3393
03:24:57.115 --> 03:25:00.855
Um, so that's kind of the top line you use Albert

3394
03:25:00.855 --> 03:25:03.535
because it's much faster, faster.

3395
03:25:03.535 --> 03:25:06.575
Um, here if Burt large was used as a baseline,

3396
03:25:06.915 --> 03:25:10.295
Albert large is 1.7 times as likely,

3397
03:25:11.175 --> 03:25:15.275
or 1.7 times faster, uh, in iterating through the data.

3398
03:25:16.305 --> 03:25:19.885
Um, and while you may not care about the speed,

3399
03:25:20.715 --> 03:25:23.685
what the speed allows is, um,

3400
03:25:24.805 --> 03:25:27.175
potential preventing overflow errors

3401
03:25:27.485 --> 03:25:30.855
that could cause your analysis to just be crushed, um,

3402
03:25:30.855 --> 03:25:34.615
because the code will not, uh, compile.

3403
03:25:39.305 --> 03:25:42.325
All right? And Ekra is not used as frequently,

3404
03:25:42.505 --> 03:25:43.885
but we're gonna touch on it briefly.

3405
03:25:45.055 --> 03:25:49.395
Um, efficient learning and, um, sorry.

3406
03:25:50.075 --> 03:25:51.395
Electra is efficient learning

3407
03:25:51.615 --> 03:25:54.755
and a coder that classifies token replacements accurately.

3408
03:25:55.815 --> 03:26:00.155
Um, so it's known for language understanding

3409
03:26:00.415 --> 03:26:03.945
and some efficiency, um,

3410
03:26:05.245 --> 03:26:08.265
and it's best described.

3411
03:26:08.925 --> 03:26:13.835
Uh, in the next slide here you can see, um,

3412
03:26:14.335 --> 03:26:17.795
we change and identify what is the original

3413
03:26:17.855 --> 03:26:18.875
and what is replaced.

3414
03:26:19.295 --> 03:26:20.435
So what is masked?

3415
03:26:21.495 --> 03:26:23.475
Um, so we can see the generator,

3416
03:26:23.475 --> 03:26:27.955
which is typically a small MLM, um, the input value,

3417
03:26:28.535 --> 03:26:31.925
the mask, uh, the is masked,

3418
03:26:32.225 --> 03:26:33.645
and then chief is the original.

3419
03:26:34.065 --> 03:26:37.845
And then, uh, co, uh, the original here is cooked,

3420
03:26:38.345 --> 03:26:39.365
uh, which is masked.

3421
03:26:40.065 --> 03:26:41.925
Um, and then we can see here

3422
03:26:44.105 --> 03:26:48.695
the Electra model identifies what has been replaced, um,

3423
03:26:48.965 --> 03:26:51.215
that is much more computationally difficult

3424
03:26:51.325 --> 03:26:54.495
because not only do you have to identify what is, um,

3425
03:26:55.075 --> 03:26:58.175
the original word, you also have to identify, uh,

3426
03:26:58.275 --> 03:26:59.295
has that been replaced.

3427
03:27:07.385 --> 03:27:11.165
So, um, it, it is typically recommended over using Electra,

3428
03:27:11.345 --> 03:27:14.885
um, to fine tune Burt, uh, for a specific task, uh,

3429
03:27:14.885 --> 03:27:17.765
like we have been doing in a couple of the, uh, code demos.

3430
03:27:19.115 --> 03:27:22.295
Um, so during that fine tuning, we can adjust the weights

3431
03:27:22.295 --> 03:27:25.655
of the model, um, by using the pre-train model along

3432
03:27:25.775 --> 03:27:27.215
with additional classification layers.

3433
03:27:28.195 --> 03:27:31.095
Um, we can update only the weights

3434
03:27:31.095 --> 03:27:32.095
of the classification layer

3435
03:27:32.835 --> 03:27:35.335
and not the pre-trained Burt model.

3436
03:27:36.155 --> 03:27:37.855
Um, and when we do this, it becomes

3437
03:27:38.565 --> 03:27:41.015
very similar using the pre-trained model as the,

3438
03:27:41.205 --> 03:27:42.245
the feature extractor.

3439
03:27:47.655 --> 03:27:50.835
And then here's some more examples about fine tuning Bert

3440
03:27:51.015 --> 03:27:52.195
for text classification.

3441
03:27:53.665 --> 03:27:56.925
Um, sentiment analysis is really important.

3442
03:27:57.185 --> 03:27:59.325
Um, I do sentiment analysis a ton in my work.

3443
03:27:59.625 --> 03:28:03.755
Um, so looking at the sentence I love

3444
03:28:04.165 --> 03:28:05.995
Paris, is that positive?

3445
03:28:06.135 --> 03:28:07.355
Is that negative sentiment?

3446
03:28:08.255 --> 03:28:12.635
Um, so once we tokenize it, um, we can identify, uh,

3447
03:28:12.775 --> 03:28:16.515
the key kernel, um, token of love, making

3448
03:28:16.585 --> 03:28:18.155
that a positive indication.

3449
03:28:18.535 --> 03:28:22.155
Um, so that, um, text classification is something

3450
03:28:22.155 --> 03:28:24.915
that we can do with, um, Burt really well.

3451
03:28:31.265 --> 03:28:35.205
Um, and then here, fine tuning,

3452
03:28:35.275 --> 03:28:37.325
this is just another way to fine tune.

3453
03:28:38.105 --> 03:28:41.765
Um, we have that same sentence. Uh, I love Paris.

3454
03:28:42.305 --> 03:28:45.965
We pass it to our naive vanilla pre-trained Bert.

3455
03:28:46.825 --> 03:28:50.405
Um, and then we have the Bert output,

3456
03:28:51.745 --> 03:28:55.485
um, that we now pass to, um,

3457
03:28:56.275 --> 03:28:58.165
more customized, uh,

3458
03:28:59.065 --> 03:29:01.325
or more fine tuning functions like the soft

3459
03:29:01.385 --> 03:29:02.445
max and the Feedforward network.

3460
03:29:02.945 --> 03:29:07.185
And from that, we can get, we're pretty sure 90% sure

3461
03:29:07.185 --> 03:29:08.785
that this is positive sentiment

3462
03:29:09.405 --> 03:29:13.065
and there's a chance 10% chance that it's negative.

3463
03:29:13.805 --> 03:29:15.145
Um, and we would be correct.

3464
03:29:15.205 --> 03:29:18.695
We as humans know, love is usually positive usually.

3465
03:29:24.375 --> 03:29:26.475
And then fine tuning for question answering.

3466
03:29:26.895 --> 03:29:30.515
Um, so previously we had classification,

3467
03:29:30.895 --> 03:29:32.355
now we have question answering.

3468
03:29:32.935 --> 03:29:37.115
So, uh, Bert can do both depending on what function you use.

3469
03:29:39.945 --> 03:29:44.345
Um, and for question answering, um,

3470
03:29:44.575 --> 03:29:47.145
just reiterating what we previously displayed in a couple

3471
03:29:47.145 --> 03:29:48.305
of different code demos.

3472
03:29:49.085 --> 03:29:51.465
Um, we have the pre-training, Burt

3473
03:29:51.975 --> 03:29:53.505
over on the left hand side here.

3474
03:29:54.645 --> 03:29:59.355
Um, and this provides some decent output, um,

3475
03:29:59.685 --> 03:30:00.915
which we can then take

3476
03:30:01.375 --> 03:30:04.715
and potentially improve in the fine tuning step.

3477
03:30:05.745 --> 03:30:08.045
So if our goal is to, um,

3478
03:30:09.505 --> 03:30:11.535
adapt the pre-trained birched

3479
03:30:11.635 --> 03:30:14.455
for a specific question answering tasks, um,

3480
03:30:14.515 --> 03:30:18.295
and our dataset might be like a, a labeled dataset, uh,

3481
03:30:18.295 --> 03:30:20.415
consisting of question answer pairs.

3482
03:30:21.285 --> 03:30:24.225
Um, each pair usually includes something like a,

3483
03:30:24.305 --> 03:30:26.505
a context passage or a question about the passage

3484
03:30:26.605 --> 03:30:28.065
and then the corresponding answer.

3485
03:30:28.065 --> 03:30:29.225
So that's our data set.

3486
03:30:30.165 --> 03:30:34.305
Um, so what our tokenization would be is the question

3487
03:30:35.085 --> 03:30:37.985
and the answer and the context passage.

3488
03:30:38.155 --> 03:30:40.125
Those are all tokenized using the same

3489
03:30:40.225 --> 03:30:41.285
sub word tokenization.

3490
03:30:42.865 --> 03:30:47.435
Um, and from that input, we then,

3491
03:30:47.775 --> 03:30:52.005
um, fine tune, um, given

3492
03:30:52.645 --> 03:30:53.805
packages, uh,

3493
03:30:53.985 --> 03:30:56.285
or passages like Bert is a

3494
03:30:56.885 --> 03:30:58.525
powerful natural language processing model.

3495
03:30:58.665 --> 03:31:01.365
We can dynamically mask those things, um,

3496
03:31:01.385 --> 03:31:02.925
and using all of the other tools

3497
03:31:03.465 --> 03:31:07.445
in our now filled tool belts to help fine tune, um,

3498
03:31:07.545 --> 03:31:11.685
our answers, um, and improve, uh, our output.

3499
03:31:20.935 --> 03:31:24.835
Um, so fine tuning,

3500
03:31:25.665 --> 03:31:29.235
this is exactly what we saw in the code demo

3501
03:31:29.885 --> 03:31:32.635
where we have our question, what is an immune system?

3502
03:31:35.245 --> 03:31:39.175
And we have our output paragraph, um,

3503
03:31:39.275 --> 03:31:40.535
the immune system is,

3504
03:31:40.715 --> 03:31:42.495
and then you can see the answer right here.

3505
03:31:44.675 --> 03:31:48.095
So we can pass many, many, many, many

3506
03:31:48.095 --> 03:31:50.975
of these question paragraph pairs like we have in our

3507
03:31:50.975 --> 03:31:55.335
example dataset, um, to fine tune this model.

3508
03:31:55.735 --> 03:31:58.455
'cause, um, having more trainings, data sets,

3509
03:31:58.455 --> 03:32:01.375
especially like, um, if you are training

3510
03:32:01.475 --> 03:32:05.095
or if you are fine tuning a model for the context of, uh,

3511
03:32:05.515 --> 03:32:07.055
the healthcare analytics,

3512
03:32:07.595 --> 03:32:09.815
you may pass this question paragraph pair

3513
03:32:10.275 --> 03:32:12.775
as an additional context for the model here.

3514
03:32:14.175 --> 03:32:15.715
Um, so that would help fine tune.

3515
03:32:24.355 --> 03:32:27.375
Um, and then like we were saying

3516
03:32:27.375 --> 03:32:28.935
before, this is another similar

3517
03:32:28.935 --> 03:32:30.215
example with a different question.

3518
03:32:30.635 --> 03:32:34.615
We have what system, uh, the immune system tissue, so

3519
03:32:34.645 --> 03:32:36.655
that question and that answer

3520
03:32:36.655 --> 03:32:38.015
that we saw on the previous slide.

3521
03:32:38.355 --> 03:32:39.815
And then we have the pre-trained Bert.

3522
03:32:40.485 --> 03:32:43.785
And then we can apply things like, um, multiple, uh,

3523
03:32:44.065 --> 03:32:45.625
training question paragraph sets

3524
03:32:45.965 --> 03:32:48.385
or, uh, running it through a dot product, uh,

3525
03:32:48.385 --> 03:32:50.745
and soft, uh, max transformations.

3526
03:32:51.405 --> 03:32:53.145
Um, and then we have the probabilities

3527
03:32:53.285 --> 03:32:54.625
of the start and end word.

3528
03:32:59.195 --> 03:33:03.845
And then, um, that's pretty much what we've got today.

3529
03:33:05.735 --> 03:33:08.035
Um, oh wow, 45 seconds left.

3530
03:33:08.035 --> 03:33:09.435
Let's get through the brain teaser section.

3531
03:33:10.475 --> 03:33:15.295
Um, so we can have, um, four Bert, some general

3532
03:33:15.595 --> 03:33:20.235
how to fine tune the model, um, and think about

3533
03:33:21.005 --> 03:33:25.805
after this class, how can we fine tune the model to generate

3534
03:33:26.345 --> 03:33:28.605
the answer word by word.

3535
03:33:29.185 --> 03:33:30.245
Um, yes or no.

3536
03:33:31.455 --> 03:33:35.885
Um, and just consider that in your day

3537
03:33:35.885 --> 03:33:36.885
to day going forward.

3538
03:33:37.665 --> 03:33:39.765
Um, I think you'll get these slides afterwards

3539
03:33:39.825 --> 03:33:40.965
so we can go through the summary.

3540
03:33:42.125 --> 03:33:46.425
Um, and there's a couple of links to demos, um,

3541
03:33:46.645 --> 03:33:49.865
and also some, uh, references and literature after that.

3542
03:33:52.505 --> 03:33:57.485
Um, so again, feel free to sign up to the, um,

3543
03:33:58.945 --> 03:34:00.285
uh, career coaching.

3544
03:34:00.795 --> 03:34:05.565
Feel free to add me on LinkedIn, um, and stay connected.

3545
03:34:05.865 --> 03:34:08.645
Um, and yeah, I'm always happy to teach

3546
03:34:08.825 --> 03:34:11.725
and discuss, um, natural language processing

3547
03:34:11.825 --> 03:34:15.165
or statistical concepts or anything like that.

3548
03:34:16.665 --> 03:34:19.565
Um, so thank you everybody for your patience.

3549
03:34:20.265 --> 03:34:21.645
Um, thank you to the TA

3550
03:34:21.825 --> 03:34:24.085
for answering a ton of great questions.

3551
03:34:25.855 --> 03:34:29.595
Um, and I'll stick around to help answer in the chat

3552
03:34:29.815 --> 03:34:31.675
as well if there's any last minute questions.
