WEBVTT - This file was automatically generated by VIMEO

0
00:00:07.955 --> 00:00:08.445
Awesome.

1
00:00:08.745 --> 00:00:11.465
Uh, thanks aj. Alright, let's get started.

2
00:00:11.725 --> 00:00:15.265
Uh, so for today's agenda, we'll go through CNNs

3
00:00:15.265 --> 00:00:17.625
and nns, uh, roughly two hours each.

4
00:00:18.605 --> 00:00:21.665
Uh, so after NN theoretical material, we'll go

5
00:00:21.665 --> 00:00:23.065
through a coding workbook.

6
00:00:23.995 --> 00:00:26.775
And, uh, after wrapping that up, we'll go through r

7
00:00:26.775 --> 00:00:29.455
and n theoretical material and a coding workbook.

8
00:00:30.075 --> 00:00:33.015
Um, so that's, uh, uh, enough for today.

9
00:00:33.595 --> 00:00:37.615
Uh, so we'll, we'll try to stay track on track, uh,

10
00:00:37.875 --> 00:00:39.255
on for time today.

11
00:00:39.435 --> 00:00:40.775
Um, make sure that, you know,

12
00:00:40.835 --> 00:00:43.095
we can finish this in four hour time window.

13
00:00:43.675 --> 00:00:46.655
Uh, I think we can, uh, it's, it's a bit of material,

14
00:00:46.715 --> 00:00:48.695
but I think we can do that in four hours.

15
00:00:49.475 --> 00:00:52.015
Uh, feel free to ask questions like you did yesterday.

16
00:00:52.235 --> 00:00:54.775
Uh, use the q and a feature

17
00:00:55.635 --> 00:00:58.455
and, uh, I'll try to answer them, um,

18
00:00:59.075 --> 00:01:00.535
in, in short time windows.

19
00:01:02.255 --> 00:01:06.945
Cool. So without further ado, diving deep into CNN's,

20
00:01:07.975 --> 00:01:10.755
um, for that, actually, uh, I want you

21
00:01:10.755 --> 00:01:12.635
to noodle on these questions a little bit.

22
00:01:13.135 --> 00:01:16.195
Uh, just get your mind thinking like what's happening here.

23
00:01:16.535 --> 00:01:18.715
Uh, like, you know, if you look at these questions,

24
00:01:18.715 --> 00:01:22.995
how Google photos tax different in individuals in the photo,

25
00:01:24.055 --> 00:01:26.715
uh, you might have experienced that, uh, uh,

26
00:01:26.725 --> 00:01:30.155
while using Google Photos or Amazon photos, uh,

27
00:01:30.335 --> 00:01:33.395
or, you know, how does autonomous driving work?

28
00:01:34.415 --> 00:01:35.475
Um, uh,

29
00:01:35.655 --> 00:01:38.555
or, you know, if you use Snapchat, you know, how,

30
00:01:38.615 --> 00:01:41.995
how is a mask overlaid on, on a particular part of the face?

31
00:01:42.735 --> 00:01:45.795
Um, and, uh, so think, think along those lines,

32
00:01:45.855 --> 00:01:46.875
how that is happening.

33
00:01:47.295 --> 00:01:52.075
Uh, what are the, uh, core methods that are happening?

34
00:01:52.105 --> 00:01:54.755
Well, you don't have to have, like, you know, deep answers,

35
00:01:54.855 --> 00:01:56.275
but think, think along those lines.

36
00:01:59.405 --> 00:02:03.055
Alright, so, um, as, as I said earlier,

37
00:02:03.275 --> 00:02:05.335
the first thing is we'll try to develop why,

38
00:02:05.835 --> 00:02:06.975
uh, CNNs are used.

39
00:02:07.085 --> 00:02:09.975
What are the, some of the salient features of CNN,

40
00:02:10.235 --> 00:02:13.015
why they're applicable for images, um,

41
00:02:13.445 --> 00:02:16.815
some principles applicable to CNN and applications.

42
00:02:20.885 --> 00:02:24.405
Let me set up my iPad real quick.

43
00:02:24.865 --> 00:02:27.315
Uh, sec

44
00:02:30.925 --> 00:02:32.165
everyone follow, become, host.

45
00:02:32.555 --> 00:02:36.285
Okay. Looks good. Alright, so, brief agenda.

46
00:02:37.365 --> 00:02:39.905
Um, we'll look at the principles

47
00:02:39.905 --> 00:02:43.775
of CNNI look at different layers that make up, uh, you know,

48
00:02:43.775 --> 00:02:46.615
convolution, neur networks, uh, different types

49
00:02:46.615 --> 00:02:49.935
of CNN models out there, um, that are,

50
00:02:50.075 --> 00:02:51.175
uh, industry standard.

51
00:02:51.715 --> 00:02:54.095
Uh, what are some advantages, limitations

52
00:02:54.835 --> 00:02:56.375
and applications of CNN.

53
00:02:56.835 --> 00:02:59.885
Um, and then we'll wrap it up with a coding walkthrough.

54
00:03:05.305 --> 00:03:08.405
So first we'll think of why not FNN for images.

55
00:03:08.665 --> 00:03:11.125
Why, uh, why is, uh, why, why do we have

56
00:03:11.125 --> 00:03:14.165
to invent a new architecture for, for, uh, images?

57
00:03:14.345 --> 00:03:16.965
So I want to contrast that a little bit.

58
00:03:17.425 --> 00:03:20.445
Uh, like what happens, you use an FNN on images.

59
00:03:20.705 --> 00:03:25.005
So think of, um, a one megapixel image, right?

60
00:03:25.065 --> 00:03:29.405
So a one megapixel image has, um, um,

61
00:03:30.065 --> 00:03:33.205
you have thousand by thousand pixels.

62
00:03:34.755 --> 00:03:39.615
Um, that's order of 10.6, um, features to begin with.

63
00:03:39.635 --> 00:03:44.595
That's your input. And with FNN, um,

64
00:03:45.495 --> 00:03:48.595
as we've seen yesterday, we have fully connected layers.

65
00:03:48.595 --> 00:03:50.515
That's the salient feature of FNN.

66
00:03:50.575 --> 00:03:53.555
So, so you have your layer one

67
00:03:53.815 --> 00:03:56.755
and layer two, which are fully connected.

68
00:03:58.015 --> 00:04:02.095
Um, and that is,

69
00:04:03.145 --> 00:04:07.535
let's say your hidden layer is also, um, the same size

70
00:04:07.555 --> 00:04:09.535
as your input layer 10, 4 6.

71
00:04:10.075 --> 00:04:13.825
So now the, let me change the color,

72
00:04:14.645 --> 00:04:17.825
the weight parameters, the weight matrix that you need

73
00:04:17.825 --> 00:04:19.865
to learn is of order

74
00:04:19.925 --> 00:04:23.705
of ten four, six times 10, 4 6 or ten four twelve.

75
00:04:25.555 --> 00:04:29.735
That's a huge explosion of parameters, uh, if you try

76
00:04:29.735 --> 00:04:34.655
to use an FNN uh, uh, for a image kind of problem.

77
00:04:36.555 --> 00:04:40.305
So we want to try to make that more efficient

78
00:04:40.885 --> 00:04:45.225
by better parameter sharing, by kind

79
00:04:45.225 --> 00:04:47.785
of exploiting the principles of images,

80
00:04:48.055 --> 00:04:49.225
like how images work.

81
00:04:50.125 --> 00:04:52.665
So images have certain inherent principles we'll look at,

82
00:04:52.925 --> 00:04:57.705
uh, that make them, um, suitable for developing some,

83
00:04:58.085 --> 00:04:59.905
uh, principles that will exploit

84
00:04:59.905 --> 00:05:01.385
through the CNN architecture.

85
00:05:02.465 --> 00:05:06.565
So first thing is the principle of, uh,

86
00:05:06.695 --> 00:05:08.525
let's see, next slide.

87
00:05:09.665 --> 00:05:12.475
Yeah, the first thing is the principle of locality.

88
00:05:13.575 --> 00:05:15.395
So what it means is fairly simple.

89
00:05:15.535 --> 00:05:18.205
So when you look at an image a

90
00:05:18.665 --> 00:05:21.925
and take a part of the image, uh, let's say in this case,

91
00:05:22.025 --> 00:05:26.045
if you're taking a part of the flower here, um,

92
00:05:27.295 --> 00:05:28.875
the pixels in

93
00:05:28.875 --> 00:05:31.355
that vicinity have more correlation to each other.

94
00:05:31.535 --> 00:05:36.075
So, so a nearby pixel is likely to be very related

95
00:05:36.215 --> 00:05:39.155
to a part of the flower rather than,

96
00:05:39.455 --> 00:05:40.635
you know, a part of the sky.

97
00:05:41.695 --> 00:05:44.145
So, uh, that's one.

98
00:05:45.085 --> 00:05:48.825
Um, so that's one, uh, principle of the images, uh,

99
00:05:48.825 --> 00:05:51.265
that is very useful and we'll see how to exploit that

100
00:05:51.265 --> 00:05:52.305
through CNNs.

101
00:05:52.765 --> 00:05:56.265
Uh, so what it means is nearby pixels are more,

102
00:05:56.735 --> 00:05:59.205
more important to tell, tell you about

103
00:05:59.205 --> 00:06:03.285
what the pixel is about than, uh, farther of pixels.

104
00:06:03.335 --> 00:06:06.365
There is more information conveyed within the nearby pixels,

105
00:06:06.985 --> 00:06:08.925
um, about that pixel.

106
00:06:11.165 --> 00:06:13.865
So that's one principle. It's called principle of locality.

107
00:06:15.815 --> 00:06:19.055
And, um, the learning patterns

108
00:06:19.055 --> 00:06:22.975
and features within these localized regions, um, are

109
00:06:23.585 --> 00:06:24.895
inherently, uh,

110
00:06:25.755 --> 00:06:28.975
unique rather than processing the entire image.

111
00:06:29.395 --> 00:06:32.175
Uh, so you can take small parts of the image

112
00:06:32.915 --> 00:06:37.095
and learn about those parts, um,

113
00:06:37.635 --> 00:06:39.695
and capture the learning patterns there

114
00:06:39.755 --> 00:06:41.615
and features there rather than, you know,

115
00:06:41.615 --> 00:06:42.815
taking the whole image.

116
00:06:46.525 --> 00:06:50.345
The second thing is, uh, translation in variance.

117
00:06:51.535 --> 00:06:53.765
Let's say you're trying to find a cat in image.

118
00:06:54.665 --> 00:06:58.725
So it doesn't matter if the cat is, uh, you know, here

119
00:06:59.705 --> 00:07:02.085
or in the next image of the cat is here

120
00:07:02.665 --> 00:07:04.525
or in the next image, and the cat is here.

121
00:07:04.625 --> 00:07:08.925
So you're trying to find the cat, um, whether you know

122
00:07:08.945 --> 00:07:12.685
how it's oriented or where it is in the particular image,

123
00:07:13.385 --> 00:07:17.205
uh, your goal of the model is trying to detect that cat.

124
00:07:18.185 --> 00:07:20.935
So if you, if your model is, um,

125
00:07:21.795 --> 00:07:25.495
not translation in variant, then you need to train the model

126
00:07:25.525 --> 00:07:28.855
with every possible combination of cat, uh,

127
00:07:28.995 --> 00:07:31.935
and various parts of the image

128
00:07:32.395 --> 00:07:35.775
or various orientations, uh, to be able

129
00:07:35.775 --> 00:07:37.535
to learn.

130
00:07:38.075 --> 00:07:40.895
Uh, and you, it won't even guarantee you in the future.

131
00:07:41.025 --> 00:07:44.215
Let's say if the cat is like upside down in this part

132
00:07:44.215 --> 00:07:47.335
of the image and you didn't learn from an image of

133
00:07:47.335 --> 00:07:51.015
that sort, then, um, the model will break.

134
00:07:51.395 --> 00:07:54.335
It won't generalize. So the goal is to develop a network

135
00:07:54.485 --> 00:07:57.495
that would be translation very irrespective of, you know,

136
00:07:57.675 --> 00:08:00.895
how a particular object is oriented in an image.

137
00:08:01.555 --> 00:08:04.375
Um, we would want the model to learn that pattern.

138
00:08:05.355 --> 00:08:07.215
So that's called translation in variance.

139
00:08:12.625 --> 00:08:15.845
So these are the two key principles, locality

140
00:08:16.625 --> 00:08:20.925
and translation variance that we want to exploit, uh, for,

141
00:08:21.305 --> 00:08:24.205
uh, making the network architectures more efficient.

142
00:08:24.865 --> 00:08:27.925
Uh, so as we've seen with FNN, uh, we, we,

143
00:08:28.025 --> 00:08:31.125
we are in the order of magnitude of 10, 12

144
00:08:31.125 --> 00:08:34.245
with just one MAGA pixel cell image and one hidden layer.

145
00:08:35.805 --> 00:08:39.305
So let's see if we can bring that down to a smaller number

146
00:08:39.495 --> 00:08:40.825
with a better architecture.

147
00:08:46.975 --> 00:08:48.155
Um, just to caveat

148
00:08:48.335 --> 00:08:53.125
before we go into this math, um, from a interview

149
00:08:53.125 --> 00:08:55.980
and practice standpoint, these principles, locality

150
00:08:55.980 --> 00:09:00.125
and translation in variants are, uh, important for you

151
00:09:00.125 --> 00:09:01.445
to know from a theoretical perspective.

152
00:09:01.985 --> 00:09:04.565
But this map is more for your understanding.

153
00:09:04.825 --> 00:09:08.885
Um, I can pretty much guarantee you this math won't be you

154
00:09:09.215 --> 00:09:12.445
asked in interviews or you would use this in practice,

155
00:09:12.905 --> 00:09:16.045
but we'll still go through this math for you to understand

156
00:09:16.115 --> 00:09:20.725
what you know, how to apply this, um, how, how

157
00:09:20.725 --> 00:09:23.205
to more intuitively learn from math, like, uh,

158
00:09:23.595 --> 00:09:25.125
this translation variance

159
00:09:25.345 --> 00:09:27.725
and locality principles translate to a,

160
00:09:28.445 --> 00:09:29.565
a better architecture.

161
00:09:30.625 --> 00:09:33.165
So for that, let me use a whiteboard.

162
00:09:35.775 --> 00:09:40.445
So in a regular FNN, uh, let's start with that.

163
00:09:40.585 --> 00:09:45.365
So you have your input vector in one dimension,

164
00:09:46.105 --> 00:09:48.915
and you probably have a hidden, uh,

165
00:09:49.645 --> 00:09:51.435
layer, H one.

166
00:09:52.495 --> 00:09:56.595
Um, so let's do a 10,

167
00:09:58.115 --> 00:10:02.015
um, uh, 10 feature vector.

168
00:10:02.365 --> 00:10:05.335
Same thing with hidden layer. Let's do a 10 node.

169
00:10:06.905 --> 00:10:09.565
So as we know that there are dense connections, right?

170
00:10:11.545 --> 00:10:15.685
And there is a matrix you are trying to learn here for

171
00:10:16.195 --> 00:10:18.365
weights and for biases.

172
00:10:19.545 --> 00:10:23.245
So you can represent this in a regular, um,

173
00:10:24.865 --> 00:10:29.715
fashion as you know, um, HIJ equal

174
00:10:29.895 --> 00:10:33.405
to, sorry, HI

175
00:10:42.125 --> 00:10:46.625
equal to UI

176
00:10:47.375 --> 00:10:51.495
plus WIJ

177
00:10:52.865 --> 00:10:55.945
that's the weight for that particular connection

178
00:10:56.795 --> 00:11:01.405
into XI, uh, right, so that's your,

179
00:11:02.315 --> 00:11:05.735
um, equation, how you create the, uh,

180
00:11:05.875 --> 00:11:07.655
the hidden layer activations.

181
00:11:07.875 --> 00:11:09.575
And then you, you wrap this up, uh,

182
00:11:09.575 --> 00:11:10.775
in an activation function.

183
00:11:10.995 --> 00:11:12.495
So let's not complicate that part.

184
00:11:12.915 --> 00:11:15.945
So essentially it's, it's your wait times,

185
00:11:16.135 --> 00:11:18.505
your input plus bias, um,

186
00:11:18.925 --> 00:11:23.585
and there is a summation here, uh, across all the ice.

187
00:11:24.485 --> 00:11:28.305
Um, so, so you are summing, um,

188
00:11:29.455 --> 00:11:32.595
across all the, uh, input features, um,

189
00:11:32.775 --> 00:11:34.035
and then adding a bias.

190
00:11:34.535 --> 00:11:38.075
So that's how you come to this particular, um, activation,

191
00:11:40.305 --> 00:11:41.925
uh, following, uh, so far.

192
00:11:42.675 --> 00:11:47.045
Cool. So let's try to contrast that with, let's say,

193
00:11:47.135 --> 00:11:48.405
let's make this a 2D.

194
00:11:49.525 --> 00:11:54.205
So now instead of your input

195
00:11:54.225 --> 00:11:56.925
as one dimension, now I have a two dimension.

196
00:11:58.635 --> 00:12:02.455
So your X is, um,

197
00:12:03.635 --> 00:12:07.575
across INJ, um, dimensions.

198
00:12:08.835 --> 00:12:13.375
And then let's say we have a weight vector

199
00:12:15.395 --> 00:12:20.205
and a bias, and then your hidden

200
00:12:20.215 --> 00:12:22.165
layer is also a two dimension.

201
00:12:22.165 --> 00:12:24.005
Now, given we we're dealing with images,

202
00:12:24.185 --> 00:12:26.205
now we have input in two dimensions

203
00:12:26.225 --> 00:12:28.485
and a hidden layer and two dimensions.

204
00:12:28.585 --> 00:12:32.515
So you have then, right,

205
00:12:33.415 --> 00:12:38.075
so now this is a two dimensional, this is two dimensional,

206
00:12:40.095 --> 00:12:44.195
and this tensor becomes a fourth degree tensor,

207
00:12:44.625 --> 00:12:45.875
four dimensional tensor.

208
00:12:46.855 --> 00:12:50.235
Um, so sorry, uh, this is not the right representation.

209
00:12:50.375 --> 00:12:53.125
So, uh, so this is

210
00:12:56.635 --> 00:13:01.405
two, And then

211
00:13:01.825 --> 00:13:03.205
the tensor that you're trying

212
00:13:03.205 --> 00:13:07.365
to learn will be a fourth order or, uh, tensor.

213
00:13:07.905 --> 00:13:10.565
Um, so just trying to contrast to the previous one.

214
00:13:10.565 --> 00:13:12.805
So you have one dimensional inputs,

215
00:13:12.825 --> 00:13:14.125
one dimensional hidden layer,

216
00:13:14.345 --> 00:13:15.565
and the weights you're trying to learn

217
00:13:15.565 --> 00:13:16.645
are two dimensional, right?

218
00:13:17.305 --> 00:13:20.925
So just trying to, um, uh, extend that

219
00:13:20.985 --> 00:13:24.205
to a two dimensional input and two dimensional hidden layer.

220
00:13:24.465 --> 00:13:26.565
So now you're trying to learn a four D, fourth,

221
00:13:26.585 --> 00:13:29.965
fourth order, uh, uh, fourth dimensional tensor

222
00:13:30.625 --> 00:13:32.405
and the bias term that you're trying to learn.

223
00:13:32.405 --> 00:13:35.485
Instead of one dimensional here, you would have

224
00:13:35.565 --> 00:13:39.525
to learn a two dimensional, um, um,

225
00:13:39.825 --> 00:13:40.965
bio stem as well.

226
00:13:41.665 --> 00:13:45.665
So four dimensional

227
00:13:52.815 --> 00:13:56.945
anyways, okay, so let's try to put

228
00:13:56.945 --> 00:13:58.265
that in mathematical form.

229
00:14:01.085 --> 00:14:04.935
So you can write this as, uh,

230
00:14:04.935 --> 00:14:06.295
lemme select the right color.

231
00:14:07.055 --> 00:14:11.795
H IJ

232
00:14:11.975 --> 00:14:16.965
can be represented as your bias term,

233
00:14:19.035 --> 00:14:21.855
UIJ plus

234
00:14:26.505 --> 00:14:30.875
summation over K, summation over LW

235
00:14:31.575 --> 00:14:34.915
weight, uh, vector, IJKL

236
00:14:36.635 --> 00:14:41.465
or weight tenor times your input.

237
00:14:44.025 --> 00:14:46.165
So, um, I'm just putting the,

238
00:14:46.435 --> 00:14:48.725
what we have seen in the previous, uh,

239
00:14:49.455 --> 00:14:52.485
slide in a mathematical form, how that would look like.

240
00:14:53.225 --> 00:14:57.665
Um, so we can use some, um,

241
00:14:58.535 --> 00:15:02.255
mathematical tricks to, to represent it in a easier fashion.

242
00:15:02.555 --> 00:15:03.815
So let's do that.

243
00:15:04.075 --> 00:15:07.615
So I'm just converting the KL here

244
00:15:08.525 --> 00:15:12.415
into a i plus A and I plus B format.

245
00:15:12.995 --> 00:15:17.375
So, so essentially I want to represent K equal to I plus A

246
00:15:18.195 --> 00:15:20.935
and L equal to J plus B.

247
00:15:21.435 --> 00:15:24.135
So, so that makes the mathematical conversion easier.

248
00:15:24.595 --> 00:15:29.455
So going back, you can write this equation as UIJ

249
00:15:31.015 --> 00:15:35.615
plus SMA A summation

250
00:15:35.895 --> 00:15:40.815
B rate IJI plus A

251
00:15:41.975 --> 00:15:45.305
K plus B times

252
00:15:47.495 --> 00:15:51.275
XI plus a K plus B.

253
00:15:52.795 --> 00:15:57.015
So essentially we have this input that is

254
00:15:57.695 --> 00:16:02.655
a one, my CapEx image, and 0.6 hidden layer is 10.6,

255
00:16:02.915 --> 00:16:06.495
and we are trying to learn a weight that is 10 point 12.

256
00:16:06.955 --> 00:16:08.935
Uh, our goal is to reduce to a smaller number,

257
00:16:08.995 --> 00:16:12.055
and this is of the order of, again,

258
00:16:12.355 --> 00:16:14.885
ten four six, right?

259
00:16:15.505 --> 00:16:19.225
So our goal is to see if we can

260
00:16:20.115 --> 00:16:22.895
reduce these to a smaller number, uh,

261
00:16:23.035 --> 00:16:25.175
by using the principles of locality

262
00:16:25.195 --> 00:16:26.695
and translation in variants.

263
00:16:33.015 --> 00:16:36.395
Um, I'm seeing from questions, um,

264
00:16:37.505 --> 00:16:39.245
why weight is becoming four dimensional.

265
00:16:39.345 --> 00:16:40.645
So as you've seen with

266
00:16:41.975 --> 00:16:45.255
one dimensional vectors, right?

267
00:16:45.395 --> 00:16:50.135
So your weight mattresses is, um, uh, two dimensional,

268
00:16:50.435 --> 00:16:55.255
um, uh, when you, uh, use one dimensional input.

269
00:16:55.315 --> 00:16:59.145
So, so you can see that, you know, per, um,

270
00:17:01.065 --> 00:17:03.245
so let's, let me try to put this in, in a way.

271
00:17:03.305 --> 00:17:07.965
So, so here, your input,

272
00:17:08.095 --> 00:17:11.805
let's say is, is, um, X one, X two,

273
00:17:12.245 --> 00:17:13.445
X three, so on, right?

274
00:17:14.695 --> 00:17:18.515
And you're trying to get to a hidden layer with H one,

275
00:17:18.795 --> 00:17:21.035
H two, H three, so on.

276
00:17:22.015 --> 00:17:24.275
So in order to get there for a fully connect network,

277
00:17:24.815 --> 00:17:25.915
how many weights do you need?

278
00:17:26.295 --> 00:17:30.275
You need this many times, this many weights.

279
00:17:30.335 --> 00:17:32.795
We looked at it yesterday, like, how many parameters?

280
00:17:32.815 --> 00:17:34.075
So that's this matrix.

281
00:17:34.935 --> 00:17:38.715
So you need, um, what are the dimension here?

282
00:17:38.765 --> 00:17:42.355
Let's say it's 10, so 10 times 10

283
00:17:43.545 --> 00:17:47.485
weights to be able to get to this middle layer metrics.

284
00:17:47.825 --> 00:17:51.335
So if you think of that, that in two dimensional terms, uh,

285
00:17:51.335 --> 00:17:52.775
that becomes a four dimensional.

286
00:17:53.355 --> 00:17:57.455
Um, so, so if this explodes to a, a two dimensional

287
00:17:57.555 --> 00:17:58.935
and this is a two dimensional,

288
00:17:58.935 --> 00:18:01.335
then this will be a fourth degree cancer.

289
00:18:02.035 --> 00:18:05.215
Um, I hope that answers, uh, your question, Sarah.

290
00:18:08.085 --> 00:18:12.505
All right. Uh, define j and d. I didn't understand that.

291
00:18:13.205 --> 00:18:13.425
Um,

292
00:18:18.355 --> 00:18:19.605
okay, uh, let's keep moving.

293
00:18:19.945 --> 00:18:23.005
Um, hopefully things will get clearer as we move forward.

294
00:18:23.065 --> 00:18:27.285
So again, the goal is to, uh, reduce these weights

295
00:18:27.285 --> 00:18:31.005
and biases to a smaller dimension, uh, by using,

296
00:18:31.375 --> 00:18:33.245
exploiting the principles of locality

297
00:18:33.705 --> 00:18:35.325
and translation variance.

298
00:18:36.345 --> 00:18:38.805
So let's first apply translation variance.

299
00:18:39.345 --> 00:18:41.685
So with translation variance, what we are saying is

300
00:18:42.205 --> 00:18:46.285
irrespective of where, um, the,

301
00:18:47.145 --> 00:18:51.125
uh, input image is, um, like, you know,

302
00:18:51.125 --> 00:18:53.805
where in the input image is, it doesn't matter, like the,

303
00:18:53.805 --> 00:18:57.005
like, even if it travels across INJ.

304
00:18:58.145 --> 00:19:00.045
Um, so, uh,

305
00:19:00.785 --> 00:19:03.565
we shouldn't see any change in the hidden representation.

306
00:19:03.635 --> 00:19:06.965
Like, let's say if the CAT is in, uh, like I said,

307
00:19:07.065 --> 00:19:10.725
if the cat is in, uh, this location or this location.

308
00:19:10.905 --> 00:19:14.725
So even if it travels in the dimensions of INJ,

309
00:19:15.585 --> 00:19:16.805
the hidden representation

310
00:19:17.105 --> 00:19:19.605
of the CAT itself should not change.

311
00:19:21.325 --> 00:19:25.185
So by that, we can essentially take this equation

312
00:19:25.525 --> 00:19:29.075
and reduce it to let use a different color.

313
00:19:29.855 --> 00:19:32.915
You can reduce the IJ terms completely

314
00:19:33.215 --> 00:19:34.995
and remove the IJ terms completely.

315
00:19:35.735 --> 00:19:39.325
So that would become something like

316
00:19:41.975 --> 00:19:46.055
h IJ equal to your bias.

317
00:19:46.195 --> 00:19:48.335
You can completely remove the IJ terms

318
00:19:48.485 --> 00:19:51.015
because it is translation variant.

319
00:19:52.115 --> 00:19:52.335
And

320
00:19:57.355 --> 00:20:02.065
w you can

321
00:20:02.205 --> 00:20:04.665
you remove the IJ terms because again, we,

322
00:20:04.815 --> 00:20:08.305
because of invoking the translation variance functionality

323
00:20:09.205 --> 00:20:09.425
and

324
00:20:10.825 --> 00:20:15.585
XAJ plus B,

325
00:20:17.565 --> 00:20:21.985
so, so we are left with, so we went from, you know, uh,

326
00:20:22.145 --> 00:20:25.985
a two dimensional, um, bias, uh,

327
00:20:26.225 --> 00:20:30.425
a fourth degree tensor weight to, uh, uh,

328
00:20:30.645 --> 00:20:32.425
one dimensional, uh, bias

329
00:20:32.885 --> 00:20:35.185
and a second degree, uh, weight

330
00:20:35.565 --> 00:20:37.385
by invoking the translation variance.

331
00:20:37.385 --> 00:20:40.625
Because what we are saying is by translation variance,

332
00:20:40.625 --> 00:20:43.585
irrespective of IJ positions of the x,

333
00:20:44.765 --> 00:20:46.465
my hidden layer should not change.

334
00:20:47.615 --> 00:20:49.355
And that would not change if your biases

335
00:20:49.575 --> 00:20:52.515
and weights do not change if you travel across

336
00:20:52.875 --> 00:20:53.915
INJ dimensions.

337
00:20:54.955 --> 00:20:56.535
Um, so that's what we did here.

338
00:20:57.755 --> 00:20:59.255
So this is

339
00:21:00.325 --> 00:21:03.265
translational in variance.

340
00:21:05.025 --> 00:21:09.645
So now let's see what our, um, order became.

341
00:21:09.825 --> 00:21:13.605
So, so we are still with ten four six here,

342
00:21:14.835 --> 00:21:17.375
uh, ten four six, uh,

343
00:21:17.425 --> 00:21:20.655
given this is traversing across two dimensions only, A

344
00:21:20.655 --> 00:21:25.375
and B now, so it's also order of 10, 4 6.

345
00:21:26.765 --> 00:21:31.505
Um, yeah, so that's, uh, that's the,

346
00:21:31.925 --> 00:21:33.105
uh, dimensions.

347
00:21:33.365 --> 00:21:37.065
Uh, we are at, at in, in the order of ten six at the moment.

348
00:21:39.125 --> 00:21:41.625
So now let's invoke the principle of locality.

349
00:21:43.275 --> 00:21:46.975
So what we are saying here is we are still traveling across

350
00:21:47.175 --> 00:21:51.015
A and B, the entire, um, part of the image.

351
00:21:51.715 --> 00:21:52.855
So, uh,

352
00:21:53.115 --> 00:21:56.575
but my weights are translation variant, um,

353
00:21:56.675 --> 00:21:57.815
by principle of locality.

354
00:21:57.815 --> 00:22:00.695
What I'm say, what we are saying is, um,

355
00:22:02.185 --> 00:22:04.325
we don't need to traverse the entire image.

356
00:22:04.425 --> 00:22:07.685
We only traverse parts of the image at any point of time.

357
00:22:07.985 --> 00:22:12.115
So, so we only, instead of traversing from,

358
00:22:13.265 --> 00:22:17.615
um, for a, uh, let's see,

359
00:22:17.615 --> 00:22:21.015
since we have one me cell image, we have, um,

360
00:22:22.825 --> 00:22:25.045
zero oh oh.

361
00:22:29.465 --> 00:22:32.155
Alright, so, uh, sorry. Yeah.

362
00:22:33.045 --> 00:22:34.025
Uh, oh oh

363
00:22:36.225 --> 00:22:41.225
0, 0 0 0,

364
00:22:45.915 --> 00:22:46.915
actually this is zero thousand.

365
00:22:47.025 --> 00:22:50.795
Okay. So anyways, so those are the coordinates of your, uh,

366
00:22:50.795 --> 00:22:51.995
one magix image.

367
00:22:52.695 --> 00:22:57.235
Uh, we're still taking one shot at the entire image, uh,

368
00:22:57.525 --> 00:23:00.915
while, you know, uh, while computing this hidden layer.

369
00:23:01.695 --> 00:23:04.665
Instead, the principle locality says, I only have to look at

370
00:23:05.895 --> 00:23:08.135
a small portion of the image at any given point of time

371
00:23:08.475 --> 00:23:12.815
to be able to glean insights from that image, uh, to be able

372
00:23:12.815 --> 00:23:14.295
to learn parts of that image.

373
00:23:15.155 --> 00:23:19.935
So this small portion technically, uh, can be a 10

374
00:23:19.955 --> 00:23:23.215
by 10 area, uh, which is a decent size.

375
00:23:23.315 --> 00:23:24.415
As we look through, like, you know,

376
00:23:24.435 --> 00:23:28.655
how convolution filters work, uh, how this is explored, uh,

377
00:23:28.875 --> 00:23:31.055
we only have to look at a small part

378
00:23:31.055 --> 00:23:32.455
of the image at any given point

379
00:23:32.455 --> 00:23:34.015
of time instead of the entire image.

380
00:23:34.905 --> 00:23:38.605
So that brings this number ab to a,

381
00:23:39.645 --> 00:23:42.125
a small number instead of, you know, in the order

382
00:23:42.125 --> 00:23:44.805
of thousand, it will become,

383
00:23:46.755 --> 00:23:48.255
um, 10.

384
00:23:48.715 --> 00:23:51.575
Now. So, so instead of, uh, you know,

385
00:23:52.025 --> 00:23:55.255
traversing the entire image, uh, we're,

386
00:23:55.255 --> 00:23:58.175
we're just looking at a small portions

387
00:23:58.175 --> 00:23:59.855
of the image at any given point of time.

388
00:24:00.155 --> 00:24:03.655
So that brings this to an order of 10 power two.

389
00:24:04.835 --> 00:24:09.495
So we went from somewhere from 10 point 12 to 10.2

390
00:24:09.495 --> 00:24:13.235
for just one mega pixel image in one layer, uh,

391
00:24:13.295 --> 00:24:17.715
by invoking the two principles of translation in variance.

392
00:24:17.895 --> 00:24:21.915
And, uh, locality, uh, we'll see how that is, uh,

393
00:24:21.915 --> 00:24:23.915
that works in action in terms of, you know,

394
00:24:23.935 --> 00:24:28.235
how convolution layers work, um, how the kernels

395
00:24:28.255 --> 00:24:31.155
and convolution filters work, how parameter sharing works.

396
00:24:31.315 --> 00:24:33.435
I think there was a question that was asked yesterday,

397
00:24:33.435 --> 00:24:36.195
we'll address that, what parameter sharing means.

398
00:24:36.855 --> 00:24:40.475
Uh, but, um, essentially that's, uh, that's the math, uh,

399
00:24:40.735 --> 00:24:43.235
in terms of trying to understand how, if you start

400
00:24:43.235 --> 00:24:48.035
with an FNN kind of network for a image problem

401
00:24:49.075 --> 00:24:52.895
and then re reduce it based on the principles

402
00:24:52.915 --> 00:24:54.215
of translation variance

403
00:24:54.215 --> 00:24:59.175
and locality to a, uh, CNN kind of, uh, network, uh,

404
00:24:59.205 --> 00:25:00.855
what would be the advantage be?

405
00:25:01.795 --> 00:25:04.815
Uh, as I said, again, this is more for, you know, trying

406
00:25:04.815 --> 00:25:06.575
to understand the math behind it.

407
00:25:07.035 --> 00:25:08.495
Um, this is, um,

408
00:25:09.055 --> 00:25:11.095
I can pretty much guarantee this is not an interview

409
00:25:11.335 --> 00:25:13.175
question or you won't use this in practice.

410
00:25:13.795 --> 00:25:16.015
Uh, but happy to take any questions.

411
00:25:16.195 --> 00:25:19.335
Uh, we can also reserve some time at the end for questions,

412
00:25:19.635 --> 00:25:21.255
uh, if you want to go deeper into math.

413
00:25:24.215 --> 00:25:25.345
Alright, so,

414
00:25:28.665 --> 00:25:30.125
uh, we went through this.

415
00:25:30.505 --> 00:25:34.925
Um, so essentially I'll, I'll get this one.

416
00:25:36.195 --> 00:25:40.595
Um, so yeah, so let's get back to, you know, some

417
00:25:40.795 --> 00:25:42.635
of the theoretical aspects of C nnn.

418
00:25:44.255 --> 00:25:48.535
Um, so convolution layers, uh,

419
00:25:48.895 --> 00:25:51.455
CNN again, you know, it stands for convolutional network.

420
00:25:52.275 --> 00:25:55.255
Um, and it's, the structure of it is designed

421
00:25:55.595 --> 00:25:57.615
and we'll see what are the salient features that

422
00:25:58.205 --> 00:26:01.735
make them useful for images, uh, why they're efficient

423
00:26:01.735 --> 00:26:04.615
for images, we see, we saw it from a math perspective.

424
00:26:04.755 --> 00:26:07.655
Now we'll see from the layers perspective, like, you know,

425
00:26:07.955 --> 00:26:11.015
why, how these layers are applying these kind of principles.

426
00:26:11.715 --> 00:26:16.015
Uh, so the key layers in CNN are convolutional layers.

427
00:26:16.755 --> 00:26:18.495
Uh, we have pooling layers,

428
00:26:19.355 --> 00:26:22.095
and there are, uh, flattening layers

429
00:26:22.115 --> 00:26:23.295
and fully connected layers.

430
00:26:23.915 --> 00:26:27.135
Uh, we'll, we'll talk about this flatten

431
00:26:28.475 --> 00:26:30.635
layers too.

432
00:26:30.775 --> 00:26:34.675
So, um, and we'll talk about each of these, uh,

433
00:26:34.775 --> 00:26:36.235
in detail in the next session.

434
00:26:37.175 --> 00:26:41.815
Um, um, in the convolution layers, uh,

435
00:26:41.815 --> 00:26:44.965
there are usually, uh, uh, uh,

436
00:26:46.965 --> 00:26:50.325
kernels, or you can call them

437
00:26:50.755 --> 00:26:52.685
convolution filters.

438
00:26:54.335 --> 00:26:57.875
Uh, these words are inter, inter, uh, uh,

439
00:26:58.575 --> 00:27:01.715
can be used interchangeably, uh, kernels

440
00:27:01.875 --> 00:27:02.995
or conation filters.

441
00:27:03.575 --> 00:27:08.355
Um, so these conation filters extract the local features,

442
00:27:09.175 --> 00:27:12.955
um, like we discussed earlier, by exploiting the principle

443
00:27:12.955 --> 00:27:15.955
of locality, instead of taking the entire image at once,

444
00:27:15.955 --> 00:27:18.115
they just look at parts of the image

445
00:27:19.575 --> 00:27:23.115
and, um, by sliding over the input data.

446
00:27:24.015 --> 00:27:28.805
So, so if you look at, take this as an image.

447
00:27:29.745 --> 00:27:34.175
So convolution filters, think of them as small windows, um,

448
00:27:34.635 --> 00:27:39.095
and they slide over parts of the image,

449
00:27:39.995 --> 00:27:44.575
um, at once tried at a time to extract features,

450
00:27:45.475 --> 00:27:48.455
um, and which, and those features will be used, uh,

451
00:27:48.595 --> 00:27:50.575
in further layers of the network.

452
00:27:51.435 --> 00:27:55.575
Um, and the small window is exploiting the principle

453
00:27:55.575 --> 00:27:57.175
of locality, uh,

454
00:27:57.195 --> 00:27:59.735
and the principle of translation in variance.

455
00:28:00.355 --> 00:28:03.295
So we'll look at each of these layers, uh, in detail,

456
00:28:03.915 --> 00:28:06.815
but, uh, those are the key salient layers

457
00:28:06.995 --> 00:28:08.255
of a convolution network.

458
00:28:09.455 --> 00:28:13.925
Until now, we have seen why FNN doesn't work for, um,

459
00:28:14.585 --> 00:28:16.285
you know, it's not efficient.

460
00:28:16.465 --> 00:28:20.925
It, it might work, it's not efficient for a, uh, uh,

461
00:28:21.135 --> 00:28:22.285
image kind of problem.

462
00:28:22.785 --> 00:28:27.765
Um, how we can exploit, uh, from a math perspective, um,

463
00:28:28.585 --> 00:28:31.045
the principles of locality and translation variance.

464
00:28:31.145 --> 00:28:33.885
Uh, we also looked at what those principles are.

465
00:28:34.705 --> 00:28:37.965
Uh, now we are trying to look at like from a convolutional,

466
00:28:38.345 --> 00:28:41.365
uh, uh, layers perspective,

467
00:28:41.785 --> 00:28:45.445
why these layers are structured in the way they are, uh, so

468
00:28:45.445 --> 00:28:48.365
that, uh, we are taking advantage of those, uh, principles.

469
00:28:56.845 --> 00:28:58.945
Uh, I think this is, uh, I think

470
00:29:00.535 --> 00:29:03.865
this is the animation slide, so let's use,

471
00:29:12.515 --> 00:29:15.445
okay, I can use this slide to explain the

472
00:29:16.975 --> 00:29:18.385
feature sharing actually.

473
00:29:18.725 --> 00:29:22.145
Um, so, so here,

474
00:29:22.535 --> 00:29:23.985
this is actually not a CNN.

475
00:29:23.985 --> 00:29:28.745
This is a f and n. Um, so, uh, here, uh,

476
00:29:29.365 --> 00:29:31.185
we just, uh, took an image

477
00:29:31.325 --> 00:29:34.185
and, uh, this is, uh, I think from the same problem.

478
00:29:34.185 --> 00:29:37.905
Yesterday. We took 28 by 28 pixels from, um,

479
00:29:38.585 --> 00:29:41.545
a handwritten digit, and we flattened

480
00:29:41.545 --> 00:29:45.225
that into one dimensional vector as input layer,

481
00:29:45.605 --> 00:29:48.785
and we used an f and n essentially as two layers.

482
00:29:49.085 --> 00:29:53.905
And then finally, um, uh, created a output layer

483
00:29:53.935 --> 00:29:55.905
with 10 classes, right?

484
00:29:56.445 --> 00:29:58.665
So as we can see, it's densely connected.

485
00:29:59.965 --> 00:30:04.265
Um, here, each hidden unit is essentially

486
00:30:05.145 --> 00:30:07.945
learning from all the,

487
00:30:08.645 --> 00:30:10.305
uh, input features.

488
00:30:10.925 --> 00:30:12.785
So there is no, like, you know,

489
00:30:13.035 --> 00:30:16.025
every hidden unit is connected to every input feature.

490
00:30:16.405 --> 00:30:18.625
So there is no kind of, you know, parameter sharing.

491
00:30:18.625 --> 00:30:19.825
It's not like, you know,

492
00:30:19.825 --> 00:30:23.825
only these hidden units are learning from this part of the,

493
00:30:24.405 --> 00:30:25.465
uh, input layer.

494
00:30:25.925 --> 00:30:28.545
And these hidden units are learning from this

495
00:30:28.545 --> 00:30:29.585
part of the input layer.

496
00:30:29.965 --> 00:30:31.065
Um, so

497
00:30:32.635 --> 00:30:35.175
that's the part we'll exploit with convolutions.

498
00:30:35.675 --> 00:30:38.815
So just to draw that out.

499
00:30:38.915 --> 00:30:42.845
So if, so, for example,

500
00:30:42.905 --> 00:30:45.845
if there is some information in this part of the layer

501
00:30:46.945 --> 00:30:51.325
that's being learned kind of by every unit, there is not,

502
00:30:51.385 --> 00:30:53.605
not much of, you know, sharing efficiency here.

503
00:30:54.225 --> 00:30:58.765
Um, instead in CNNs, we'll try to learn that once

504
00:30:59.345 --> 00:31:03.285
and try to reuse, uh, across different, uh, layers.

505
00:31:05.005 --> 00:31:08.865
We'll see how that happens. Cool.

506
00:31:09.165 --> 00:31:13.615
So, um, again, I think this picture

507
00:31:14.155 --> 00:31:16.655
is trying to indicate like, you know, how the principles

508
00:31:16.655 --> 00:31:18.775
of locality, um, is used.

509
00:31:18.795 --> 00:31:21.375
And also in CNNs, there is an aspect

510
00:31:21.375 --> 00:31:23.495
of hierarchical hierarchical learning.

511
00:31:24.435 --> 00:31:28.345
So if you think of, um,

512
00:31:29.425 --> 00:31:32.505
a C NNN network, as, you know, multiple layers,

513
00:31:34.835 --> 00:31:39.085
input layer one, layer two, layer three.

514
00:31:40.345 --> 00:31:44.485
Um, so in the initial layers, the intuition is to

515
00:31:45.015 --> 00:31:48.125
learn the edges and textures, uh,

516
00:31:48.385 --> 00:31:51.685
and the further layers as we go through.

517
00:31:52.385 --> 00:31:55.245
Um, the network learns the object parts

518
00:31:56.345 --> 00:32:00.565
and further down, uh, the entire objects themselves.

519
00:32:01.265 --> 00:32:05.925
Um, so instead of, um, so they do this by using

520
00:32:06.685 --> 00:32:10.605
a concept called kernels or local receptive fields, um,

521
00:32:10.605 --> 00:32:13.365
because you're only looking at small parts,

522
00:32:13.985 --> 00:32:17.005
and then you're looking at bigger parts later on.

523
00:32:17.545 --> 00:32:19.645
Um, so the way I could show this is

524
00:32:21.805 --> 00:32:24.385
in layer one, your filters are

525
00:32:25.365 --> 00:32:27.175
looking at smaller edges.

526
00:32:27.795 --> 00:32:31.995
And in layer two, the filters are looking at

527
00:32:33.515 --> 00:32:35.535
all of the information extracted

528
00:32:35.555 --> 00:32:37.935
by smaller filters in a previous layer.

529
00:32:39.205 --> 00:32:42.785
And in layer three, the filters are looking at all

530
00:32:42.785 --> 00:32:45.785
of the information extracted by filters in the layer,

531
00:32:46.225 --> 00:32:47.585
previous layer, the layer two.

532
00:32:48.045 --> 00:32:50.985
So it's kind of building on top of, um,

533
00:32:51.405 --> 00:32:52.905
uh, one, one another.

534
00:32:53.765 --> 00:32:55.985
And, uh, essentially, uh,

535
00:32:56.085 --> 00:32:59.105
the receptive field gets bigger and bigger.

536
00:32:59.445 --> 00:33:01.105
So the, the area

537
00:33:01.295 --> 00:33:04.465
that this filter looks at is called receptive field.

538
00:33:05.685 --> 00:33:08.745
And that receptive field is, uh, gets bigger

539
00:33:08.765 --> 00:33:10.425
and bigger as we pass through the layers.

540
00:33:10.845 --> 00:33:13.225
Uh, that's how the hierarchical learning is achieved.

541
00:33:15.205 --> 00:33:18.105
Uh, and then finally, once all that learning happens,

542
00:33:18.605 --> 00:33:20.785
all this information is flattened

543
00:33:22.665 --> 00:33:25.245
and connected to a fully connected layer

544
00:33:25.255 --> 00:33:26.445
where reasoning happens.

545
00:33:26.625 --> 00:33:30.045
So, so I've learned all these features, uh, now I want

546
00:33:30.045 --> 00:33:32.085
to make some reasons, like whether it's a dog or cat

547
00:33:32.105 --> 00:33:36.605
or, you know, or, um, some other, uh, answer.

548
00:33:36.985 --> 00:33:41.045
Uh, so, so that's, um, uh, that's how, uh,

549
00:33:41.515 --> 00:33:45.605
CNNs work, uh, by starting from, uh,

550
00:33:45.605 --> 00:33:48.285
smaller receptive fields to, uh,

551
00:33:48.385 --> 00:33:50.405
moving towards a bigger receptor field.

552
00:33:51.785 --> 00:33:52.005
So

553
00:33:59.835 --> 00:34:02.635
quickly touch base on the concept of,

554
00:34:03.175 --> 00:34:05.315
um, parameter sharing.

555
00:34:06.175 --> 00:34:11.085
Um, so here this filter

556
00:34:12.995 --> 00:34:17.215
is denoted by a list of weights.

557
00:34:17.515 --> 00:34:19.255
Uh, we'll look at it, what filter means?

558
00:34:19.355 --> 00:34:22.855
Oh, so a filter is, let's say a three by three filter

559
00:34:24.175 --> 00:34:27.785
is denoted by, you know, nine weights.

560
00:34:29.045 --> 00:34:31.025
And these weights are learned

561
00:34:31.405 --> 00:34:32.825
as part of the training process.

562
00:34:34.085 --> 00:34:36.905
But what we are doing is we are using the same filter

563
00:34:37.925 --> 00:34:40.305
and moving across the image, uh,

564
00:34:40.305 --> 00:34:42.345
through a process called convolution.

565
00:34:43.335 --> 00:34:46.035
So that's how the parameters are being shared.

566
00:34:46.175 --> 00:34:48.955
The same weights are used for the entire image.

567
00:34:49.055 --> 00:34:51.315
You are not learning one weight for this part

568
00:34:51.335 --> 00:34:52.595
or one weight for this part,

569
00:34:53.015 --> 00:34:55.315
but you're using the exact same weights.

570
00:34:55.735 --> 00:34:57.755
That's how the parameters are shared,

571
00:34:58.575 --> 00:35:03.195
and, uh, that's why the CNN um, networks are efficient.

572
00:35:07.695 --> 00:35:10.275
And then, uh, let's see.

573
00:35:15.145 --> 00:35:17.735
So, so for CNNs,

574
00:35:19.115 --> 00:35:23.735
we looked at principle of locality, looked at translation

575
00:35:25.125 --> 00:35:29.405
in variance, and then

576
00:35:30.865 --> 00:35:35.485
we looked at the architecture that, uh,

577
00:35:35.625 --> 00:35:37.525
is influenced due to this, which is

578
00:35:38.305 --> 00:35:40.245
you have convolution layers,

579
00:35:44.775 --> 00:35:49.455
Putting layers and

580
00:35:49.625 --> 00:35:50.815
fully connected layers.

581
00:35:54.775 --> 00:35:59.055
So with convolution layers, we looked at, you know,

582
00:35:59.115 --> 00:36:00.655
how there are filters

583
00:36:01.035 --> 00:36:05.995
or kernels that traverses across the image

584
00:36:08.175 --> 00:36:10.075
and this process called convolutions.

585
00:36:10.895 --> 00:36:14.035
And these filters have weights

586
00:36:16.125 --> 00:36:19.175
that are learned during the training process

587
00:36:19.995 --> 00:36:22.465
and shared, um,

588
00:36:22.795 --> 00:36:25.145
these filters are shared for the entire image.

589
00:36:25.245 --> 00:36:27.345
So, so that's the parameter sharing part.

590
00:36:28.285 --> 00:36:32.625
Um, so that's one aspect of one

591
00:36:32.645 --> 00:36:34.465
or one layer of CNNs.

592
00:36:34.465 --> 00:36:38.915
And the pooling layer, um, are used to down sample.

593
00:36:39.695 --> 00:36:42.475
Uh, so once you learn a lot

594
00:36:42.475 --> 00:36:46.595
of information from different filters, um, again,

595
00:36:46.735 --> 00:36:48.235
to reduce the information

596
00:36:48.815 --> 00:36:51.235
and reduce the noise, um,

597
00:36:52.065 --> 00:36:53.835
CNNs use a concept called pooling layers.

598
00:36:54.335 --> 00:36:57.435
So here in these layers, the information is downs sampled,

599
00:36:57.985 --> 00:37:01.355
meaning these are also like filters,

600
00:37:02.565 --> 00:37:05.705
but instead of filtering them, what they do is

601
00:37:06.215 --> 00:37:10.185
when the convolute, they just take in that receptor field,

602
00:37:10.185 --> 00:37:11.545
they take a average

603
00:37:12.645 --> 00:37:17.345
or max of these, uh, of all the values in that area,

604
00:37:18.245 --> 00:37:22.625
and convert the nine, uh, three by three nine,

605
00:37:23.085 --> 00:37:24.345
um, uh,

606
00:37:24.765 --> 00:37:28.385
values into one value essentially boils down to one value.

607
00:37:29.005 --> 00:37:31.585
So that's what happens in the pooling layer that try

608
00:37:31.585 --> 00:37:33.425
to down sample the number of dimensions.

609
00:37:34.525 --> 00:37:37.665
Um, and we'll look at, you know, uh, actual, like, you know,

610
00:37:37.725 --> 00:37:39.585
how the pooling layers work.

611
00:37:40.285 --> 00:37:44.465
Um, and, um, so, so with the process

612
00:37:44.605 --> 00:37:47.705
of convolutions and pooling, um,

613
00:37:48.855 --> 00:37:52.995
and then, um, using a fully connected layer for reasoning,

614
00:37:53.415 --> 00:37:57.715
uh, C Ns learn, uh, features in a hierarchical way.

615
00:38:04.895 --> 00:38:09.745
Okay? Um, this is just a mathematical representation of,

616
00:38:10.125 --> 00:38:13.225
uh, convolutions in, uh, 1D

617
00:38:13.685 --> 00:38:16.385
and, uh, 2D uh, essentially.

618
00:38:16.885 --> 00:38:19.785
Um, so, uh, FNG are two mattresses,

619
00:38:19.885 --> 00:38:24.625
and if you are convoluting, um, uh, across these

620
00:38:25.345 --> 00:38:26.625
mattresses, this, this is the

621
00:38:26.625 --> 00:38:28.145
mathematical representation of it.

622
00:38:33.595 --> 00:38:35.565
Cool. Let's talk about filters.

623
00:38:35.825 --> 00:38:38.325
Um, what they're, uh, why they're important.

624
00:38:39.675 --> 00:38:42.645
So, uh, in the

625
00:38:44.055 --> 00:38:46.165
early stages of CNN, um,

626
00:38:46.825 --> 00:38:50.645
before AlexNet, maybe in the, uh, stage of linnet

627
00:38:51.105 --> 00:38:53.205
or even earlier, um,

628
00:38:54.915 --> 00:38:57.355
scientists created these kernels by hand.

629
00:38:57.935 --> 00:38:59.835
Uh, like, you know, they know like, Hey,

630
00:38:59.895 --> 00:39:03.995
by applying this filter on this image, I can, um,

631
00:39:05.065 --> 00:39:09.995
extract edges, or I can extract, uh, identities, I can,

632
00:39:10.055 --> 00:39:12.035
or I can sharpen the image.

633
00:39:12.175 --> 00:39:15.755
So, so, so these are some filters created that way.

634
00:39:16.455 --> 00:39:18.875
So, uh, these are all hand created filters.

635
00:39:18.975 --> 00:39:23.775
So, um, so this is one type of filter where, you know,

636
00:39:23.885 --> 00:39:27.455
your, uh, the filter is, you know, just one in the middle

637
00:39:27.995 --> 00:39:29.295
and everything else is zero.

638
00:39:29.355 --> 00:39:31.815
So, as you can think of this intuitively,

639
00:39:32.645 --> 00:39:36.725
when you apply this filter on an image, right?

640
00:39:36.945 --> 00:39:40.125
So, so this is a 10 by 10 image,

641
00:39:40.425 --> 00:39:45.155
and you're applying this filter of three by three

642
00:39:45.705 --> 00:39:48.555
with only one in the center, right?

643
00:39:49.215 --> 00:39:52.235
So when you apply this filter, this is giving, you know,

644
00:39:52.265 --> 00:39:53.795
highest way to the center

645
00:39:53.895 --> 00:39:56.875
of this filter area at any given point of time.

646
00:39:57.535 --> 00:40:00.995
And then, uh, making zero everything else.

647
00:40:01.095 --> 00:40:02.275
And then you keep on moving,

648
00:40:03.025 --> 00:40:04.875
keep on moving, and do the same thing.

649
00:40:04.895 --> 00:40:07.875
So it's giving way to only the center part

650
00:40:07.875 --> 00:40:09.755
of the filter at any given point of time.

651
00:40:10.055 --> 00:40:11.835
That's what this one means.

652
00:40:12.615 --> 00:40:13.955
Um, so, so that's,

653
00:40:13.955 --> 00:40:15.675
that's why it's called an identity filter.

654
00:40:15.785 --> 00:40:18.315
It's try to extract the, the core information

655
00:40:18.575 --> 00:40:20.835
or salient information from the image.

656
00:40:21.605 --> 00:40:24.635
Again, these are hand created filters in practice.

657
00:40:24.735 --> 00:40:27.995
In, in CNN's today, these filters are learned.

658
00:40:28.935 --> 00:40:31.035
We don't hand create these filters.

659
00:40:31.575 --> 00:40:34.115
Uh, these are learned, all these values are learned

660
00:40:34.175 --> 00:40:35.475
as part of the training process.

661
00:40:36.575 --> 00:40:39.275
Um, so for example, if you look at this edge detection,

662
00:40:39.375 --> 00:40:42.625
so here we are trying to give, uh, more way

663
00:40:42.645 --> 00:40:43.865
to these edges, right?

664
00:40:44.565 --> 00:40:48.745
So, and not so much way to, to these kind of edges.

665
00:40:49.325 --> 00:40:51.745
Um, similarly, like, you know, in this,

666
00:40:51.975 --> 00:40:53.745
this is a different kind of edge direction

667
00:40:53.745 --> 00:40:57.505
where you're giving more way to, uh, you know, these edges,

668
00:40:58.325 --> 00:41:01.945
uh, and, uh, not so much to these edges.

669
00:41:03.065 --> 00:41:04.485
So, uh, so again,

670
00:41:04.485 --> 00:41:06.885
these are all hand created filters in the earlier days

671
00:41:06.885 --> 00:41:10.685
of CNN where scientists use these filters, applied them

672
00:41:11.025 --> 00:41:14.765
to extract, uh, you know, uh, certain, uh,

673
00:41:15.115 --> 00:41:16.245
aspects of the image.

674
00:41:16.355 --> 00:41:18.005
Like, for example, if you apply this filter,

675
00:41:18.075 --> 00:41:19.165
this would be the output.

676
00:41:19.515 --> 00:41:21.205
This would be the output for this, this kind

677
00:41:21.205 --> 00:41:23.305
of filter, so on and so forth.

678
00:41:23.365 --> 00:41:26.625
For sharpening, you know, when they used these kind

679
00:41:26.625 --> 00:41:30.025
of fitters, um, or blurring, uh, or gush

680
00:41:30.025 --> 00:41:31.585
and blur, they use these kinders.

681
00:41:32.045 --> 00:41:34.505
So, uh, again, just to give you an intuition of, you know,

682
00:41:34.575 --> 00:41:39.225
what filters mean and, um, uh, why, uh,

683
00:41:39.645 --> 00:41:42.785
how, you know, you can also hand create filter filters

684
00:41:43.165 --> 00:41:44.305
to do certain kind of things.

685
00:41:46.095 --> 00:41:48.675
Uh, let me take quick pause there.

686
00:41:48.745 --> 00:41:52.755
What is the major difference between f and share?

687
00:41:53.175 --> 00:41:56.945
The, so, as we've seen, right, so,

688
00:41:57.365 --> 00:41:59.785
so far we've seen that f and S are fully connected,

689
00:42:00.985 --> 00:42:03.925
and, uh, CNNs don't operate that way.

690
00:42:04.315 --> 00:42:06.645
CNNs have different, uh, types

691
00:42:06.645 --> 00:42:08.125
of layers called convolutional layers

692
00:42:08.745 --> 00:42:13.125
and, uh, pooling layers that are unique to them, um,

693
00:42:13.275 --> 00:42:17.245
that make them take advantage of, of the principles

694
00:42:17.305 --> 00:42:21.605
of an image, um, uh, to, to, to make it more efficient.

695
00:42:22.425 --> 00:42:22.645
Um,

696
00:42:31.195 --> 00:42:32.975
so we looked at like, you know, some

697
00:42:32.975 --> 00:42:34.855
of the hand created kernels earlier.

698
00:42:34.995 --> 00:42:38.655
So, so, uh, just talk, let's talk a little bit more about,

699
00:42:38.755 --> 00:42:41.415
you know, uh, what are these, why do we need them?

700
00:42:42.075 --> 00:42:45.445
Um, so, um,

701
00:42:47.455 --> 00:42:50.445
again, uh, kernels are small mattresses essentially.

702
00:42:50.585 --> 00:42:53.965
Uh, typically, you know, um, most architectures use a three

703
00:42:53.965 --> 00:42:56.365
by three, five by five or seven by seven,

704
00:42:57.065 --> 00:42:58.365
uh, in my experience.

705
00:42:58.865 --> 00:43:03.285
Um, and these kernels slide across the input image, uh, to,

706
00:43:03.465 --> 00:43:06.205
and during the sliding operation, they're trying

707
00:43:06.205 --> 00:43:10.685
to focus at any given point of time on a small, uh, part

708
00:43:10.685 --> 00:43:12.285
of the image, just like your eyes, right?

709
00:43:12.285 --> 00:43:14.045
Your, your, when you look at things here,

710
00:43:14.045 --> 00:43:15.885
you focus on certain parts of the image,

711
00:43:16.585 --> 00:43:18.845
and you try to learn, and then, you know, you,

712
00:43:18.955 --> 00:43:21.285
then you take the bigger picture, uh,

713
00:43:21.385 --> 00:43:24.805
and assimilate that, uh, just like that CNN is trying to,

714
00:43:24.945 --> 00:43:26.645
you know, using this filtered mechanism.

715
00:43:26.915 --> 00:43:29.765
It's trying to look at small parts of the image and,

716
00:43:30.225 --> 00:43:32.805
and then, you know, learn from, from that image and move on.

717
00:43:34.235 --> 00:43:37.335
Um, and during this filter process, it's essentially,

718
00:43:37.515 --> 00:43:38.735
uh, we'll look at the math.

719
00:43:38.875 --> 00:43:42.455
So, uh, is a, uh, is,

720
00:43:42.475 --> 00:43:43.935
is a dot product and a sum.

721
00:43:44.115 --> 00:43:48.675
So, so, so that's an image,

722
00:43:48.815 --> 00:43:51.595
and this is a filter essentially for every

723
00:43:52.605 --> 00:43:57.075
Excel you're doing a, so let's say X is the, um,

724
00:43:58.205 --> 00:44:01.635
input image, so XIJ times

725
00:44:02.815 --> 00:44:04.835
and filter is the filter value.

726
00:44:05.055 --> 00:44:09.845
So you have FIJ, uh, so you submit, um,

727
00:44:09.945 --> 00:44:12.445
by doing a dot product, straight dot product.

728
00:44:23.955 --> 00:44:26.215
And, um, um,

729
00:44:26.355 --> 00:44:28.575
and we'll look at this formula in the next slide.

730
00:44:28.755 --> 00:44:31.685
So, um, so again, uh, just want

731
00:44:31.685 --> 00:44:34.005
to give you a bit more intuition on what filters mean.

732
00:44:34.385 --> 00:44:38.245
Um, uh, as, as we move, move through this, uh, understanding

733
00:44:38.265 --> 00:44:39.485
of how kernels work.

734
00:44:40.265 --> 00:44:43.125
Um, so we don't have to use one filter.

735
00:44:43.425 --> 00:44:44.685
We can use, you know,

736
00:44:44.965 --> 00:44:46.725
multiple filters in a convolution layer.

737
00:44:46.835 --> 00:44:51.445
Typically, a convolution layer is one con layer

738
00:44:52.805 --> 00:44:55.275
equal to a number of filters,

739
00:44:56.305 --> 00:44:58.485
and each filter can be unique.

740
00:44:58.635 --> 00:45:01.845
Like, one filter can be detecting edges, one filter can be,

741
00:45:02.185 --> 00:45:03.445
uh, detecting texture.

742
00:45:04.105 --> 00:45:06.605
One filter can be detecting, you know, uh,

743
00:45:07.205 --> 00:45:08.805
a different aspect of the image.

744
00:45:08.905 --> 00:45:11.085
All of these are essentially learned

745
00:45:11.305 --> 00:45:12.605
as part of the training process.

746
00:45:13.225 --> 00:45:16.805
So a convolution layer is, think of it as a set of filters,

747
00:45:17.805 --> 00:45:19.825
and these filters are applied to the images

748
00:45:20.685 --> 00:45:22.545
and new feature maps are formed.

749
00:45:22.685 --> 00:45:25.785
So once you apply these filters, you get some new features,

750
00:45:26.325 --> 00:45:29.225
and those features become the input to the next layer.

751
00:45:35.195 --> 00:45:37.175
So, bringing this all together.

752
00:45:37.595 --> 00:45:39.895
So going back, I think this answer,

753
00:45:40.255 --> 00:45:43.495
I think someone asked a question while I was talking about,

754
00:45:43.515 --> 00:45:45.015
you know, hidden layers and stuff.

755
00:45:45.475 --> 00:45:49.335
So, um, yeah, so convolution layers,

756
00:45:50.485 --> 00:45:53.655
pooling layers, fully connected layers, all

757
00:45:53.655 --> 00:45:54.895
of these are hidden layers, right?

758
00:45:55.675 --> 00:45:59.575
Um, uh, if you talk in the sense of f

759
00:45:59.575 --> 00:46:02.815
and n, uh, only the input layer and all layer or, or,

760
00:46:02.915 --> 00:46:04.735
or, uh, external layers.

761
00:46:05.355 --> 00:46:09.815
So, uh, so instead of one generic vanilla hidden layer,

762
00:46:09.965 --> 00:46:13.655
like in f and n where it's a fully connected layer here,

763
00:46:13.675 --> 00:46:18.175
we have different kinds of hidden layers, uh, con pooling,

764
00:46:19.165 --> 00:46:22.665
um, uh, and, uh, fully connected as well.

765
00:46:23.465 --> 00:46:27.525
So that's, uh, uh, that's the, uh, that's how you,

766
00:46:27.665 --> 00:46:28.805
you need to think about that.

767
00:46:29.705 --> 00:46:32.925
Um, so, um,

768
00:46:34.265 --> 00:46:36.325
we talked about a lot of these in the previous slide.

769
00:46:36.425 --> 00:46:40.325
So, so input layer, again, receives the raw input, um,

770
00:46:40.825 --> 00:46:42.685
you know, such as an image or sequence.

771
00:46:43.265 --> 00:46:46.645
Uh, and it's usually represented as a grid, right?

772
00:46:47.145 --> 00:46:48.885
Uh, so it's, it's a grid data.

773
00:46:49.225 --> 00:46:52.045
So that's, that's what CNNs are efficient at.

774
00:46:52.345 --> 00:46:56.045
So that's input layer and conation layer are,

775
00:46:56.335 --> 00:46:58.845
these are core building blocks of CNNs.

776
00:46:58.905 --> 00:47:02.525
Uh, this consists of multiple filters, like n filters,

777
00:47:03.265 --> 00:47:07.455
and each filter by doing an operation called convolution.

778
00:47:08.235 --> 00:47:12.495
Uh, and by looking at a small region of the input, uh,

779
00:47:12.525 --> 00:47:15.495
they perform element, element wise, multiplication,

780
00:47:16.315 --> 00:47:18.535
and summations to produce a feature map.

781
00:47:19.275 --> 00:47:21.515
So, feature map,

782
00:47:24.855 --> 00:47:27.755
um, so convolution layers, you know, by using

783
00:47:28.835 --> 00:47:33.005
multiple filters, and through a process called convolutions,

784
00:47:33.315 --> 00:47:34.645
they produce feature maps.

785
00:47:36.235 --> 00:47:38.215
And the depth of, you know,

786
00:47:38.635 --> 00:47:41.055
the output feature maps correspond

787
00:47:41.055 --> 00:47:42.335
to the number of filters used.

788
00:47:42.435 --> 00:47:47.015
So if you have 10 filters, um, so, uh,

789
00:47:47.115 --> 00:47:50.335
and each filter produces a one feature map.

790
00:47:50.675 --> 00:47:55.415
So now you have a, a 10 dimensional depth, uh, uh,

791
00:47:55.465 --> 00:47:59.935
sorry, 10, 10 depth of 10 for, you know, uh, uh,

792
00:48:00.155 --> 00:48:01.815
in terms of feature maps, uh,

793
00:48:01.995 --> 00:48:06.125
as an output from the convolution layer, uh,

794
00:48:06.125 --> 00:48:07.885
activation functions, we know about them.

795
00:48:07.885 --> 00:48:08.965
There is nothing new here.

796
00:48:09.185 --> 00:48:11.885
So in CNNs, typically, uh,

797
00:48:12.285 --> 00:48:14.445
relu is a very common activation function.

798
00:48:14.945 --> 00:48:18.445
Um, sigmoid and tan hs are also used,

799
00:48:18.505 --> 00:48:22.045
but relu is a very common activation function used in, uh,

800
00:48:23.545 --> 00:48:27.125
con, uh, CNNs pooling layers.

801
00:48:27.385 --> 00:48:29.125
Uh, like I said, pooling layers, uh,

802
00:48:29.175 --> 00:48:32.245
given convolution layers create lot of feature maps

803
00:48:32.265 --> 00:48:33.805
and increase the dimensionality.

804
00:48:34.435 --> 00:48:35.845
Pooling layers are used

805
00:48:35.845 --> 00:48:38.845
to reduce the spatial dimensions by downsampling.

806
00:48:39.585 --> 00:48:43.125
Uh, the most common pooling operation is Maxs.

807
00:48:43.665 --> 00:48:46.525
Uh, so you just take in a given receptor field,

808
00:48:46.525 --> 00:48:50.565
what is the max of this entire, um, uh, area.

809
00:48:51.385 --> 00:48:55.245
Uh, but average is also, uh, used in certain scenarios.

810
00:48:55.865 --> 00:48:58.365
Uh, the, the essence of pooling the intuition

811
00:48:58.365 --> 00:49:02.805
behind pooling is capturing the most salient features, uh,

812
00:49:02.945 --> 00:49:05.645
and reducing the com computational complexity.

813
00:49:08.375 --> 00:49:10.475
And then fully connected layers, uh,

814
00:49:10.985 --> 00:49:12.475
like these are the same kind

815
00:49:12.475 --> 00:49:14.075
of layers we've seen in f and m.

816
00:49:14.735 --> 00:49:17.355
Um, uh, these are also called dense layers.

817
00:49:18.055 --> 00:49:22.035
Uh, these are used, uh, at the end of CNN architecture

818
00:49:22.055 --> 00:49:25.035
to perform high level reasoning and classification.

819
00:49:25.575 --> 00:49:28.275
Um, these layers capture complex relationships

820
00:49:28.275 --> 00:49:31.315
between features, uh, learned by, you know,

821
00:49:31.545 --> 00:49:33.835
preceding convolutional and pooling layers,

822
00:49:35.035 --> 00:49:37.255
and output layer produces a final predictions.

823
00:49:37.835 --> 00:49:41.855
Um, the structure of the output layer, um, it depends on,

824
00:49:41.875 --> 00:49:44.695
you know, what task, what is the task at hand.

825
00:49:44.805 --> 00:49:46.935
Like, for example, in image classification,

826
00:49:46.935 --> 00:49:50.495
the output layer may consist of, um, soft max, like,

827
00:49:50.495 --> 00:49:52.895
you know, soft max activation across different classes.

828
00:49:53.795 --> 00:49:55.975
Uh, like you can a regression task,

829
00:49:55.975 --> 00:49:58.495
like let's say you're trying to output the bonding box

830
00:49:58.955 --> 00:50:01.895
of the cat, uh, then it's a regression task,

831
00:50:01.995 --> 00:50:04.855
so it'll output, uh, um, uh,

832
00:50:04.975 --> 00:50:06.615
a single neuron providing some sort

833
00:50:06.615 --> 00:50:10.175
of continuous prediction, like this is the, uh, pixel, uh,

834
00:50:10.655 --> 00:50:12.455
location for that bonding box.

835
00:50:18.835 --> 00:50:20.375
So bring this all together.

836
00:50:20.435 --> 00:50:24.055
So this is how A CNN network looks, right?

837
00:50:24.115 --> 00:50:26.575
So you have your input image,

838
00:50:27.265 --> 00:50:30.045
and in the, this is your convolution layer.

839
00:50:31.195 --> 00:50:34.695
Um, and in the convolution layer, as you can see, there are

840
00:50:35.215 --> 00:50:38.015
multiple feature maps, uh, sorry, multiple kernels.

841
00:50:38.715 --> 00:50:43.135
Uh, and each kernel is producing, uh, multiple feature maps.

842
00:50:43.795 --> 00:50:48.215
Um, so, and all these feature maps are go

843
00:50:48.215 --> 00:50:51.255
through a pooling layer where it is down sample

844
00:50:51.355 --> 00:50:52.415
to a smaller size.

845
00:50:53.325 --> 00:50:56.425
Um, you can do another iteration of convolution

846
00:50:56.425 --> 00:50:58.985
and pooling depending on, you know, the learning complexity.

847
00:50:59.525 --> 00:51:00.985
So there is another convolution layer

848
00:51:00.985 --> 00:51:02.705
that's creating more feature maps,

849
00:51:03.285 --> 00:51:05.665
and then downsampling using pooling layer.

850
00:51:06.205 --> 00:51:09.025
And finally, uh, we put all of this

851
00:51:09.575 --> 00:51:13.785
into a flattened layer in, in one dimension, uh,

852
00:51:13.785 --> 00:51:18.425
which would be used towards a fully connected layer, uh,

853
00:51:18.435 --> 00:51:22.625
where the final classification is happening, uh, of, of,

854
00:51:22.685 --> 00:51:24.905
you know, uh, of, for the particular task.

855
00:51:24.965 --> 00:51:26.785
In this case, it's a classification task

856
00:51:27.125 --> 00:51:28.345
for across three classes.

857
00:51:29.085 --> 00:51:30.825
Um, so that's how, um,

858
00:51:31.165 --> 00:51:33.585
and all of these are hidden layers, like someone asked,

859
00:51:33.585 --> 00:51:35.305
like, all of these are hidden layers.

860
00:51:35.885 --> 00:51:38.825
Um, if you think of this in the aspect of, you know, uh,

861
00:51:38.875 --> 00:51:40.625
input hidden and output layers.

862
00:51:46.035 --> 00:51:48.245
Alright, uh, let's see some questions.

863
00:51:48.465 --> 00:51:51.365
Can there be more than one such trial in the full network?

864
00:51:51.475 --> 00:51:52.125
What is trial?

865
00:52:01.145 --> 00:52:03.205
Uh, I, I didn't understand what trial means.

866
00:52:03.425 --> 00:52:05.925
Uh, so maybe you can help out there,

867
00:52:06.515 --> 00:52:07.565
type it in the chat window.

868
00:52:09.075 --> 00:52:12.515
Uh, okay, let's keep moving.

869
00:52:13.995 --> 00:52:15.965
Yeah, yeah. So, yeah. Yes, yes.

870
00:52:17.715 --> 00:52:22.675
Um, there would be, uh, more than,

871
00:52:22.855 --> 00:52:26.515
you know, um, uh, one combination

872
00:52:26.775 --> 00:52:29.555
or two combinations of these, uh, combination cooling.

873
00:52:29.605 --> 00:52:31.635
We'll see that more of that in how some

874
00:52:31.635 --> 00:52:35.995
of the more popular architectures like, um, inception net

875
00:52:36.015 --> 00:52:38.355
or google net, uh, use these layers.

876
00:52:38.975 --> 00:52:41.355
Uh, yes, there is, there are multiple, actually,

877
00:52:41.365 --> 00:52:44.675
there are seven times these combinations

878
00:52:44.895 --> 00:52:47.515
or more, uh, in most popular networks.

879
00:52:48.455 --> 00:52:51.085
Would you lose spatial relationship in the flattened layer?

880
00:52:51.345 --> 00:52:55.705
Um, so we already learned all the spatial

881
00:52:55.705 --> 00:52:58.265
relationships in the previous layers.

882
00:52:58.965 --> 00:53:03.105
Um, so now is the time to actually use that information

883
00:53:03.165 --> 00:53:04.665
to make decisions.

884
00:53:05.245 --> 00:53:08.505
So, so, yes, then no kind of, so,

885
00:53:08.645 --> 00:53:11.145
so those patient relationships, like, you know, if you take

886
00:53:11.885 --> 00:53:13.065
one feature map,

887
00:53:13.985 --> 00:53:16.925
so the way this flattening happens is it takes the entire

888
00:53:17.025 --> 00:53:21.725
row and makes it as, as, you know, one vector

889
00:53:22.185 --> 00:53:24.725
and then stacks on top of each other like this.

890
00:53:25.185 --> 00:53:27.525
Uh, sorry, my colors are not that great here.

891
00:53:27.945 --> 00:53:32.795
So, so it takes a, um, feature map

892
00:53:32.895 --> 00:53:34.675
and essentially takes this whole thing

893
00:53:35.255 --> 00:53:37.355
and transposes it like this,

894
00:53:37.895 --> 00:53:40.395
and this one transposes here, this one trans here.

895
00:53:40.895 --> 00:53:45.735
So there is some, uh, spatial relationships still

896
00:53:46.255 --> 00:53:48.295
maintained because, you know, uh, the,

897
00:53:48.295 --> 00:53:50.735
these features are still, you know, closer to each other,

898
00:53:51.835 --> 00:53:55.295
but we've already taken advantage of the learning part

899
00:53:55.295 --> 00:53:56.455
of those spatial features.

900
00:53:56.685 --> 00:53:58.815
Like, you know, these were close to each other,

901
00:53:59.035 --> 00:54:03.415
we already learned and gleaned the information and insights,

902
00:54:03.415 --> 00:54:06.815
and now it's time to use that information to make decisions,

903
00:54:07.355 --> 00:54:08.415
uh, if that makes sense.

904
00:54:15.815 --> 00:54:17.115
Uh, good questions. Uh,

905
00:54:17.735 --> 00:54:21.795
and I haven't sent, so, uh, let, feel free to

906
00:54:22.345 --> 00:54:24.435
type more if those haven't been answered.

907
00:54:24.695 --> 00:54:26.675
Uh, I'm trying to find my mouse.

908
00:54:28.905 --> 00:54:30.795
Oops, where is it?

909
00:54:33.595 --> 00:54:37.275
Okay, anyways, um, okay, cool.

910
00:54:39.215 --> 00:54:41.085
Yeah. Alright.

911
00:54:41.465 --> 00:54:45.175
Um, so,

912
00:54:48.995 --> 00:54:52.375
um, you might have, you might hear a term called channel,

913
00:54:53.195 --> 00:54:54.895
uh, in convolution networks.

914
00:54:55.075 --> 00:54:59.935
So, uh, channel refers to just one of the dimensions, um,

915
00:55:00.395 --> 00:55:01.815
uh, of the image.

916
00:55:03.175 --> 00:55:04.195
So in image,

917
00:55:04.195 --> 00:55:06.635
you have your typical heightened width dimension, right?

918
00:55:07.295 --> 00:55:09.595
And there is also a channel dimension.

919
00:55:10.295 --> 00:55:14.155
Uh, if in a simplistic terms in RGB, you can think

920
00:55:14.155 --> 00:55:16.155
of RGB S3 channels.

921
00:55:16.255 --> 00:55:20.315
So, uh, if you take a gray scale image, um, there is

922
00:55:20.905 --> 00:55:24.175
just one channel, uh, with, you know,

923
00:55:24.235 --> 00:55:25.855
you've seen the values, right?

924
00:55:25.855 --> 00:55:27.215
Zero to 2 55.

925
00:55:27.835 --> 00:55:31.105
So now when you make

926
00:55:31.105 --> 00:55:34.985
that A RGB image color image, so you have three channels.

927
00:55:35.805 --> 00:55:39.745
So you have the same metrics for,

928
00:55:41.105 --> 00:55:44.085
uh, you know, the green values and the red values

929
00:55:44.945 --> 00:55:47.005
and, uh, blue values as well.

930
00:55:47.865 --> 00:55:49.565
Um, so, so,

931
00:55:49.905 --> 00:55:53.645
but, uh, that's a very simplistic definition of channel.

932
00:55:54.745 --> 00:55:59.085
Um, so as you see how CNNs work,

933
00:56:00.305 --> 00:56:04.235
um, CNN's take input image,

934
00:56:04.365 --> 00:56:07.995
apply the kernels and create feature maps, right?

935
00:56:08.215 --> 00:56:10.395
The feature maps are also called channels.

936
00:56:11.015 --> 00:56:13.515
So, so if you apply 10 filters,

937
00:56:14.095 --> 00:56:16.795
you are creating 10 channels, essentially.

938
00:56:17.415 --> 00:56:19.795
Uh, so that's, um, that's the, that's

939
00:56:19.795 --> 00:56:22.035
how channel terminology is used in

940
00:56:22.105 --> 00:56:23.605
convolution, neur networks.

941
00:56:26.235 --> 00:56:28.295
Um, yeah. Now let's dive deep into the

942
00:56:28.295 --> 00:56:29.495
convolution operation.

943
00:56:30.155 --> 00:56:34.015
Um, so as you've seen, uh, earlier, uh,

944
00:56:34.015 --> 00:56:36.095
convolution operation, um,

945
00:56:36.165 --> 00:56:39.535
essentially involves applying a set of learnable filters,

946
00:56:40.195 --> 00:56:44.375
uh, to the input data, sliding them over, uh,

947
00:56:44.795 --> 00:56:46.695
the input, uh, sequentially

948
00:56:46.715 --> 00:56:49.135
and performing element wise, multiplication

949
00:56:49.595 --> 00:56:51.855
and suming to extract local features.

950
00:56:52.595 --> 00:56:56.855
Uh, so this is the core operation in the convolution layer,

951
00:56:57.315 --> 00:57:00.495
and a convolution layer is essentially a set of filters,

952
00:57:00.755 --> 00:57:02.775
and each filter is slided over the image.

953
00:57:04.115 --> 00:57:06.055
Uh, so here is a math, so,

954
00:57:07.775 --> 00:57:09.395
so if this is an input image,

955
00:57:15.415 --> 00:57:16.475
and this is the filter.

956
00:57:18.625 --> 00:57:21.125
So essentially when you slide this, uh,

957
00:57:21.125 --> 00:57:24.045
filter over the input image, uh,

958
00:57:24.145 --> 00:57:27.605
we are doing a element wise, multiplication and summation.

959
00:57:27.665 --> 00:57:32.065
So essentially, um, uh, in this particular case,

960
00:57:32.605 --> 00:57:36.505
we are doing seven into one plus two times zero,

961
00:57:37.335 --> 00:57:42.145
plus three times minus one plus four times one

962
00:57:43.215 --> 00:57:46.965
plus zero times five, plus

963
00:57:47.895 --> 00:57:49.525
three times minus one

964
00:57:53.355 --> 00:57:57.705
one plus times minus one.

965
00:57:58.285 --> 00:58:01.425
So let's see if we can get the answer of six here.

966
00:58:01.515 --> 00:58:05.145
Seven, um, minus three,

967
00:58:06.135 --> 00:58:09.845
four plus 4, 8, 8 minus

968
00:58:09.845 --> 00:58:12.385
3, 5, 5 plus

969
00:58:12.475 --> 00:58:16.025
3, 8, 8 minus two six.

970
00:58:16.215 --> 00:58:20.185
Yeah. So, so that's, um, that's essentially how, you know,

971
00:58:20.265 --> 00:58:24.065
a filter is applied to a part of, um, input image,

972
00:58:24.805 --> 00:58:29.065
and you come up with a particular, um, uh, feature

973
00:58:29.205 --> 00:58:30.305
for that part of the image.

974
00:58:30.925 --> 00:58:32.825
And the, the part

975
00:58:32.825 --> 00:58:34.385
of convolution is you do this

976
00:58:34.705 --> 00:58:36.465
sequentially multiple times, right?

977
00:58:36.525 --> 00:58:40.625
So in the next convolution, this filter slides

978
00:58:40.765 --> 00:58:44.835
to this portion of the image, uh, if the stride,

979
00:58:45.175 --> 00:58:46.475
stride factor is one.

980
00:58:46.655 --> 00:58:49.715
So you are, you are striding by one, one step at a time.

981
00:58:51.015 --> 00:58:52.715
Now this value is created,

982
00:58:53.455 --> 00:58:58.075
and in the next convolution, uh, it slides by one more,

983
00:58:58.455 --> 00:59:01.515
and, uh, this value is created and so on and forth.

984
00:59:01.815 --> 00:59:05.635
So this what this is created, this is the feature map.

985
00:59:07.565 --> 00:59:12.035
So this is one feature map created from one filter.

986
00:59:13.095 --> 00:59:14.595
Um, and as I said,

987
00:59:14.915 --> 00:59:18.515
a convolution layer is a combination of multiple filters.

988
00:59:18.975 --> 00:59:22.925
So there are multiple feature maps like this created, uh,

989
00:59:23.375 --> 00:59:26.685
based on, you know, um, different filters.

990
00:59:26.705 --> 00:59:28.085
So let's say you have four filters.

991
00:59:28.085 --> 00:59:29.485
There are four feature maps created.

992
00:59:30.585 --> 00:59:33.845
So here is a map, like, you know, uh, what is the dimension

993
00:59:33.845 --> 00:59:37.045
of the feature map created, uh, based on the size

994
00:59:37.045 --> 00:59:39.005
of the filter and the size of the input.

995
00:59:39.625 --> 00:59:44.485
So if you take, uh, your input, uh, size as you know, W one

996
00:59:44.505 --> 00:59:45.605
and H one width

997
00:59:45.785 --> 00:59:49.685
and height, also in this case, it's uh, five, five by five.

998
00:59:50.225 --> 00:59:55.045
So w one equal five, H one equal five.

999
00:59:55.865 --> 00:59:58.925
And if you filter size, in this case,

1000
00:59:58.925 --> 01:00:00.525
the filter size is three.

1001
01:00:01.335 --> 01:00:02.715
Uh, that's your f

1002
01:00:04.375 --> 01:00:07.715
and, uh, padding, uh, we'll talk about padding

1003
01:00:07.775 --> 01:00:10.075
and what that means and why we need to do that.

1004
01:00:10.345 --> 01:00:12.035
Here. We are not doing any padding.

1005
01:00:12.035 --> 01:00:14.115
We are doing a zero padding. So p is zero.

1006
01:00:15.965 --> 01:00:19.385
So if you apply, and stride size is step, size is one.

1007
01:00:19.925 --> 01:00:21.305
Um, so, uh,

1008
01:00:21.305 --> 01:00:24.105
because we are only moving one at a time, um,

1009
01:00:24.685 --> 01:00:28.105
so if you apply all that, um, you can come to the,

1010
01:00:28.715 --> 01:00:30.545
these dimensions of three by three.

1011
01:00:31.205 --> 01:00:33.125
Um, uh,

1012
01:00:34.185 --> 01:00:37.485
and, um, so that's, that's essentially, you know, uh,

1013
01:00:37.625 --> 01:00:40.005
how you can come to like, what is the output

1014
01:00:40.025 --> 01:00:42.125
of my feature map, what are the output dimensions

1015
01:00:42.125 --> 01:00:43.405
of the feature map through a formula?

1016
01:00:44.185 --> 01:00:47.445
Uh, uh, again, you don't need to use these formulas

1017
01:00:47.545 --> 01:00:49.685
and practical, this is just more for your understanding

1018
01:00:49.825 --> 01:00:54.805
how the convolution filter creates these element wise,

1019
01:00:55.125 --> 01:00:58.645
multiplication and summations to come up with feature maps.

1020
01:00:59.065 --> 01:01:01.445
And what the dimensions of these feature maps are.

1021
01:01:04.485 --> 01:01:05.845
A good question on the filter size.

1022
01:01:05.905 --> 01:01:07.965
So, uh, as we look through like, you know,

1023
01:01:07.965 --> 01:01:12.005
network architectures, uh, the filter sizes ease a, uh,

1024
01:01:12.455 --> 01:01:16.285
hyper parameter essentially, so different, um, depending on,

1025
01:01:16.465 --> 01:01:19.925
uh, so if you think of it as an intuition, uh,

1026
01:01:20.065 --> 01:01:23.525
and your image has like very fine elements,

1027
01:01:23.525 --> 01:01:26.605
like small elements, fine elements, you might want to think

1028
01:01:26.605 --> 01:01:27.845
of smaller filter sizes.

1029
01:01:27.845 --> 01:01:29.565
Like, you know, the smallest I've seen

1030
01:01:29.565 --> 01:01:30.605
is a two by two filter.

1031
01:01:31.465 --> 01:01:34.965
Um, uh, you can think of, you know, three by three, two

1032
01:01:34.965 --> 01:01:36.085
by two small filters.

1033
01:01:36.425 --> 01:01:40.205
But if you have like, you know, large features in image, um,

1034
01:01:40.425 --> 01:01:44.205
and you know, uh, less finer details, you can go with, uh,

1035
01:01:44.205 --> 01:01:45.925
bigger feature filter sizes as well.

1036
01:01:46.115 --> 01:01:48.885
Typically, in CNN's, we see filter sizes

1037
01:01:50.205 --> 01:01:54.825
of various sizes used, uh, in more popular architectures.

1038
01:01:55.325 --> 01:01:59.105
Uh, and, uh, also as you go deeper into the network,

1039
01:01:59.285 --> 01:02:01.185
you will see bigger filter sizes as well.

1040
01:02:03.235 --> 01:02:04.855
Uh, hopefully that answers your question.

1041
01:02:06.875 --> 01:02:11.775
Any questions on the tried, how the filter is applied

1042
01:02:12.835 --> 01:02:14.455
and how the future map is created?

1043
01:02:17.645 --> 01:02:21.915
Cool. Again, I want to to quickly tie it all together, like,

1044
01:02:22.335 --> 01:02:25.395
you know, we looked at a lot of different kinds of math.

1045
01:02:25.895 --> 01:02:28.875
So, so this is where with tone, right?

1046
01:02:28.975 --> 01:02:31.675
So we are looking at small parts of the image

1047
01:02:32.335 --> 01:02:34.315
by not looking at the entire image.

1048
01:02:34.545 --> 01:02:36.515
This is how the filter work,

1049
01:02:37.215 --> 01:02:41.035
and we've seen how those filters can create, uh, you know,

1050
01:02:41.125 --> 01:02:44.635
small feature maps, uh, by looking at portions of the image.

1051
01:02:48.165 --> 01:02:50.775
Yeah, we'll talk about padding altogether, uh,

1052
01:02:50.915 --> 01:02:53.535
in a separate slide, uh, in a moment.

1053
01:03:04.115 --> 01:03:07.365
Alright? Uh, I think this is an animation slide.

1054
01:03:07.505 --> 01:03:09.165
Let me try this.

1055
01:03:18.315 --> 01:03:23.165
Okay, so again, uh, shows here the filter

1056
01:03:23.705 --> 01:03:26.675
is, uh, three by three,

1057
01:03:27.525 --> 01:03:29.465
and the input size is five by five.

1058
01:03:30.295 --> 01:03:34.265
Uh, and we are using, um,

1059
01:03:35.635 --> 01:03:37.445
what is the stride size here?

1060
01:03:37.555 --> 01:03:40.605
Just curious, uh, if anybody recognized that,

1061
01:03:42.915 --> 01:03:44.095
how much is the stride?

1062
01:03:46.945 --> 01:03:51.345
Yeah, exactly. Uh, uh, so we're, instead

1063
01:03:51.345 --> 01:03:52.785
of moving one step at a time,

1064
01:03:52.815 --> 01:03:54.425
it's actually moving two steps at a time.

1065
01:03:54.885 --> 01:03:57.865
So that's why the output feature map is only two by two.

1066
01:03:58.575 --> 01:04:02.345
If you saw the previous slide, like, uh,

1067
01:04:02.415 --> 01:04:03.705
it's the same dimensions, right?

1068
01:04:03.705 --> 01:04:05.865
Five by five input, three by three filter.

1069
01:04:07.215 --> 01:04:11.195
But we ended up, uh, coming up with a future map of three

1070
01:04:11.195 --> 01:04:13.155
by three because stride size is one.

1071
01:04:13.815 --> 01:04:17.485
So the only ch thing that changed here is right size,

1072
01:04:17.625 --> 01:04:19.525
and that changed the size of the feature map.

1073
01:04:19.905 --> 01:04:21.425
Mm-Hmm, yep.

1074
01:04:23.585 --> 01:04:26.565
Uh, yeah, aj, yeah, right on.

1075
01:04:27.635 --> 01:04:30.335
So, uh, again, just to visualize how those, uh,

1076
01:04:30.455 --> 01:04:31.455
convolutions work

1077
01:04:39.705 --> 01:04:42.275
padding, um, there were a lot of questions around this.

1078
01:04:42.535 --> 01:04:45.195
So, first of all, why, why do we need padding, uh,

1079
01:04:45.265 --> 01:04:46.795
from an inclusion standpoint?

1080
01:04:47.495 --> 01:04:51.305
So let's use a whiteboard for this.

1081
01:04:51.445 --> 01:04:53.105
So, so we have an image, right?

1082
01:04:54.195 --> 01:04:56.615
Um, let's, for sake of simplicity,

1083
01:04:56.615 --> 01:04:57.935
let's do a three by three image.

1084
01:04:58.515 --> 01:05:00.895
Uh, and let's say we are doing

1085
01:05:01.175 --> 01:05:04.575
a two by two filter.

1086
01:05:06.725 --> 01:05:11.135
So, so we're, we're looking at this part of the image

1087
01:05:11.715 --> 01:05:16.325
coming up with some value for the feature map,

1088
01:05:17.145 --> 01:05:20.245
and then we shift by one stride.

1089
01:05:20.345 --> 01:05:23.045
We are looking at this part of the image now coming up

1090
01:05:23.045 --> 01:05:27.005
with this value, uh, and then we do this, we do this,

1091
01:05:27.465 --> 01:05:30.125
and, you know, we come up with all these values.

1092
01:05:31.125 --> 01:05:33.425
Uh, but what you're seeing here is

1093
01:05:34.255 --> 01:05:38.475
when you do these convolutions, you are inherently giving

1094
01:05:39.645 --> 01:05:41.975
less voltage to the hedges,

1095
01:05:43.125 --> 01:05:47.485
because every time a filter moves, so

1096
01:05:47.985 --> 01:05:52.875
for the first convolution, uh, let's try this again.

1097
01:05:53.415 --> 01:05:57.385
So for the first convolution, uh,

1098
01:05:57.385 --> 01:06:02.265
the receptive field area is this whole part.

1099
01:06:02.725 --> 01:06:06.585
So all these four blocks get, get to be seen by the filter

1100
01:06:07.645 --> 01:06:09.105
in the second convolution.

1101
01:06:14.665 --> 01:06:16.165
The receptive field area is this.

1102
01:06:17.235 --> 01:06:22.055
So you, as you can see, these two blocks got seen twice,

1103
01:06:23.275 --> 01:06:27.015
uh, versus this block was only seen once,

1104
01:06:27.075 --> 01:06:28.575
and this block was only seen once.

1105
01:06:29.395 --> 01:06:32.735
So there is an inherent disadvantage, uh, in the,

1106
01:06:32.735 --> 01:06:35.815
in the way the convolution and strides work for the edges.

1107
01:06:37.005 --> 01:06:39.545
So if you think your

1108
01:06:40.615 --> 01:06:44.065
problem needs has like very important information

1109
01:06:44.065 --> 01:06:47.635
around the edges, uh, it's important to pad it.

1110
01:06:48.095 --> 01:06:52.195
So by padding, what we do is we take this input image

1111
01:06:53.315 --> 01:06:57.125
of three by three, and we add

1112
01:06:58.545 --> 01:06:59.875
just dummy pixels.

1113
01:07:01.035 --> 01:07:05.725
So, uh, if you add one layer, it's called one padding.

1114
01:07:06.425 --> 01:07:08.485
In the previous case, it's zero padding.

1115
01:07:09.505 --> 01:07:13.935
If you add two layers, it's called two padding.

1116
01:07:14.385 --> 01:07:17.815
Again, how much weight you want to give to the edges, uh,

1117
01:07:18.155 --> 01:07:20.295
that's, uh, that's how much you want to pad.

1118
01:07:20.325 --> 01:07:23.575
Typically, I've seen like, you know, either zero padding

1119
01:07:23.575 --> 01:07:24.815
or one padding or two padding.

1120
01:07:24.815 --> 01:07:26.375
That's the most I've seen. Uh,

1121
01:07:26.375 --> 01:07:28.775
unless you know you have much bigger filter sizes,

1122
01:07:28.775 --> 01:07:30.295
you might want to pad a little bit more.

1123
01:07:30.355 --> 01:07:32.895
That's, uh, um, that's the intuition

1124
01:07:32.915 --> 01:07:36.415
behind why we do padding is to give more important stages.

1125
01:07:37.485 --> 01:07:38.655
Does it make sense? Hopefully

1126
01:07:38.655 --> 01:07:40.895
that answers some of the questions earlier.

1127
01:07:42.645 --> 01:07:42.865
Um,

1128
01:07:47.345 --> 01:07:51.545
padding doesn't impact the, well, it's the other way around.

1129
01:07:51.925 --> 01:07:54.305
If you have a bigger filter size, you might want

1130
01:07:54.305 --> 01:07:55.505
to do a bigger padding, right?

1131
01:07:55.855 --> 01:07:59.945
Because, you know, your area of, of, uh, dimensional

1132
01:07:59.985 --> 01:08:03.385
of the filter is bigger, then you might want to do a,

1133
01:08:03.705 --> 01:08:04.865
a larger padding.

1134
01:08:05.245 --> 01:08:06.945
Um, that makes sense.

1135
01:08:11.085 --> 01:08:15.685
So actually, yeah, so here, uh, again, uh,

1136
01:08:17.295 --> 01:08:20.225
just what we just discussed is padding is a technique

1137
01:08:20.525 --> 01:08:23.145
to add extra border pixels, um,

1138
01:08:23.765 --> 01:08:25.385
to preserve the spatial dimensions

1139
01:08:25.405 --> 01:08:27.945
and avoid information loss at the edges.

1140
01:08:28.645 --> 01:08:32.385
Uh, so that's why we do, uh, padding. Um,

1141
01:08:37.425 --> 01:08:41.365
and, um, again, I think,

1142
01:08:43.095 --> 01:08:46.185
yeah, uh, these are just the same formulas, uh, we use.

1143
01:08:46.285 --> 01:08:48.105
So, so if you are doing padding, uh,

1144
01:08:48.605 --> 01:08:50.145
and it's not zero padding,

1145
01:08:50.145 --> 01:08:52.785
you might wanna put the right numbers there to get

1146
01:08:52.785 --> 01:08:54.425
to the right dimensions.

1147
01:08:56.775 --> 01:08:59.035
Uh, we talked about that. So pooling.

1148
01:08:59.495 --> 01:09:03.235
So next important operation of, uh, CNNs is pooling,

1149
01:09:04.205 --> 01:09:08.385
as I said earlier, pooling is a down sampling operation.

1150
01:09:09.165 --> 01:09:13.785
Um, so, um, in the pooling what happens is, uh,

1151
01:09:13.925 --> 01:09:17.145
we take the feature maps created by the convolution filters.

1152
01:09:17.845 --> 01:09:22.225
So let's say we created a feature map of

1153
01:09:23.045 --> 01:09:27.435
three by three, and if we apply max pooling,

1154
01:09:28.375 --> 01:09:30.795
so it just takes the max of all these values.

1155
01:09:31.005 --> 01:09:32.635
Let's say one of the values is nine,

1156
01:09:32.635 --> 01:09:33.955
and everything is like, you know,

1157
01:09:34.015 --> 01:09:39.005
1, 2, 5, 6, 0 minus one,

1158
01:09:39.825 --> 01:09:41.205
uh, two, three.

1159
01:09:41.745 --> 01:09:45.125
Uh, so the max pooling results in, in the value of nine.

1160
01:09:45.585 --> 01:09:49.645
Uh, again, you do the pooling same way like you do, uh,

1161
01:09:49.785 --> 01:09:54.445
in the convolutions, like you take a pooling, uh, filter

1162
01:09:55.385 --> 01:10:00.165
and you pass it, convolute it, uh, in the same fashion,

1163
01:10:00.705 --> 01:10:02.525
uh, across the input.

1164
01:10:02.615 --> 01:10:05.205
Input. Here is the feature map, right?

1165
01:10:05.705 --> 01:10:07.605
So you take the input feature map,

1166
01:10:08.955 --> 01:10:13.855
and then, um, you do a pooling filter on top of it

1167
01:10:15.145 --> 01:10:16.805
and convoluted.

1168
01:10:17.185 --> 01:10:21.885
So from this feature map, you'll get like four values, uh,

1169
01:10:21.885 --> 01:10:26.805
based on what the max is in this, uh, for, uh, uh,

1170
01:10:27.205 --> 01:10:29.045
receptor field at any given point of time.

1171
01:10:29.705 --> 01:10:34.005
Um, so just to make that a little bit more clearer.

1172
01:10:34.665 --> 01:10:37.605
Um, so,

1173
01:10:41.665 --> 01:10:44.445
so let's say this is one of the feature maps created from,

1174
01:10:45.105 --> 01:10:47.445
uh, so input,

1175
01:10:50.005 --> 01:10:52.025
you applied your filter

1176
01:10:53.895 --> 01:10:55.235
and you got this feature map.

1177
01:10:56.085 --> 01:10:58.145
Now we are applying pooling,

1178
01:11:00.265 --> 01:11:05.045
and we got this, uh, feature map down sample feature map.

1179
01:11:05.145 --> 01:11:08.685
So the pool pooling operation is essentially you take, um,

1180
01:11:09.185 --> 01:11:12.805
uh, pooling filter sites, uh, in this case, uh,

1181
01:11:12.805 --> 01:11:15.645
let's say my pooling filter size is two by two,

1182
01:11:16.465 --> 01:11:20.885
and, uh, looks at these four Excels or four values

1183
01:11:21.505 --> 01:11:25.165
and uses the max from this value, uh,

1184
01:11:26.435 --> 01:11:27.495
and populates this.

1185
01:11:28.235 --> 01:11:32.015
And next it looks at these four values,

1186
01:11:32.615 --> 01:11:34.455
use the max and populate serve.

1187
01:11:35.035 --> 01:11:36.295
Uh, so, so that's,

1188
01:11:36.365 --> 01:11:38.575
it's a very similar congregation operation,

1189
01:11:38.635 --> 01:11:42.015
except here you're not doing the element wise,

1190
01:11:42.335 --> 01:11:43.455
multiplication and summation.

1191
01:11:43.455 --> 01:11:47.175
Rather, you're just taking max of, of all the four,

1192
01:11:47.435 --> 01:11:49.415
of all the parts of the receptive period,

1193
01:12:01.885 --> 01:12:02.585
uh, ish.

1194
01:12:02.745 --> 01:12:05.065
I didn't understand your question, but, uh, let's see.

1195
01:12:05.205 --> 01:12:09.495
Um, yeah,

1196
01:12:09.735 --> 01:12:11.415
f is the size of the filter here.

1197
01:12:11.635 --> 01:12:15.935
Um, uh, yeah, so since the filters are equally dimensional,

1198
01:12:16.075 --> 01:12:18.095
so if it's a two by two filter, applic, two,

1199
01:12:18.755 --> 01:12:20.015
uh, that makes sense.

1200
01:12:24.945 --> 01:12:27.125
Uh, again, the intuition, hand pooling,

1201
01:12:27.545 --> 01:12:29.245
do we always need to use pooling?

1202
01:12:30.265 --> 01:12:33.355
Uh, yes.

1203
01:12:33.855 --> 01:12:37.075
Uh, typically it's, uh,

1204
01:12:39.275 --> 01:12:41.335
uh, typically pooling layer is followed

1205
01:12:41.435 --> 01:12:42.495
by a convolution layer.

1206
01:12:43.675 --> 01:12:46.975
Um, again, the intuition is, uh, is

1207
01:12:46.975 --> 01:12:48.215
to reduce the noise, right?

1208
01:12:48.355 --> 01:12:52.855
So you are trying to generalize more

1209
01:12:52.915 --> 01:12:54.095
by removing some,

1210
01:12:54.155 --> 01:12:55.775
or losing some information

1211
01:12:55.775 --> 01:12:58.135
that makes the network learn the right parameters.

1212
01:12:58.635 --> 01:13:01.495
Uh, it's a kind of generalization technique

1213
01:13:01.495 --> 01:13:02.815
that the network is using.

1214
01:13:03.435 --> 01:13:04.735
Um, so yeah,

1215
01:13:14.745 --> 01:13:15.035
cool.

1216
01:13:15.245 --> 01:13:18.595
Again, um, for pooling layer, if you're trying to come up

1217
01:13:18.595 --> 01:13:21.155
with the output dimension, uh,

1218
01:13:21.185 --> 01:13:22.715
this is the formula I can use.

1219
01:13:22.975 --> 01:13:27.475
Um, um, so, uh, just like similar to the formula we seen

1220
01:13:27.815 --> 01:13:32.435
for the, uh, the convolution layer,

1221
01:13:32.935 --> 01:13:36.915
um, here for the pooling layer, it's, it's this formula.

1222
01:13:37.805 --> 01:13:39.185
Now here, there is no padding

1223
01:13:39.185 --> 01:13:42.705
because, you know, you don't pad between, uh, uh,

1224
01:13:42.735 --> 01:13:44.185
convolution and pooling layer.

1225
01:13:51.405 --> 01:13:56.265
So again, uh, uh, this shows, uh, uh, in a better way,

1226
01:13:56.265 --> 01:13:57.985
more better than what I've drawn.

1227
01:13:58.485 --> 01:14:00.545
Uh, so this is how the pooling works.

1228
01:14:00.615 --> 01:14:03.465
It's just taking the max of these, uh, receptor field,

1229
01:14:03.645 --> 01:14:05.265
in this case seven, uh,

1230
01:14:05.525 --> 01:14:09.225
and, uh, it's, uh, trying to, uh, populate

1231
01:14:09.225 --> 01:14:11.025
that number into the pool feature map.

1232
01:14:14.925 --> 01:14:18.435
So this would be three, uh, this would be nine,

1233
01:14:19.495 --> 01:14:21.555
and this would be three, right?

1234
01:14:21.695 --> 01:14:24.435
So, uh, that's, uh, that's how pooling works.

1235
01:14:24.975 --> 01:14:28.035
Uh, and average pooling is, it just takes average, uh,

1236
01:14:28.055 --> 01:14:30.835
of this, uh, entire, uh, set.

1237
01:14:31.535 --> 01:14:36.115
Uh, so, uh, this is, in this case it could be one, uh,

1238
01:14:36.625 --> 01:14:37.675
16, four.

1239
01:14:38.735 --> 01:14:42.205
Um, and two, um, so

1240
01:14:43.595 --> 01:14:46.365
just intuition wise, like, you know, if you think of, uh,

1241
01:14:46.545 --> 01:14:51.085
max pooling versus average pooling, um, average pooling, uh,

1242
01:14:51.625 --> 01:14:55.405
is if you want to try to smoothen out the features,

1243
01:14:55.405 --> 01:14:56.885
average pooling is used

1244
01:14:57.505 --> 01:14:59.485
and a max pooling, we are trying

1245
01:14:59.485 --> 01:15:03.045
to only extract the most salient feature, uh,

1246
01:15:03.105 --> 01:15:04.205
of that feature map.

1247
01:15:04.585 --> 01:15:06.565
Uh, then, you know, max pooling is used,

1248
01:15:14.735 --> 01:15:18.315
so we already extracted the information from the edges in

1249
01:15:18.315 --> 01:15:20.195
the convolution layers, uh,

1250
01:15:20.655 --> 01:15:22.955
by giving them proper weightage through padding.

1251
01:15:23.375 --> 01:15:27.115
So that's why, you know, when we do pooling, it's kind of t

1252
01:15:27.115 --> 01:15:28.355
to use padding again.

1253
01:15:28.935 --> 01:15:31.955
Uh, so it's not a, uh, a popular practice to you

1254
01:15:32.055 --> 01:15:33.115
to that, right?

1255
01:15:33.145 --> 01:15:35.075
Hope that answers your question ish.

1256
01:15:38.325 --> 01:15:39.735
Alright, let's keep moving.

1257
01:15:41.095 --> 01:15:43.955
So after you do all this feature maps, right,

1258
01:15:43.975 --> 01:15:47.975
so you got your input image, um,

1259
01:15:49.515 --> 01:15:52.455
and then you created bunch of feature maps

1260
01:15:54.745 --> 01:15:56.115
through convolutions.

1261
01:15:56.365 --> 01:15:57.915
Let's say I use three filters.

1262
01:15:57.975 --> 01:16:00.155
And, and then, uh,

1263
01:16:00.205 --> 01:16:02.835
those three filters resulted in three feature maps.

1264
01:16:03.455 --> 01:16:05.165
Um, and then

1265
01:16:06.175 --> 01:16:10.615
after pooling, I reduced those feature maps to smaller size

1266
01:16:12.315 --> 01:16:16.705
by down sampling, and now it's time to flatten this.

1267
01:16:17.205 --> 01:16:18.865
Uh, so you just take this

1268
01:16:18.965 --> 01:16:22.665
and stack on top of each other, uh, and flatten this.

1269
01:16:23.165 --> 01:16:26.785
So this information can be used for, um, uh,

1270
01:16:26.965 --> 01:16:28.385
for further stages where, you know,

1271
01:16:28.505 --> 01:16:30.145
a fully connected layer can be used.

1272
01:16:31.005 --> 01:16:34.745
Um, so it's just a reshaping operation, uh, where you take,

1273
01:16:34.805 --> 01:16:37.065
you know, these multidimensional feature maps

1274
01:16:37.125 --> 01:16:39.025
and, uh, create, uh,

1275
01:16:39.135 --> 01:16:41.305
translate into one di one dimensional vector.

1276
01:16:42.695 --> 01:16:45.475
And, uh, these, um, uh, this helps

1277
01:16:46.295 --> 01:16:51.275
to apply a fully connected layer, um, uh, so that, uh,

1278
01:16:51.495 --> 01:16:53.755
you know, you can make more reasoning in terms of

1279
01:16:53.755 --> 01:16:55.155
what the task is at hand.

1280
01:17:00.235 --> 01:17:05.215
Cool. So here

1281
01:17:05.235 --> 01:17:08.535
we are kind of branching out to, uh, uh, a

1282
01:17:09.165 --> 01:17:10.205
slightly different topic of,

1283
01:17:10.265 --> 01:17:13.485
or fitting, uh, uh, this is very important.

1284
01:17:13.905 --> 01:17:17.205
Um, uh, this can be interview questions.

1285
01:17:17.205 --> 01:17:18.845
These are very popular interview questions.

1286
01:17:19.345 --> 01:17:22.605
Uh, and you'll also use this in practice, so pay attention.

1287
01:17:23.105 --> 01:17:28.055
Um, I think I see intuition and max and average.

1288
01:17:28.235 --> 01:17:33.005
So, so, yeah. So if you're taking the max, right?

1289
01:17:33.105 --> 01:17:37.125
So then you are just boiling this whole thing to one

1290
01:17:37.955 --> 01:17:40.205
most important feature from that area.

1291
01:17:40.745 --> 01:17:44.125
So, so you're, if you're trying to extract the only,

1292
01:17:44.185 --> 01:17:47.405
the most important features, like, you know, uh,

1293
01:17:47.735 --> 01:17:50.205
let's say the darkest pixel, uh,

1294
01:17:50.705 --> 01:17:54.205
or the, the brightest, uh, area of the image

1295
01:17:54.425 --> 01:17:57.525
or things like that, then you are trying to, uh,

1296
01:17:57.945 --> 01:17:59.325
you use a max pooling.

1297
01:18:00.465 --> 01:18:03.845
Uh, however, with average pooling, you're trying to like

1298
01:18:04.965 --> 01:18:08.285
smoothen the entire area to one mean number.

1299
01:18:08.985 --> 01:18:12.565
Uh, like I don't care about the, the most darkest picture,

1300
01:18:12.665 --> 01:18:15.685
but I just want what the average of this area is in terms

1301
01:18:15.685 --> 01:18:17.005
of colorwise or things like that.

1302
01:18:17.665 --> 01:18:19.845
So, uh, so that's, that's the intuition

1303
01:18:19.845 --> 01:18:20.885
behind max and average.

1304
01:18:22.955 --> 01:18:26.045
Pooling can be overlapping, uh, number, I didn't understand

1305
01:18:26.045 --> 01:18:28.125
that, uh, question.

1306
01:18:33.905 --> 01:18:35.675
It's a convolution operation, right?

1307
01:18:35.735 --> 01:18:38.155
So, so it is inherently overlapping.

1308
01:18:38.895 --> 01:18:43.115
Um, like, uh, you know, when you do pooling, you are doing,

1309
01:18:43.115 --> 01:18:45.835
depending on the stride size, you have laps happening.

1310
01:18:47.235 --> 01:18:51.255
That answers your question. Okay?

1311
01:18:52.115 --> 01:18:55.975
So we looked at conation layers, filters, uh,

1312
01:18:55.975 --> 01:18:58.455
the pooling layers, uh, and flattening layers.

1313
01:18:58.875 --> 01:19:02.575
So before we go on to the, the entire architecture, uh,

1314
01:19:02.625 --> 01:19:04.815
there is slight diversion here, you know,

1315
01:19:04.815 --> 01:19:05.975
what are the different techniques

1316
01:19:06.205 --> 01:19:08.055
that we use to prevent all fitting?

1317
01:19:08.465 --> 01:19:10.175
These are not specific to CNN.

1318
01:19:10.305 --> 01:19:13.615
These are, these can be applied to any, uh, neural networks,

1319
01:19:14.115 --> 01:19:17.055
uh, sometimes even, you know, non neural networks as well.

1320
01:19:17.475 --> 01:19:19.575
Um, so, so these are some techniques.

1321
01:19:19.575 --> 01:19:20.655
So what is, or fitting

1322
01:19:20.875 --> 01:19:22.735
before we go there, uh, what do you think is,

1323
01:19:22.735 --> 01:19:27.455
or fitting, can any takers

1324
01:19:28.865 --> 01:19:29.435
type in the

1325
01:19:45.095 --> 01:19:48.935
Variance perform well in training Lee

1326
01:19:49.035 --> 01:19:51.215
and bad in evaluation?

1327
01:19:51.325 --> 01:19:56.075
Okay, good. Yeah.

1328
01:19:56.345 --> 01:19:59.715
Tall fitting is when your model fits really well on the

1329
01:19:59.835 --> 01:20:03.155
training data, but when you apply that to validation

1330
01:20:03.255 --> 01:20:08.045
or even in test, uh, it is, uh, not doing well.

1331
01:20:08.345 --> 01:20:13.125
So we try to learn noise more than actual, uh, pattern.

1332
01:20:13.975 --> 01:20:15.875
Uh, so that's what, or fitting is.

1333
01:20:15.965 --> 01:20:19.355
Again, high variance is also a good term.

1334
01:20:20.335 --> 01:20:21.515
Uh, it need not be low bias,

1335
01:20:21.535 --> 01:20:22.915
but it's definitely high variance.

1336
01:20:23.455 --> 01:20:28.155
Um, so, so, uh, so yeah, uh, how do we prevent that?

1337
01:20:28.245 --> 01:20:32.795
There are some techniques that are popular in the, uh,

1338
01:20:32.855 --> 01:20:36.875
neural network world, uh, that can help prevent or fitting.

1339
01:20:37.055 --> 01:20:39.315
The first one is dropout, um,

1340
01:20:39.535 --> 01:20:42.635
and al l to regularization, uh,

1341
01:20:42.885 --> 01:20:44.755
batch normalization, early stopping.

1342
01:20:44.755 --> 01:20:48.035
These are some techniques, so we'll look at each

1343
01:20:48.035 --> 01:20:49.195
of those in detail.

1344
01:20:50.055 --> 01:20:54.335
Um, so just to

1345
01:20:56.015 --> 01:20:58.875
be clear, so if you have your epoch

1346
01:21:00.305 --> 01:21:03.795
and last, um,

1347
01:21:04.775 --> 01:21:09.425
and training loss, this is for your training

1348
01:21:11.885 --> 01:21:15.095
and sort validation.

1349
01:21:15.315 --> 01:21:19.215
So, um, so this means like, at this point, you know,

1350
01:21:19.275 --> 01:21:22.905
the network kind of, um, so

1351
01:21:23.575 --> 01:21:26.145
this is the over fitting area.

1352
01:21:27.425 --> 01:21:31.055
Uh, this is probably under fitting area.

1353
01:21:32.115 --> 01:21:35.965
So, so, um, that's what, or fitting looks like.

1354
01:21:35.965 --> 01:21:39.885
You know, when you look at loss, uh, uh, uh, in terms of

1355
01:21:39.885 --> 01:21:42.765
between validation and training data sets, uh,

1356
01:21:42.765 --> 01:21:44.645
and these are some techniques, dropout, L one

1357
01:21:44.645 --> 01:21:47.725
and two, regularization, batch norm, uh,

1358
01:21:47.865 --> 01:21:49.845
at least dropping out some techniques that are used

1359
01:21:49.845 --> 01:21:52.005
to prevent this, uh, in practice.

1360
01:21:53.275 --> 01:21:58.055
So, dropout. So with dropout, um, the biggest,

1361
01:21:58.395 --> 01:22:02.255
um, uh, intuition here is you kind of, we kind

1362
01:22:02.255 --> 01:22:06.295
of turn off some parts of the network, uh, randomly.

1363
01:22:06.875 --> 01:22:10.815
Um, so let's say in epoch one,

1364
01:22:12.035 --> 01:22:15.095
um, batch one, uh,

1365
01:22:15.105 --> 01:22:17.415
we're turning off this neuron and this neuron.

1366
01:22:17.415 --> 01:22:20.095
So this neuron is not used at all, uh,

1367
01:22:20.285 --> 01:22:21.735
when in all the computations

1368
01:22:22.475 --> 01:22:24.895
and in batch two PAC one,

1369
01:22:25.155 --> 01:22:27.175
we are turning off this neuron and this neuron.

1370
01:22:27.435 --> 01:22:30.535
Uh, so randomly you turn off certain parts of network,

1371
01:22:30.805 --> 01:22:33.135
certain neurons, um,

1372
01:22:33.515 --> 01:22:38.095
and, uh, this is designated by a factor dropout factor.

1373
01:22:38.385 --> 01:22:43.095
Let's say I want to use a dropout factor of 0.3, uh,

1374
01:22:43.105 --> 01:22:46.535
which means I'm turning off 30% of neurons, uh,

1375
01:22:47.055 --> 01:22:48.655
randomly at any given point of time.

1376
01:22:49.315 --> 01:22:52.935
So, so, uh, uh, we randomly

1377
01:22:52.955 --> 01:22:55.335
by randomly turning off the neurons, we are forcing

1378
01:22:55.875 --> 01:23:00.695
the non turned off neurons, uh, the, uh, uh, the disease

1379
01:23:01.155 --> 01:23:03.135
to learn a little bit more about

1380
01:23:03.165 --> 01:23:05.015
what these other neurons are doing.

1381
01:23:06.105 --> 01:23:08.565
So intuitively, this is like the new,

1382
01:23:08.565 --> 01:23:10.885
making the neurons good at their specialization,

1383
01:23:11.065 --> 01:23:14.245
but also learn a little bit more about the nearby neurons.

1384
01:23:14.585 --> 01:23:16.605
So, dropout, uh, is, uh,

1385
01:23:16.945 --> 01:23:19.965
is a powerful regularization technique that helps improve

1386
01:23:20.545 --> 01:23:23.325
the generalization ability by, um,

1387
01:23:24.045 --> 01:23:27.205
actually forcing the neurons, uh, to, uh,

1388
01:23:27.385 --> 01:23:29.725
not just overfit on one particular aspect,

1389
01:23:29.745 --> 01:23:32.045
but also learn about the other aspects as well.

1390
01:23:33.655 --> 01:23:36.035
Um, so that's how property is used.

1391
01:23:36.415 --> 01:23:39.915
Uh, in, um, in practice, you just, uh,

1392
01:23:40.305 --> 01:23:44.875
give a dropout factor, uh, uh, whether it's 0.5 or 0.3

1393
01:23:44.875 --> 01:23:46.435
or 0.4, and,

1394
01:23:46.855 --> 01:23:49.075
and that is applied to, uh,

1395
01:23:49.135 --> 01:23:52.835
the network in automated fashion across batches and,

1396
01:23:54.055 --> 01:23:55.715
and the network learns better.

1397
01:23:59.445 --> 01:24:03.705
Uh, the next one is, um, L one and L two regularization.

1398
01:24:05.375 --> 01:24:09.755
Um, so, uh, L one is also called lasso.

1399
01:24:11.205 --> 01:24:12.685
L two is called ridge.

1400
01:24:13.885 --> 01:24:16.345
Uh, so again, this, this can come up

1401
01:24:16.345 --> 01:24:17.545
as a good interview question.

1402
01:24:18.005 --> 01:24:20.665
So with regularization, what we do is we,

1403
01:24:20.765 --> 01:24:24.025
we already know a generic loss function, right?

1404
01:24:24.165 --> 01:24:27.305
So, so in the case of, um, uh,

1405
01:24:27.355 --> 01:24:30.025
let's say A MSC loss, uh,

1406
01:24:30.125 --> 01:24:33.365
we know this is the loss function, right?

1407
01:24:34.065 --> 01:24:38.445
So with regularization applied, we use a, uh,

1408
01:24:39.105 --> 01:24:44.045
uh, an additional factor to the regression, uh, with a,

1409
01:24:44.145 --> 01:24:48.195
um, regularization factor called Lambda.

1410
01:24:49.095 --> 01:24:53.555
Um, and this is the additional factor we use for, um,

1411
01:24:54.255 --> 01:24:56.515
uh, to regularize the network.

1412
01:24:56.695 --> 01:25:00.395
So, so when certain weights become too big, um,

1413
01:25:01.945 --> 01:25:05.935
so let's use a loss of

1414
01:25:08.555 --> 01:25:13.295
ridge L one or two.

1415
01:25:16.595 --> 01:25:21.515
So loss equal to, you know,

1416
01:25:23.005 --> 01:25:27.605
uh, M-S-C-M-S-C loss plus

1417
01:25:27.905 --> 01:25:30.205
regularization factor times.

1418
01:25:31.065 --> 01:25:35.525
Um, we use a mod of all the weights, W one plus,

1419
01:25:35.765 --> 01:25:38.285
W2, plus W three, and so on and so forth.

1420
01:25:39.105 --> 01:25:43.565
So, so by providing that, uh, the network

1421
01:25:44.855 --> 01:25:46.835
during the loss, computation

1422
01:25:47.055 --> 01:25:50.555
and loss optimization, uh, if, if one

1423
01:25:50.555 --> 01:25:53.515
of these weights is extremely big, um,

1424
01:25:53.815 --> 01:25:57.315
by using the Lambda, um, uh,

1425
01:25:57.575 --> 01:25:59.955
and the regularization factor, it tries to

1426
01:26:00.855 --> 01:26:04.245
reduce these weights to a very small number, a zero.

1427
01:26:04.665 --> 01:26:07.925
Uh, so if a rate is like, you know, too big, it tries

1428
01:26:07.925 --> 01:26:08.965
to reduce that to zero.

1429
01:26:09.825 --> 01:26:13.415
Um, and for ridge, uh,

1430
01:26:14.155 --> 01:26:16.495
the first part is exactly the same.

1431
01:26:17.525 --> 01:26:19.425
Uh, it would be

1432
01:26:19.985 --> 01:26:24.585
W one square plus W2 square plus, so on so forth.

1433
01:26:25.285 --> 01:26:29.665
So here, instead of making a bigger weight zero, uh,

1434
01:26:29.765 --> 01:26:33.985
during the loss optimization, the network tries to, um,

1435
01:26:34.135 --> 01:26:35.345
make it smaller.

1436
01:26:36.415 --> 01:26:41.275
Uh, so you can think of this continually by, you know,

1437
01:26:41.375 --> 01:26:45.785
if you, uh, dow by,

1438
01:26:47.065 --> 01:26:51.485
uh, w so in this case, uh, uh, the

1439
01:26:52.105 --> 01:26:56.345
weight can times one, right?

1440
01:26:56.405 --> 01:26:59.785
So Dow W one by Dow W one,

1441
01:26:59.965 --> 01:27:01.585
so it can 10 to one.

1442
01:27:02.485 --> 01:27:04.545
Uh, so it can essentially,

1443
01:27:04.545 --> 01:27:06.865
you can reduce the weight to a zero.

1444
01:27:07.405 --> 01:27:11.905
Um, uh, if, uh, uh, if, if it's too empowering

1445
01:27:11.905 --> 01:27:15.465
or too overpowering on a network here, it'll only boil down

1446
01:27:15.465 --> 01:27:16.865
to two.

1447
01:27:17.525 --> 01:27:21.585
So the L by W will boil down to

1448
01:27:22.525 --> 01:27:24.905
two times lambda times.

1449
01:27:25.445 --> 01:27:28.465
In this case, let's say if we're W one, W one.

1450
01:27:28.925 --> 01:27:31.065
So in that particular case,

1451
01:27:31.485 --> 01:27:33.585
you can't really make the weight down to zero,

1452
01:27:33.605 --> 01:27:35.865
but you can make it a much small number

1453
01:27:35.865 --> 01:27:37.545
during the optimization phase.

1454
01:27:38.125 --> 01:27:40.545
Um, so, so this is how, you know,

1455
01:27:40.665 --> 01:27:42.465
a regularization factor is added

1456
01:27:42.565 --> 01:27:47.225
to a existing loss function, so that during optimization,

1457
01:27:48.725 --> 01:27:52.465
um, you can essentially make those weights come down

1458
01:27:52.465 --> 01:27:55.505
to a small number, uh, in rich,

1459
01:27:55.965 --> 01:28:00.665
or make it a zero in, uh, uh, lasso or L one.

1460
01:28:01.005 --> 01:28:05.065
Uh, so, so that way a certain weight cannot become too big,

1461
01:28:05.725 --> 01:28:08.865
uh, uh, uh, too overpowering on other weights.

1462
01:28:17.435 --> 01:28:21.615
And, uh, just so that recap, so how this all plays

1463
01:28:22.285 --> 01:28:26.775
into the overall weight update is, you know, you have your

1464
01:28:27.765 --> 01:28:32.225
weight new, called to weight, old minus,

1465
01:28:33.365 --> 01:28:37.215
you know, learning rate times, um,

1466
01:28:38.435 --> 01:28:40.775
double by OW, right?

1467
01:28:41.155 --> 01:28:43.095
And that is this one, essentially.

1468
01:28:43.555 --> 01:28:46.175
So, uh, and that's how kind of

1469
01:28:46.835 --> 01:28:50.015
can control a larger weight, uh, into the weight updates.

1470
01:28:55.955 --> 01:28:58.895
So the next, uh, method, uh, to

1471
01:28:59.795 --> 01:29:02.285
reduce our fitting is called, called batch normalization.

1472
01:29:03.025 --> 01:29:04.965
Uh, you all have experienced

1473
01:29:06.005 --> 01:29:09.015
some normalization methods, right?

1474
01:29:09.045 --> 01:29:10.935
Like standardization, uh,

1475
01:29:11.835 --> 01:29:16.835
can someone, can someone share?

1476
01:29:16.935 --> 01:29:18.215
They'll answer your question in a bit,

1477
01:29:18.235 --> 01:29:22.255
but, um, can someone tell me like what normalization means?

1478
01:29:42.145 --> 01:29:45.875
Cool. Uh, uh, so normalization is essentially you, uh,

1479
01:29:46.295 --> 01:29:47.715
to normalize a variable.

1480
01:29:47.975 --> 01:29:52.235
So you do the, some sort of, uh, you take the mean, uh,

1481
01:29:52.255 --> 01:29:54.915
and you, uh, use a standard deviation.

1482
01:29:55.095 --> 01:29:58.515
So, uh, there is an x, uh, variable.

1483
01:29:58.575 --> 01:30:00.955
So X minus new by standard deviation.

1484
01:30:00.955 --> 01:30:03.315
That's how you normalize a variable, right?

1485
01:30:03.815 --> 01:30:05.955
Uh, so that's a standard normalization.

1486
01:30:06.015 --> 01:30:07.995
You want to take a regular distribution

1487
01:30:07.995 --> 01:30:09.315
and make it a normal distribution.

1488
01:30:09.855 --> 01:30:13.675
Uh, you do this across all the values to,

1489
01:30:14.055 --> 01:30:15.515
to make it a normal distribution.

1490
01:30:15.935 --> 01:30:20.185
Um, so, so usually you do that in the,

1491
01:30:20.885 --> 01:30:25.705
uh, initial input layer, uh, to make the network features,

1492
01:30:26.285 --> 01:30:29.145
uh, in the same range so that the network is,

1493
01:30:29.325 --> 01:30:32.785
is not struggling to, you know, fit differently

1494
01:30:32.965 --> 01:30:34.305
to different features.

1495
01:30:35.055 --> 01:30:39.275
Uh, but what happens as you, um, move through the network,

1496
01:30:39.535 --> 01:30:41.555
so, so you have your input layer,

1497
01:30:42.355 --> 01:30:46.505
and then you have your hidden layers, uh, H one,

1498
01:30:46.705 --> 01:30:47.825
H two, right?

1499
01:30:47.925 --> 01:30:49.945
And so on. So for the input layer, yes,

1500
01:30:50.005 --> 01:30:52.185
you did your normalization well and good.

1501
01:30:52.335 --> 01:30:54.225
Your features are all normalized.

1502
01:30:54.765 --> 01:30:57.825
Uh, so, so your network is, you know, set up properly to,

1503
01:30:57.925 --> 01:30:59.505
to learn from the features.

1504
01:31:00.005 --> 01:31:02.145
But as you pass through this network, uh,

1505
01:31:02.245 --> 01:31:06.385
and new inputs are created through activations, uh,

1506
01:31:06.685 --> 01:31:08.865
and, uh, the range

1507
01:31:08.865 --> 01:31:11.545
of the values change dramatically ba

1508
01:31:11.545 --> 01:31:12.625
based on the activations.

1509
01:31:13.675 --> 01:31:16.815
So batch normalization essentially a technique

1510
01:31:17.075 --> 01:31:19.095
of taking this input, normalization

1511
01:31:19.355 --> 01:31:22.495
and doing it across the layers as well.

1512
01:31:23.155 --> 01:31:27.135
Uh, so, so by doing it, so essentially across each layer,

1513
01:31:27.755 --> 01:31:29.255
uh, you're doing the same technique,

1514
01:31:29.255 --> 01:31:32.455
you are taking an input, um, uh,

1515
01:31:33.195 --> 01:31:37.375
and, uh, you are taking a mean from a batch, uh,

1516
01:31:37.735 --> 01:31:40.415
standard deviation from the batch, and then normalizing it

1517
01:31:41.035 --> 01:31:44.495
and using those values instead of the actual, um, um,

1518
01:31:44.715 --> 01:31:46.495
values, actual activation values.

1519
01:31:47.675 --> 01:31:50.695
One additional thing we are doing in batch normalization is

1520
01:31:50.775 --> 01:31:52.695
a scaler and shift our operation.

1521
01:31:53.515 --> 01:31:56.865
So, so this is the regular normalization operation.

1522
01:31:57.125 --> 01:32:00.405
So you take an input, you remove the, uh,

1523
01:32:00.995 --> 01:32:04.085
mean divide by standard deviation, uh,

1524
01:32:04.185 --> 01:32:05.205
and then you take this

1525
01:32:05.265 --> 01:32:07.885
and do use two, uh, um,

1526
01:32:08.835 --> 01:32:10.845
parameter learnable parameters, beta

1527
01:32:10.845 --> 01:32:13.845
and gamma, uh, to do a scale and shift.

1528
01:32:13.945 --> 01:32:18.485
So, so you're just changing the scale by this multiplication

1529
01:32:18.665 --> 01:32:23.245
and shifting it by p uh, so these are learnable parameters

1530
01:32:23.505 --> 01:32:26.725
and give the network the ability to learn this, like,

1531
01:32:26.725 --> 01:32:28.805
you know, uh, so

1532
01:32:31.615 --> 01:32:32.645
think of it this way.

1533
01:32:32.705 --> 01:32:36.525
So your input, your

1534
01:32:37.105 --> 01:32:40.595
not optimizing all well and good.

1535
01:32:41.095 --> 01:32:44.755
Now, in the hidden layer, you're doing batch normalization.

1536
01:32:46.095 --> 01:32:49.595
So the first part is not normalization,

1537
01:32:50.975 --> 01:32:53.555
and then second part is scale and shift.

1538
01:32:54.255 --> 01:32:57.745
So you might want ask like, why scale and shift?

1539
01:32:57.765 --> 01:32:58.985
Why not just normalize, right?

1540
01:32:59.525 --> 01:33:00.865
So, so by normalizing,

1541
01:33:00.895 --> 01:33:02.985
it's essentially taking all the input features

1542
01:33:03.085 --> 01:33:07.675
and putting it in, uh, one normal range,

1543
01:33:07.775 --> 01:33:09.355
so everything is in the same range,

1544
01:33:10.255 --> 01:33:13.275
but network might benefit a bit more.

1545
01:33:13.275 --> 01:33:17.435
Like if there is a slight variation in that, uh, uh, range,

1546
01:33:18.055 --> 01:33:20.835
uh, like, you know, it might benefit a bit more by,

1547
01:33:20.835 --> 01:33:22.275
you know, representing some

1548
01:33:22.275 --> 01:33:24.635
of the hidden activations in a slightly different range.

1549
01:33:25.175 --> 01:33:27.075
So that's why by adding these beta

1550
01:33:27.075 --> 01:33:29.995
and gamma, uh, parameters, uh,

1551
01:33:30.135 --> 01:33:33.875
you are giving the opportunity to the network to scale

1552
01:33:33.975 --> 01:33:36.835
and shift in different ways, even these normalized values.

1553
01:33:36.895 --> 01:33:39.715
So it gives a bit more representative power to the network.

1554
01:33:40.855 --> 01:33:44.235
So, so you do normalization and you scale and shift,

1555
01:33:45.015 --> 01:33:49.275
and then, uh, use those as your, uh, inputs, activations

1556
01:33:49.335 --> 01:33:50.995
for the subsequent layers.

1557
01:33:51.615 --> 01:33:54.235
So why is it called batch?

1558
01:33:55.065 --> 01:33:58.285
So, because this operation is done in batches.

1559
01:33:58.665 --> 01:34:01.885
So as you all know, like, you know, a, uh,

1560
01:34:02.405 --> 01:34:06.525
training happens in epochs, an epoch is a combination

1561
01:34:06.525 --> 01:34:09.645
of batches, uh, batches, like, you know, uh,

1562
01:34:09.645 --> 01:34:11.445
so let's take an example.

1563
01:34:11.505 --> 01:34:14.045
So let's say you have a, a training data set of

1564
01:34:16.765 --> 01:34:18.715
let's say a thousand observations.

1565
01:34:19.745 --> 01:34:23.565
And once you go through this entire thousand observations,

1566
01:34:23.585 --> 01:34:24.925
that's one epoch.

1567
01:34:26.525 --> 01:34:28.665
But before you even do that, uh,

1568
01:34:28.945 --> 01:34:31.465
a the entire training data set is broken down

1569
01:34:31.495 --> 01:34:32.545
into mini batches.

1570
01:34:33.005 --> 01:34:35.905
Uh, that's, so let's say a batch size is a hundred.

1571
01:34:36.685 --> 01:34:40.305
So for a batch size of a hundred, uh, you kind

1572
01:34:40.305 --> 01:34:44.545
of do a forward propagation compute loss and back propagate.

1573
01:34:45.325 --> 01:34:49.105
Um, and, uh, during that batch size,

1574
01:34:49.125 --> 01:34:51.425
you are also doing this batch normalization.

1575
01:34:51.685 --> 01:34:54.745
So, uh, you're doing the, uh, the mean

1576
01:34:54.765 --> 01:34:58.645
and standard division within this, uh, a hundred batch size

1577
01:34:59.425 --> 01:35:02.885
and scaling and shifting, uh, and learning the scaling

1578
01:35:02.945 --> 01:35:04.125
and shifting parameters

1579
01:35:04.825 --> 01:35:08.685
and, uh, doing the forward prop as well as, you know,

1580
01:35:08.685 --> 01:35:10.005
backward propagation.

1581
01:35:10.775 --> 01:35:14.755
So that's how, uh, you know, batch normalization, uh, works,

1582
01:35:15.255 --> 01:35:19.995
uh, and it helps to, uh, reduce variances in the

1583
01:35:20.895 --> 01:35:23.275
inputs to various hidden layers, uh,

1584
01:35:23.295 --> 01:35:26.395
but also gives the network some control over, you know,

1585
01:35:26.495 --> 01:35:28.835
how much those inputs should vary, uh, so

1586
01:35:28.835 --> 01:35:30.795
that the network is able to learn better.

1587
01:35:34.545 --> 01:35:38.885
Um, so the last one here, at least stopping,

1588
01:35:39.345 --> 01:35:41.565
uh, there is no, no, no slide for that,

1589
01:35:41.585 --> 01:35:44.325
but essentially it's fairly straightforward.

1590
01:35:44.545 --> 01:35:49.105
So again, it's the ability to look at,

1591
01:35:49.325 --> 01:35:51.905
you know, how the loss is progressing, uh,

1592
01:35:51.965 --> 01:35:56.225
by monitoring both validation and training losses

1593
01:35:57.165 --> 01:36:02.125
and using some sort of criteria that if validation loss,

1594
01:36:02.865 --> 01:36:04.365
uh, is not decreasing,

1595
01:36:04.865 --> 01:36:07.885
but the training loss is decreasing, uh, that's a,

1596
01:36:08.365 --> 01:36:11.005
a good time to stop, uh, the training process.

1597
01:36:11.745 --> 01:36:14.765
Uh, or if the validation loss is actually increasing, but,

1598
01:36:14.765 --> 01:36:16.325
and the training loss is decreasing,

1599
01:36:16.325 --> 01:36:17.765
that's also a good time to stop.

1600
01:36:18.065 --> 01:36:20.685
So, so there are different stopping criteria you could use

1601
01:36:20.785 --> 01:36:22.005
to do the at least stopping.

1602
01:36:27.005 --> 01:36:30.755
Cool. So let's see what's next?

1603
01:36:33.215 --> 01:36:36.305
Alright, uh, next we'll go through like, you know, different

1604
01:36:37.145 --> 01:36:40.545
PNN architectures, um, that are out there.

1605
01:36:40.805 --> 01:36:42.145
Uh, um,

1606
01:36:42.485 --> 01:36:44.905
and, uh, these are like, you know, the,

1607
01:36:45.565 --> 01:36:48.785
the most popular architectures that are used, uh,

1608
01:36:48.845 --> 01:36:52.025
in practice, um, starting from Linnet.

1609
01:36:52.165 --> 01:36:56.985
Um, it was, uh, introduced by Yan, uh,

1610
01:36:57.285 --> 01:37:00.025
for, uh, handwritten digits.

1611
01:37:00.095 --> 01:37:02.905
It's, uh, basically, I think it's back in 1998.

1612
01:37:03.525 --> 01:37:06.465
Um, again, at this point, the convolution layers,

1613
01:37:06.535 --> 01:37:09.025
pooling layers, all of these like, you know,

1614
01:37:09.085 --> 01:37:12.625
are hand created filters, like, you know, uh, so

1615
01:37:14.155 --> 01:37:18.565
they're essentially trying to find, uh, the right filters

1616
01:37:18.705 --> 01:37:19.965
to detect edges

1617
01:37:20.225 --> 01:37:23.365
and detect different features of the, uh,

1618
01:37:23.365 --> 01:37:24.445
hand created features.

1619
01:37:25.145 --> 01:37:28.885
Um, AlexNet, as I said yesterday, you know, uh,

1620
01:37:29.885 --> 01:37:34.325
I think it 2012, this kind of revolutionized the CNN space.

1621
01:37:34.505 --> 01:37:38.525
One is because in the paper they also introduced a,

1622
01:37:38.685 --> 01:37:40.045
a data set called ImageNet,

1623
01:37:40.895 --> 01:37:43.005
which is a huge open source data set.

1624
01:37:43.115 --> 01:37:44.405
This is the first time, like, you know,

1625
01:37:44.585 --> 01:37:47.085
that's such a huge data set, is open sourced,

1626
01:37:47.425 --> 01:37:49.165
and everyone, everyone can start using

1627
01:37:49.185 --> 01:37:50.965
and training from those data sets.

1628
01:37:51.425 --> 01:37:53.405
Uh, so that was a big deal then, back then.

1629
01:37:53.905 --> 01:37:58.085
Uh, so, so it introduced that huge data set, uh, as well as,

1630
01:37:58.145 --> 01:38:02.205
you know, introduced, uh, architecture, uh, where, you know,

1631
01:38:02.585 --> 01:38:06.845
the, how different convolution layers, cooling layers,

1632
01:38:06.905 --> 01:38:09.285
and fully connected layers are stacked on top of each other.

1633
01:38:09.785 --> 01:38:14.765
Um, so, uh, was a much deeper architecture.

1634
01:38:14.825 --> 01:38:16.245
So lean was very simpler.

1635
01:38:16.825 --> 01:38:20.085
Uh, with AlexNet, you have, you know, multiple stacks

1636
01:38:20.085 --> 01:38:23.165
of these, uh, convolution, um,

1637
01:38:23.795 --> 01:38:28.045
cooling layers stacked on top of each other, um, uh,

1638
01:38:28.745 --> 01:38:31.285
before coming up with a final fully connected layer.

1639
01:38:36.655 --> 01:38:41.155
Um, so if, if I believe, uh, AlexNet, um,

1640
01:38:41.735 --> 01:38:45.635
was using, uh, filter sizes like, you know, three by three

1641
01:38:45.635 --> 01:38:47.235
and five by five, uh,

1642
01:38:47.455 --> 01:38:52.275
and, um, uh, Vnet is again, an offshoot from AlexNet, uh,

1643
01:38:52.275 --> 01:38:55.035
that was introduced, uh, from Oxford.

1644
01:38:55.855 --> 01:38:59.795
Um, uh, it has a very uniform architecture, uh,

1645
01:39:00.025 --> 01:39:01.875
with very smaller sized filters.

1646
01:39:01.875 --> 01:39:05.035
So this introduced like, you know, small size filters, um,

1647
01:39:05.375 --> 01:39:08.475
so consistent small size filters to be able

1648
01:39:08.475 --> 01:39:10.475
to extract, uh, information.

1649
01:39:11.575 --> 01:39:14.955
Um, so that, again, these are all of these variations

1650
01:39:15.055 --> 01:39:18.675
of different offshoots of AlexNet, uh,

1651
01:39:18.785 --> 01:39:20.355
with slight improvements.

1652
01:39:20.775 --> 01:39:25.435
So for V Gnet, they introduced small filters, um, with,

1653
01:39:26.095 --> 01:39:27.155
uh, Google net

1654
01:39:27.295 --> 01:39:29.995
or inception net, um, as it's famously called.

1655
01:39:30.655 --> 01:39:33.875
So they introduced instead of one filter size,

1656
01:39:33.875 --> 01:39:36.675
they introduced like, you know, multiple filters, sizes,

1657
01:39:36.675 --> 01:39:39.315
like, you know, in the same layer, like there is three

1658
01:39:39.315 --> 01:39:42.315
by three, five by five, seven by seven filters, uh,

1659
01:39:42.315 --> 01:39:44.755
happening in the same layer in parallel, uh,

1660
01:39:44.775 --> 01:39:46.795
to capture features at different scales.

1661
01:39:47.455 --> 01:39:48.595
Uh, so, so

1662
01:39:48.595 --> 01:39:51.275
because of this parallelization nature, again,

1663
01:39:51.395 --> 01:39:53.435
Google Net is known for its efficiency

1664
01:39:54.055 --> 01:39:56.355
and has achieved very high accuracy, uh,

1665
01:39:56.355 --> 01:39:58.035
while reducing the number of parameters.

1666
01:39:59.065 --> 01:40:03.245
Uh, resnet is also another, uh, famous innovation here.

1667
01:40:03.275 --> 01:40:06.325
They introduce, introduce residual connections, uh,

1668
01:40:06.385 --> 01:40:09.805
or skip connections, so skip connections

1669
01:40:11.025 --> 01:40:13.615
or, uh, residual connections.

1670
01:40:14.685 --> 01:40:18.305
Um, so with this, they, uh, what they're trying

1671
01:40:18.305 --> 01:40:20.225
to address is, you know, with some

1672
01:40:20.225 --> 01:40:24.505
of the previous architectures, uh, there was this, um, uh,

1673
01:40:24.855 --> 01:40:26.665
vanishing gradient problem happening

1674
01:40:26.665 --> 01:40:30.625
because you know how deeper these network got, um, uh,

1675
01:40:30.765 --> 01:40:32.705
and, uh, that was kind of addressed

1676
01:40:32.705 --> 01:40:34.225
through this, uh, skip connection.

1677
01:40:34.725 --> 01:40:37.825
Uh, again, going through what Skip connection is and,

1678
01:40:37.885 --> 01:40:39.265
and how it's structured is kind

1679
01:40:39.265 --> 01:40:40.585
of out scope of this particular class.

1680
01:40:40.605 --> 01:40:42.305
You'll learn it later in the CV classes.

1681
01:40:42.925 --> 01:40:46.185
Um, but, um, but that's, that's the core, uh,

1682
01:40:46.255 --> 01:40:48.145
edition in the resnet paper.

1683
01:40:49.895 --> 01:40:52.235
Um, uh, dense net.

1684
01:40:52.455 --> 01:40:55.445
Uh, again, uh, same thing like, you know, it's,

1685
01:40:55.445 --> 01:40:56.845
it's another offshoot of how

1686
01:40:56.845 --> 01:40:59.605
to alleviate the vanishing gradient problem, uh,

1687
01:40:59.705 --> 01:41:02.325
and strengthen the, uh, feature propagation.

1688
01:41:02.825 --> 01:41:05.645
So it introduced a different way called dense connections,

1689
01:41:06.105 --> 01:41:08.005
uh, in the feet forward manner.

1690
01:41:08.705 --> 01:41:11.405
Um, mobile net and efficient head.

1691
01:41:11.615 --> 01:41:15.085
These are lighter weight versions, uh, of, of the,

1692
01:41:15.385 --> 01:41:16.885
the more popular architectures.

1693
01:41:17.465 --> 01:41:20.605
Uh, these are used in more edge device applications,

1694
01:41:20.715 --> 01:41:22.485
like mobiles and embedded devices.

1695
01:41:23.105 --> 01:41:28.005
Uh, so the parameters are pruned, quantized for efficiency,

1696
01:41:28.745 --> 01:41:31.565
uh, while also maintaining the reasonable accuracy.

1697
01:41:31.565 --> 01:41:33.565
These are kind of distilled variations of

1698
01:41:34.085 --> 01:41:35.245
existing larger models.

1699
01:41:37.375 --> 01:41:41.485
Uh, unit, um, is another, uh,

1700
01:41:41.485 --> 01:41:44.405
important appre heavily used in semantic segmentation.

1701
01:41:45.065 --> 01:41:49.865
Uh, so, uh, it uses also skip connections.

1702
01:41:50.325 --> 01:41:53.105
Uh, the unit, the name comes from the fact that, you know,

1703
01:41:53.105 --> 01:41:54.465
there are so many skip connections

1704
01:41:54.465 --> 01:41:55.825
between different parts of the network.

1705
01:41:56.605 --> 01:42:00.465
Um, and, um, it has like, you know, um,

1706
01:42:00.765 --> 01:42:01.865
an ENC quarter pathway

1707
01:42:02.245 --> 01:42:06.545
and decoder pathway, uh, kind, uh, which non samples

1708
01:42:06.725 --> 01:42:09.545
and up samples, the feature maps to generate predictions.

1709
01:42:10.165 --> 01:42:13.065
Uh, these, uh, unit is typically used like in, you know,

1710
01:42:13.065 --> 01:42:16.625
if you saw like an autonomous driving, if you saw like

1711
01:42:17.145 --> 01:42:18.465
semantic segmentation, right?

1712
01:42:18.465 --> 01:42:22.135
Like different, uh, instances of the object being like,

1713
01:42:22.135 --> 01:42:23.735
you know, clearly segmented.

1714
01:42:23.735 --> 01:42:28.295
Like instead of abounding box on a human, uh,

1715
01:42:28.435 --> 01:42:29.775
you can actually see a mask

1716
01:42:30.325 --> 01:42:32.455
like very close at a pixel level.

1717
01:42:32.925 --> 01:42:36.175
It's kind of, you know, uh, uh,

1718
01:42:36.535 --> 01:42:37.535
classifying at a pixel level.

1719
01:42:38.375 --> 01:42:41.035
So in those applications, it's definitely like, you know,

1720
01:42:41.105 --> 01:42:45.155
unit kind of architecture is used, uh, to get

1721
01:42:45.275 --> 01:42:48.195
to the pixel level, uh, classification and segmentation.

1722
01:42:53.975 --> 01:42:57.555
Uh, here is a quick diagram of, uh, gold architecture.

1723
01:42:58.015 --> 01:43:01.905
Um, yeah, so

1724
01:43:02.645 --> 01:43:03.465
if you see this,

1725
01:43:09.875 --> 01:43:13.285
yeah, so if you see, uh, this architecture more closer,

1726
01:43:14.115 --> 01:43:19.015
oops, you can see like, you know, different, um,

1727
01:43:19.275 --> 01:43:21.295
you know, filtering operations, uh,

1728
01:43:21.365 --> 01:43:23.095
with different kernel sizes.

1729
01:43:23.555 --> 01:43:25.815
You have one into one filter, three into three,

1730
01:43:26.645 --> 01:43:28.495
five into five, um,

1731
01:43:28.715 --> 01:43:33.455
and all of that, um, you know, are con needed, uh,

1732
01:43:33.615 --> 01:43:34.695
together before moving on

1733
01:43:34.695 --> 01:43:37.495
to another max pooling layer and convolution layer.

1734
01:43:38.195 --> 01:43:42.215
So, again, a unique architecture that was introduced to

1735
01:43:43.385 --> 01:43:45.365
be able to capture, um,

1736
01:43:47.165 --> 01:43:48.685
features at different scales.

1737
01:43:49.385 --> 01:43:52.405
In parallel, uh, this was, uh, become very famous

1738
01:43:52.625 --> 01:43:56.285
for various, uh, image related, uh, tasks.

1739
01:43:56.945 --> 01:44:00.445
Uh, so, uh, you can use the Google net architecture

1740
01:44:00.665 --> 01:44:04.125
and pre-trained model, uh, for various downstream tasks.

1741
01:44:09.265 --> 01:44:11.605
Uh, here is what I was alluding to in a,

1742
01:44:11.905 --> 01:44:13.205
uh, resonate architecture.

1743
01:44:13.365 --> 01:44:15.925
This is the residual connection I was talking about.

1744
01:44:16.425 --> 01:44:20.805
Uh, so in order to, uh, avoid the, uh,

1745
01:44:21.675 --> 01:44:23.045
vanishing gradient problem.

1746
01:44:23.785 --> 01:44:28.605
So in the Resonate architecture, what, uh, the salient, uh,

1747
01:44:28.605 --> 01:44:33.085
feature they introduced is as, as the inputs pass

1748
01:44:33.085 --> 01:44:35.205
through different layers, uh,

1749
01:44:35.655 --> 01:44:38.245
there is a skip connection here happening

1750
01:44:38.895 --> 01:44:43.615
where you also added the input to the, uh,

1751
01:44:44.275 --> 01:44:47.655
uh, to be concatenated, uh, to the output of the layers.

1752
01:44:48.395 --> 01:44:51.495
Uh, so this was introduced in 2015.

1753
01:44:51.595 --> 01:44:55.455
So, uh, so as the network grew deeper, um,

1754
01:44:55.915 --> 01:44:59.695
it was observed that adding more layers, uh, led to

1755
01:45:00.205 --> 01:45:01.255
more training error.

1756
01:45:01.875 --> 01:45:03.375
Uh, this is not due to or fitting,

1757
01:45:03.395 --> 01:45:06.695
but due to their degradation problem where information is,

1758
01:45:06.875 --> 01:45:09.285
is failing to flow, uh,

1759
01:45:09.285 --> 01:45:11.485
because of the depth of the architecture.

1760
01:45:12.185 --> 01:45:14.365
Uh, another reason is the improving

1761
01:45:14.365 --> 01:45:15.525
the ease of optimization.

1762
01:45:16.145 --> 01:45:20.365
So instead of learning what, um, the,

1763
01:45:21.765 --> 01:45:26.265
uh, FFX is, uh, now we are trying to learn

1764
01:45:26.265 --> 01:45:29.265
what the difference of FFX from access.

1765
01:45:29.805 --> 01:45:33.825
Uh, so this is a much smaller, uh, delta

1766
01:45:34.005 --> 01:45:35.385
to learn by the network.

1767
01:45:35.645 --> 01:45:37.625
So it made things easier, uh,

1768
01:45:37.625 --> 01:45:39.545
through this residual connections.

1769
01:45:40.085 --> 01:45:43.425
Uh, so the, those are the main reasons why, you know, uh,

1770
01:45:43.565 --> 01:45:44.945
uh, why, uh,

1771
01:45:45.015 --> 01:45:47.105
skip connections were introduced in the resident

1772
01:45:47.105 --> 01:45:50.905
architecture, uh, to avoid one, um,

1773
01:45:52.465 --> 01:45:54.225
training loss, training error

1774
01:45:56.225 --> 01:45:57.565
or training degradation,

1775
01:45:59.195 --> 01:46:03.105
and also improved training speed.

1776
01:46:17.225 --> 01:46:20.325
So, uh, again, um,

1777
01:46:20.865 --> 01:46:23.925
you look at the resonate architecture, there are these

1778
01:46:24.565 --> 01:46:26.485
residual blocks, uh, where you,

1779
01:46:26.485 --> 01:46:29.605
you're essentially taking the input from this part too,

1780
01:46:29.745 --> 01:46:32.445
and then, uh, adding it, uh,

1781
01:46:32.545 --> 01:46:34.085
to the output of the residual block.

1782
01:46:34.705 --> 01:46:36.845
Uh, and you're doing that multiple times here.

1783
01:46:37.345 --> 01:46:39.965
Uh, again, this is to, uh, allow

1784
01:46:40.025 --> 01:46:44.845
for better information flow while also reducing the demand

1785
01:46:44.845 --> 01:46:46.965
on the network to learn the entire function.

1786
01:46:46.965 --> 01:46:48.325
Instead, just learn the delta

1787
01:46:48.425 --> 01:46:50.205
of the function from this input.

1788
01:46:50.785 --> 01:46:54.485
Um, so, uh, that's the whole intuition behind, uh,

1789
01:46:54.485 --> 01:46:57.445
these keep connections in resonate architecture.

1790
01:47:04.145 --> 01:47:08.765
Um, we talked about these, uh, earlier, uh,

1791
01:47:09.145 --> 01:47:10.565
uh, transformer architecture.

1792
01:47:10.745 --> 01:47:11.925
So transformers,

1793
01:47:11.995 --> 01:47:14.965
they become famous in 2018 in the NLP space.

1794
01:47:15.665 --> 01:47:18.245
Um, and they were quickly adapted in the

1795
01:47:18.815 --> 01:47:19.885
image space as well.

1796
01:47:20.465 --> 01:47:23.985
Uh, so some of the, yeah, uh,

1797
01:47:24.165 --> 01:47:25.505
we can have a break at 11.

1798
01:47:25.685 --> 01:47:29.145
Uh, I want to finish the CNN part before having the break.

1799
01:47:30.705 --> 01:47:33.625
I hope you can understand, uh, just bear with me

1800
01:47:33.645 --> 01:47:35.025
for 10, 15 more minutes.

1801
01:47:37.145 --> 01:47:41.085
Uh, cool. Uh, so transformer architecture, uh, we are, uh,

1802
01:47:41.425 --> 01:47:45.365
as I said, like, you know, there are screen transformers

1803
01:47:45.955 --> 01:47:49.525
that are very famous state of that right now in the,

1804
01:47:50.185 --> 01:47:51.285
uh, CB space.

1805
01:47:51.945 --> 01:47:56.365
Uh, uh, the original twin transformer was, I believe,

1806
01:47:56.365 --> 01:47:58.405
introduced from Microsoft Research,

1807
01:47:59.105 --> 01:48:02.285
but, uh, essentially transformer architectures are, are, are

1808
01:48:03.035 --> 01:48:04.965
kind of having state of the art accuracy

1809
01:48:05.865 --> 01:48:09.005
and performance for image related tasks these days,

1810
01:48:18.425 --> 01:48:18.775
right?

1811
01:48:18.955 --> 01:48:23.635
Um, we talked about all of this, so just recapping,

1812
01:48:24.215 --> 01:48:27.315
um, the advantages of CN and so fnn

1813
01:48:27.375 --> 01:48:29.235
and MO mostly contrasting to fnn.

1814
01:48:29.735 --> 01:48:31.675
So there is more parameter sharing.

1815
01:48:31.855 --> 01:48:34.115
We saw how the same filter can be applied across,

1816
01:48:34.135 --> 01:48:36.075
and it's just one filter weights.

1817
01:48:36.575 --> 01:48:38.275
Uh, it's not like 10 filter weights,

1818
01:48:38.535 --> 01:48:41.635
but the same filter can be applied across the entire image.

1819
01:48:42.375 --> 01:48:45.975
Uh, and, uh, there are translation in variant

1820
01:48:46.165 --> 01:48:49.815
through pooling operations and, uh, conation operations.

1821
01:48:50.555 --> 01:48:52.255
Um, there is a reduced parameter count.

1822
01:48:52.255 --> 01:48:56.655
We saw how we went from 10 code 12 to, you know, uh, 10, 10

1823
01:48:56.875 --> 01:48:58.095
to the order of 10.2.

1824
01:48:58.755 --> 01:49:00.935
Um, they also use hierarchical learning.

1825
01:49:01.155 --> 01:49:04.535
So small fitters, big fitters, much bigger fitters.

1826
01:49:04.555 --> 01:49:08.095
So you go from edges to objects to bigger,

1827
01:49:08.475 --> 01:49:09.735
you know, shapes to objects.

1828
01:49:10.155 --> 01:49:14.095
Um, um, this is through the effective use

1829
01:49:14.095 --> 01:49:15.415
of convolution and cooling layers.

1830
01:49:16.035 --> 01:49:17.495
Um, and because you know

1831
01:49:17.495 --> 01:49:19.655
how these network architectures are structured, like,

1832
01:49:19.655 --> 01:49:21.255
you know, there is parallelism inherently.

1833
01:49:21.475 --> 01:49:24.295
So multiple filters can happen in parallel.

1834
01:49:24.915 --> 01:49:28.895
Um, thus speeding up the network, uh, uh,

1835
01:49:30.375 --> 01:49:34.575
training capability, uh, limitations.

1836
01:49:34.875 --> 01:49:37.695
Um, we still don't have the ability

1837
01:49:37.795 --> 01:49:41.335
to retain memory in the network, like, you know, like, uh,

1838
01:49:41.335 --> 01:49:42.895
what happened in the previous frame,

1839
01:49:43.235 --> 01:49:46.855
or what happened in the, uh, uh, in the, in the next,

1840
01:49:46.965 --> 01:49:48.125
next frame, or things like that.

1841
01:49:48.145 --> 01:49:50.405
So there is no, no inherent memory in the network.

1842
01:49:51.145 --> 01:49:53.965
Um, so, uh, that's under limitation.

1843
01:49:54.385 --> 01:49:56.565
Uh, the networks also can't handle like

1844
01:49:56.565 --> 01:49:57.765
different input sizes.

1845
01:49:58.065 --> 01:50:00.365
Uh, inherent, you have to do some pre-processing

1846
01:50:00.365 --> 01:50:04.245
to make sure the measures of, uh, are of the same size, uh,

1847
01:50:04.245 --> 01:50:06.445
because your input layer is, is kind

1848
01:50:06.445 --> 01:50:07.925
of standard the same size.

1849
01:50:08.745 --> 01:50:12.965
Um, and, uh, again, these are super compute intensive,

1850
01:50:12.965 --> 01:50:15.365
irrespective of the parallelism across filters.

1851
01:50:15.615 --> 01:50:17.445
These are super computing intensive

1852
01:50:17.445 --> 01:50:21.725
because, you know, uh, there is pooling operations, uh, uh,

1853
01:50:22.435 --> 01:50:24.605
convolution operations, uh,

1854
01:50:24.615 --> 01:50:26.325
fully connected operations happening.

1855
01:50:27.355 --> 01:50:30.935
And, uh, and you do this across multiple times, uh,

1856
01:50:30.935 --> 01:50:32.415
through the process of training.

1857
01:50:32.595 --> 01:50:36.255
So, uh, uh, the requirements for com computation

1858
01:50:36.255 --> 01:50:37.295
and memory are heavy.

1859
01:50:38.725 --> 01:50:42.105
Um, also, there is a need for, you know,

1860
01:50:42.115 --> 01:50:43.945
large scale data sets, uh,

1861
01:50:43.945 --> 01:50:47.785
because, uh, to train a good network, uh, that is able

1862
01:50:47.785 --> 01:50:49.785
to classify the reasonable accuracy,

1863
01:50:50.325 --> 01:50:51.705
you need large training data.

1864
01:50:52.245 --> 01:50:55.745
And for that, uh, the techniques like data augmentation,

1865
01:50:55.755 --> 01:50:57.705
which we'll look at later, are used.

1866
01:50:58.805 --> 01:51:02.665
Um, again, this is a fallacy for all neural networks,

1867
01:51:02.665 --> 01:51:06.825
interpretability and explainability is not a, uh, a thing

1868
01:51:06.885 --> 01:51:08.905
for, uh, you know, deep neural networks.

1869
01:51:09.805 --> 01:51:13.785
Um, so those are some limitations of cnn.

1870
01:51:14.165 --> 01:51:15.945
Uh, we've seen this yesterday, like, you know,

1871
01:51:15.945 --> 01:51:19.825
object detection use cases, uh, as well as, you know, uh,

1872
01:51:20.205 --> 01:51:22.545
object tracking use cases, uh,

1873
01:51:22.545 --> 01:51:24.665
where your CNNs are applied in real life.

1874
01:51:25.695 --> 01:51:28.675
Um, so we'll take a quick break, come back

1875
01:51:28.975 --> 01:51:32.395
and go through the working example on CNN dataset.

1876
01:51:32.935 --> 01:51:35.795
Um, so in the meantime, like if you have any questions,

1877
01:51:35.825 --> 01:51:39.915
feel free to, you know, type in, uh, and I'll try to answer,

1878
01:51:40.575 --> 01:51:41.595
but, uh, but yeah,

1879
01:51:41.835 --> 01:51:44.635
I think we covered like a good chunk of the material.

1880
01:51:45.175 --> 01:51:48.395
Uh, uh, and I think we're still on track of time.

1881
01:51:48.975 --> 01:51:53.595
Uh, R ns, uh, is about like 20, 25 slides, uh,

1882
01:51:53.595 --> 01:51:55.955
and there is a working example for r and n as well.

1883
01:51:56.855 --> 01:52:00.035
Uh, so now we looked at FNN,

1884
01:52:00.495 --> 01:52:02.435
and we looked at FCNN architecture,

1885
01:52:02.455 --> 01:52:05.955
how CNN architecture is more suited for three, uh, you know,

1886
01:52:05.975 --> 01:52:08.475
two dimensional spatial data, like images.

1887
01:52:09.255 --> 01:52:11.435
Now we'll look at like RNN architecture,

1888
01:52:11.435 --> 01:52:15.755
how it's more suited for, um, uh, uh,

1889
01:52:16.255 --> 01:52:18.835
uh, different kind of data, uh, sequential data.

1890
01:52:19.295 --> 01:52:21.875
Uh, and these are kind of like, you know, you, I want you

1891
01:52:21.895 --> 01:52:25.115
to contrast like how these different network architectures

1892
01:52:25.115 --> 01:52:28.115
evolved, uh, to serve different use cases

1893
01:52:28.495 --> 01:52:29.555
and to be more efficient.

1894
01:52:30.855 --> 01:52:32.825
Yeah, RNs have nothing to do with trustnet.

1895
01:52:33.045 --> 01:52:37.345
Uh, so, uh, those are two different things, uh, rather not.

1896
01:52:38.945 --> 01:52:42.635
Alright, uh, let's take a break, uh, 10 minute break

1897
01:52:42.975 --> 01:52:46.965
and we'll come back at, uh, 1107.

1898
01:52:47.855 --> 01:52:49.545
Alright, uh, see you in a

1899
01:52:49.545 --> 01:52:53.305
bit, everyone.

1900
01:52:53.305 --> 01:52:54.905
Hopefully everyone is back.

1901
01:52:57.235 --> 01:53:02.215
So let's start going through the coding workbook for CNN.

1902
01:53:06.035 --> 01:53:09.095
And I think, uh, there is also a poll launched

1903
01:53:09.355 --> 01:53:11.815
for both TA and instructor.

1904
01:53:12.635 --> 01:53:14.535
Uh, please give your feedback, uh,

1905
01:53:14.535 --> 01:53:15.535
that would be very helpful.

1906
01:53:16.785 --> 01:53:21.315
So, yeah. Um, so for the coding workbook,

1907
01:53:21.335 --> 01:53:24.875
we are still using the same dataset like yesterday.

1908
01:53:25.615 --> 01:53:30.075
Um, we are using the data dig recogni dataset, uh,

1909
01:53:30.135 --> 01:53:34.435
but instead of using FNN for our digital classification, uh,

1910
01:53:34.565 --> 01:53:38.675
we'll use the ENN network for, for today.

1911
01:53:39.415 --> 01:53:40.915
So, and we'll see how it works.

1912
01:53:42.275 --> 01:53:46.625
Um, want to do a quick check since, uh,

1913
01:53:47.725 --> 01:53:50.905
uh, uh, I wanna make sure I'm audible, uh,

1914
01:53:51.085 --> 01:53:52.785
and visible as well.

1915
01:53:53.045 --> 01:53:55.265
Uh, someone can gimme some feedback.

1916
01:53:56.905 --> 01:53:59.795
Awesome, thanks ish. Cool.

1917
01:54:00.345 --> 01:54:03.835
Alright, so for this, uh, dataset, um,

1918
01:54:06.335 --> 01:54:09.995
um, we'll use a different, uh, framework today.

1919
01:54:10.085 --> 01:54:11.795
We'll use TensorFlow framework.

1920
01:54:12.375 --> 01:54:16.355
Uh, uh, would you increase font a little bit?

1921
01:54:16.355 --> 01:54:19.535
Yes, absolutely. So let's

1922
01:54:19.535 --> 01:54:21.985
see better.

1923
01:54:22.765 --> 01:54:27.065
So, uh, yeah, yeah. Cool. Alright.

1924
01:54:28.525 --> 01:54:32.185
So, yeah, so basic libraries, uh, absolutely, um,

1925
01:54:32.355 --> 01:54:36.945
basic libraries, uh, done by map for plotting cbo.

1926
01:54:37.485 --> 01:54:40.265
Uh, again, uh, CBO is a interesting library

1927
01:54:40.485 --> 01:54:42.585
for doing some beautiful plots

1928
01:54:43.605 --> 01:54:47.265
and skeleton for, you know, basic trained test players

1929
01:54:47.505 --> 01:54:48.665
and computation mattresses.

1930
01:54:49.655 --> 01:54:52.475
And we'll primarily use carass and sensor flow.

1931
01:54:52.975 --> 01:54:57.515
Uh, so from carass, we are importing sequential,

1932
01:54:57.515 --> 01:54:59.915
that is like the toch NNN yesterday.

1933
01:55:00.255 --> 01:55:04.555
So sequential is, is, um, is the, uh, module

1934
01:55:04.735 --> 01:55:06.155
for, uh, models.

1935
01:55:07.175 --> 01:55:11.095
Um, and, uh, uh,

1936
01:55:11.145 --> 01:55:14.095
we're specifically importing the, these layer stands,

1937
01:55:14.365 --> 01:55:17.855
dropout flat and con, uh, max Pool.

1938
01:55:17.995 --> 01:55:20.815
Uh, we talked about all of these layers, uh,

1939
01:55:20.955 --> 01:55:22.055
in the class in theory.

1940
01:55:22.875 --> 01:55:26.255
Um, uh, for optimizer, we are using RMS prep.

1941
01:55:26.255 --> 01:55:27.615
Yesterday we used aam.

1942
01:55:28.585 --> 01:55:32.045
And again, just for, you know, variation here, uh,

1943
01:55:32.045 --> 01:55:33.445
nothing, nothing different.

1944
01:55:34.145 --> 01:55:37.765
Um, and then we are also using, uh, image data generator.

1945
01:55:37.785 --> 01:55:41.365
We talked about how CNNs need more data

1946
01:55:42.065 --> 01:55:44.855
and, uh, data augmentation is one

1947
01:55:44.855 --> 01:55:47.655
of the technique we can use to, uh, uh,

1948
01:55:48.325 --> 01:55:50.415
improve the amount of training data.

1949
01:55:51.655 --> 01:55:55.715
And, uh, we are also using some, uh, stopping criteria by,

1950
01:55:55.935 --> 01:55:58.355
um, uh, actually this is an, uh,

1951
01:55:58.675 --> 01:56:00.115
learning data optimization criteria.

1952
01:56:00.775 --> 01:56:03.035
Uh, we'll talk about it a little bit as well.

1953
01:56:04.695 --> 01:56:07.355
So, uh, before I go into that,

1954
01:56:07.695 --> 01:56:09.795
uh, let me see.

1955
01:56:10.385 --> 01:56:11.955
Yeah, uh, we can keep moving.

1956
01:56:12.735 --> 01:56:15.675
Um, so just connecting my drive here,

1957
01:56:33.775 --> 01:56:34.125
Right?

1958
01:56:34.545 --> 01:56:34.765
Uh,

1959
01:56:42.015 --> 01:56:44.715
um, just loading the same data sets

1960
01:56:45.055 --> 01:56:48.445
and looking at them, um, basic stuff,

1961
01:56:48.445 --> 01:56:50.325
which we looked at yesterday.

1962
01:56:50.665 --> 01:56:52.805
So I'll just paste the paste through those steps.

1963
01:56:58.415 --> 01:57:03.285
Um, assigning the label,

1964
01:57:06.305 --> 01:57:08.215
again, doing some basic sanity checks

1965
01:57:11.075 --> 01:57:12.175
in terms of, you know, how,

1966
01:57:12.195 --> 01:57:14.055
if there is any class imbalance or not.

1967
01:57:22.105 --> 01:57:25.155
Nothing new here. It's the same, same thing from yesterday.

1968
01:57:25.255 --> 01:57:28.735
So I'm just gonna pass through, uh, looking

1969
01:57:28.735 --> 01:57:32.015
for any null values, uh, if there exist any.

1970
01:57:32.795 --> 01:57:36.945
Um, so,

1971
01:57:36.945 --> 01:57:39.285
uh, cool.

1972
01:57:39.705 --> 01:57:41.205
So, uh, here we,

1973
01:57:41.255 --> 01:57:45.045
we'll normalize the input data We talked about, uh,

1974
01:57:45.045 --> 01:57:47.485
normalization before batch normalization.

1975
01:57:47.485 --> 01:57:48.965
We talked about input normalization.

1976
01:57:49.545 --> 01:57:51.725
Uh, here we are just doing a scale normalization.

1977
01:57:51.945 --> 01:57:54.845
We, since we know that input pixel values are in the range

1978
01:57:54.845 --> 01:57:58.405
of zero to 2 55, we're just dividing by two five

1979
01:57:58.545 --> 01:58:01.525
to bring all the values in the range of zero to one.

1980
01:58:02.345 --> 01:58:06.365
Uh, this will again, help with, uh, better optimization

1981
01:58:06.385 --> 01:58:09.645
for the net, um, network, um,

1982
01:58:10.745 --> 01:58:15.045
and then doing some basic reshaping into 28 by 28.

1983
01:58:26.745 --> 01:58:28.725
Um, so one quick note here.

1984
01:58:28.905 --> 01:58:32.405
Uh, the, the last one here is the channel size here.

1985
01:58:32.535 --> 01:58:34.485
Since it's a gray scale at this point,

1986
01:58:35.065 --> 01:58:36.245
the channel size is one.

1987
01:58:36.465 --> 01:58:37.965
Uh, there, there is only one channel.

1988
01:58:38.705 --> 01:58:43.125
Um, so Kara notation requires this additional,

1989
01:58:43.585 --> 01:58:45.285
um, dimension at the end.

1990
01:58:46.455 --> 01:58:49.075
Uh, it should be if the car, uh, if it's not, let me know.

1991
01:58:49.175 --> 01:58:51.515
Uh, so, uh, I'll also

1992
01:58:52.465 --> 01:58:54.535
paste this here just in case,

1993
01:58:57.615 --> 01:59:02.405
But, Uh, yeah, uh, hopefully the TA

1994
01:59:02.545 --> 01:59:05.645
or like operations can help with sharing this notebook.

1995
01:59:07.935 --> 01:59:12.055
Cool. Uh, so, um,

1996
01:59:13.765 --> 01:59:17.865
we want to encode the labels, uh, to, uh, categorical.

1997
01:59:18.485 --> 01:59:20.945
Uh, so doing some basic label and coding,

1998
01:59:28.205 --> 01:59:30.425
and then basic train test plate.

1999
01:59:30.615 --> 01:59:34.585
Nothing new here. Here we are doing a split size of 90 10.

2000
01:59:35.365 --> 01:59:39.225
Um, so 90% of training data

2001
01:59:40.275 --> 01:59:43.015
for training, and 10% for validation.

2002
01:59:45.255 --> 01:59:47.115
And then just visualizing some images.

2003
01:59:48.605 --> 01:59:52.705
Um, cool.

2004
01:59:52.805 --> 01:59:56.385
So next we'll go through how to define the model itself, uh,

2005
01:59:56.385 --> 01:59:57.865
through Caras APIs.

2006
01:59:59.795 --> 02:00:03.495
So here we'll look at, you know, um, the,

2007
02:00:04.855 --> 02:00:09.675
um, the, so again, we are using the carass sequential, uh,

2008
02:00:10.135 --> 02:00:12.195
module, uh, the model.

2009
02:00:12.975 --> 02:00:17.715
And as you can see, the, uh, first layer is,

2010
02:00:17.895 --> 02:00:19.795
you know, uh, number of filters.

2011
02:00:19.795 --> 02:00:21.155
We're using 32 filters.

2012
02:00:21.775 --> 02:00:23.995
So these are all the different kernels we'll use

2013
02:00:23.995 --> 02:00:25.075
in the convolution layer.

2014
02:00:25.775 --> 02:00:28.635
Uh, the first layer is the, we're adding a convolution layer

2015
02:00:29.215 --> 02:00:31.995
and the kernel size, each filter size is five by five

2016
02:00:33.195 --> 02:00:34.405
with rail activation.

2017
02:00:35.305 --> 02:00:38.445
Uh, and we already know that the input shape is 28, 28 1.

2018
02:00:39.545 --> 02:00:40.925
Um, so, um,

2019
02:00:48.905 --> 02:00:51.325
so maybe tell me, uh, like, you know, um,

2020
02:00:51.985 --> 02:00:54.205
we talked about this a little bit earlier.

2021
02:00:55.065 --> 02:00:59.725
Uh, tell me like, if you, if I apply these many filters

2022
02:01:00.765 --> 02:01:03.185
to the input shape of 28 28,

2023
02:01:03.695 --> 02:01:05.425
what is the output dimension like?

2024
02:01:13.365 --> 02:01:15.385
Uh, so this was a detail I mentioned, like, you know

2025
02:01:16.925 --> 02:01:18.505
how think of, you know,

2026
02:01:18.925 --> 02:01:20.825
how each filter creates feature maps.

2027
02:01:20.825 --> 02:01:23.105
So one filter creates one feature map, right?

2028
02:01:23.925 --> 02:01:26.865
So, so there are n filters in a convolution layer

2029
02:01:26.865 --> 02:01:28.505
that create n feature maps.

2030
02:01:29.325 --> 02:01:33.435
And, uh, so which means that, um,

2031
02:01:33.695 --> 02:01:35.835
so in this case, we are applying 32 filters.

2032
02:01:35.835 --> 02:01:39.755
So what is the output feature maps?

2033
02:01:47.615 --> 02:01:52.255
Yeah, so, so yeah,

2034
02:01:52.355 --> 02:01:54.975
so it would be 32, uh, feature maps, right?

2035
02:01:55.035 --> 02:01:56.535
So, so in this case,

2036
02:01:56.635 --> 02:01:59.015
the input dimensions now will be transformed to

2037
02:02:00.365 --> 02:02:04.215
something like 28, 28 and 32 feature maps.

2038
02:02:04.875 --> 02:02:06.615
Um, so padding equaled the same.

2039
02:02:06.725 --> 02:02:11.055
That setting is very specific to the kras API.

2040
02:02:11.595 --> 02:02:14.695
Uh, what it does is it keeps the input dimension

2041
02:02:15.675 --> 02:02:18.775
and the feature map dimension the same.

2042
02:02:18.915 --> 02:02:20.815
So 28, 28 remains the same.

2043
02:02:21.355 --> 02:02:25.015
Um, so, so it does the padding in that way, such a way that,

2044
02:02:25.995 --> 02:02:30.015
uh, we, uh, even with using the five by five filter

2045
02:02:30.395 --> 02:02:33.095
for convolutions, we'll keep the output dimension the same,

2046
02:02:34.305 --> 02:02:36.085
and that's what padding equal the same does.

2047
02:02:36.965 --> 02:02:40.985
Um, so again, adding another convolution layer on top

2048
02:02:40.985 --> 02:02:43.945
of this, uh, with, again, 32 more filters.

2049
02:02:44.925 --> 02:02:46.905
Um, so, um,

2050
02:02:47.765 --> 02:02:50.385
and keeping the padding equal to same.

2051
02:02:51.105 --> 02:02:54.875
Um, so do you know what this would create?

2052
02:02:56.015 --> 02:02:59.355
So this is the input dimension 28, 28 32.

2053
02:03:08.625 --> 02:03:11.645
Yes, stride is default size one. Yep.

2054
02:03:44.945 --> 02:03:48.275
Cool. So, um, it would be the same size

2055
02:03:48.275 --> 02:03:50.835
because, you know, we're using the same number

2056
02:03:50.855 --> 02:03:51.995
of, uh, filters.

2057
02:03:52.495 --> 02:03:56.795
So the output dimension is also, uh, the same,

2058
02:03:57.115 --> 02:03:58.155
B is the bad size here.

2059
02:03:58.535 --> 02:04:00.355
So, um, so,

2060
02:04:00.355 --> 02:04:01.715
but the actual dimensions

2061
02:04:01.715 --> 02:04:03.395
of the feature maps would be the same.

2062
02:04:03.975 --> 02:04:07.435
And then we are using a dropout layer with a dropout factor

2063
02:04:07.495 --> 02:04:09.075
of, uh, 25%.

2064
02:04:09.535 --> 02:04:12.355
Uh, we talked about how dropout can improve, uh,

2065
02:04:12.495 --> 02:04:13.555
the normalization.

2066
02:04:14.535 --> 02:04:18.955
Um, then using the a, um,

2067
02:04:23.525 --> 02:04:25.225
we are using 64 filters now.

2068
02:04:25.225 --> 02:04:30.025
So we started do 32 filter conation layers, did a max pull.

2069
02:04:31.465 --> 02:04:32.805
And, um,

2070
02:04:33.735 --> 02:04:37.165
after the max pull, we are using, uh, 64 conation filters.

2071
02:04:37.425 --> 02:04:41.805
So, so that, uh, increased the, the

2072
02:04:42.385 --> 02:04:44.765
number of, uh, dimensions to 64.

2073
02:04:47.855 --> 02:04:51.355
And again, uh, again, another 64 layer,

2074
02:04:52.055 --> 02:04:53.155
uh, with a max pool.

2075
02:04:53.575 --> 02:04:56.235
So when you do max pool with a pool size,

2076
02:04:56.255 --> 02:04:58.075
so you can use this formula.

2077
02:04:58.295 --> 02:05:01.715
So this would reduce the dimension to seven by seven.

2078
02:05:02.855 --> 02:05:07.575
Um, so we're taking

2079
02:05:08.135 --> 02:05:09.855
14, 14 64

2080
02:05:11.645 --> 02:05:16.225
and applying a convolution filter,

2081
02:05:16.965 --> 02:05:18.465
uh, with 64 filters.

2082
02:05:18.685 --> 02:05:20.505
So that will keep the dimensions the same.

2083
02:05:21.405 --> 02:05:26.065
And now when we apply max pool, uh, with two by two, uh,

2084
02:05:26.215 --> 02:05:29.865
pool size and stride two by two, uh, so

2085
02:05:29.865 --> 02:05:31.905
that would reduce the dimensions, uh,

2086
02:05:31.905 --> 02:05:34.225
because of the stride size, uh, um,

2087
02:05:34.845 --> 02:05:37.185
you can apply the formula that we used in the class.

2088
02:05:37.445 --> 02:05:40.245
So that would essentially bring it down to one

2089
02:05:40.245 --> 02:05:44.215
by seven and 64.

2090
02:05:57.915 --> 02:06:00.375
Um, again, we are doing another dropout, um,

2091
02:06:00.375 --> 02:06:01.575
before flattening it.

2092
02:06:02.355 --> 02:06:06.695
Um, and the flattened layer will have, um, these many,

2093
02:06:07.035 --> 02:06:08.335
um, features.

2094
02:06:09.235 --> 02:06:13.775
Um, we'll do a dense layer activation with, with

2095
02:06:14.315 --> 02:06:18.015
2 56, uh, notes, activations, um,

2096
02:06:18.365 --> 02:06:21.935
also doing a final dropout layer with 0.5

2097
02:06:22.515 --> 02:06:25.695
and then final soft max with, uh, you know,

2098
02:06:25.715 --> 02:06:27.215
how many classes, 10 classes, right?

2099
02:06:27.215 --> 02:06:29.775
So that's our final output layer. Um,

2100
02:06:42.815 --> 02:06:44.265
yeah, so we, we saw

2101
02:06:44.265 --> 02:06:46.225
that max pool did reduce dimension, right?

2102
02:06:46.285 --> 02:06:49.505
So 14, 14, uh, 64, the number

2103
02:06:49.505 --> 02:06:50.905
of feature maps would not reduce,

2104
02:06:51.405 --> 02:06:55.625
but you know, the, the dimensions would reduce, uh, to, uh,

2105
02:06:55.645 --> 02:06:57.065
no, we started with 14.

2106
02:06:57.305 --> 02:06:59.745
14 to, uh, we came up with seven, seven.

2107
02:07:06.535 --> 02:07:07.875
How do we pick the kennel size?

2108
02:07:09.135 --> 02:07:11.435
Uh, why add a second conation to the layer?

2109
02:07:11.905 --> 02:07:12.995
Yeah, good questions.

2110
02:07:13.205 --> 02:07:16.635
Again, the, the network architecture, like, you know, some

2111
02:07:16.635 --> 02:07:20.435
of the, uh, parameters here, like, you know, why these,

2112
02:07:20.645 --> 02:07:23.835
these layers, why these kind of, uh, filter sizes?

2113
02:07:24.455 --> 02:07:28.665
Uh, the intuition is, you know, if you look at some

2114
02:07:28.665 --> 02:07:31.265
of the architectures like, you know, resnet

2115
02:07:31.285 --> 02:07:34.065
or Google net, uh, they have very similar architectures,

2116
02:07:34.065 --> 02:07:36.165
like, you know, you have multiple convolution layers

2117
02:07:36.485 --> 02:07:38.325
followed by pooling layers and dropout.

2118
02:07:38.325 --> 02:07:40.805
So there is some inspiration from those.

2119
02:07:41.735 --> 02:07:45.075
The second thing is, again, it's something you have to play

2120
02:07:45.075 --> 02:07:46.715
around for, for these tasks.

2121
02:07:46.865 --> 02:07:49.395
Like, you know, what is the right architecture said, it's,

2122
02:07:49.395 --> 02:07:51.475
I'm, I'm going to repeat the same answer I said yesterday,

2123
02:07:51.475 --> 02:07:54.075
like, what is the right architecture for any given case?

2124
02:07:54.495 --> 02:07:56.715
One, you can use literature to inspire.

2125
02:07:56.785 --> 02:07:59.075
Like what is the, what are the different architectures

2126
02:07:59.075 --> 02:08:00.995
that are being used for similar kind of problems?

2127
02:08:01.495 --> 02:08:05.035
Two, uh, you can do some experimentation yourself, um,

2128
02:08:05.415 --> 02:08:07.795
and find out, uh, what that is.

2129
02:08:07.795 --> 02:08:10.155
Again, there is an active area research called, uh,

2130
02:08:10.185 --> 02:08:12.875
near architecture set, uh, which kind

2131
02:08:12.875 --> 02:08:13.995
of like takes the input

2132
02:08:14.015 --> 02:08:15.755
and tries to come up with the right architecture.

2133
02:08:16.215 --> 02:08:20.875
Uh, so, so all of these are kind of, you know, by, uh, uh,

2134
02:08:21.385 --> 02:08:24.195
depending on the problem at hand, depending on what kind

2135
02:08:24.195 --> 02:08:28.475
of problem you decide, uh, the network size and structure.

2136
02:08:29.675 --> 02:08:33.175
Uh, sometimes it also depends on like if your input size is,

2137
02:08:33.355 --> 02:08:34.895
you know, one megapixel

2138
02:08:34.895 --> 02:08:38.415
and you're trying to come down to, you know, certain answer,

2139
02:08:38.485 --> 02:08:42.735
like, you know, like a a 10 class, uh, output.

2140
02:08:43.155 --> 02:08:46.935
Uh, so how do you down sample, uh, across the layers?

2141
02:08:47.325 --> 02:08:49.055
Like how do you ups sample and downs sample?

2142
02:08:49.155 --> 02:08:50.735
How much do you want to change?

2143
02:08:51.155 --> 02:08:52.895
Uh, how much computation power do you have?

2144
02:08:52.915 --> 02:08:54.375
How many parameters you can learn?

2145
02:08:54.475 --> 02:08:56.975
All depends on, you know, the problem at hand.

2146
02:08:57.475 --> 02:09:01.055
So there is no one answer like, you know, why we used, uh,

2147
02:09:01.295 --> 02:09:02.575
a given network size

2148
02:09:02.675 --> 02:09:03.775
or network architecture,

2149
02:09:07.615 --> 02:09:10.425
then 10, because, you know, we need 10 classes, right?

2150
02:09:10.765 --> 02:09:12.825
Uh, so it's, uh, fairly straightforward.

2151
02:09:12.965 --> 02:09:16.585
So why we need 10 in output layer, uh, 2 56.

2152
02:09:16.595 --> 02:09:19.425
We're trying to reduce the dimensions from this number to,

2153
02:09:19.605 --> 02:09:23.025
uh, a smaller number before we even reduce to 10.

2154
02:09:23.785 --> 02:09:26.845
So it's kind of, you know, proportional reduce, uh,

2155
02:09:26.915 --> 02:09:29.645
from this number to, you know, a small number.

2156
02:09:30.265 --> 02:09:31.885
Um, so, so

2157
02:09:31.885 --> 02:09:35.625
yeah, why max pool?

2158
02:09:35.625 --> 02:09:39.545
Is it trend? Uh, we talked about max pooling, right?

2159
02:09:39.645 --> 02:09:43.545
So it's not maximizing the metric, it's, it's a ma, it's a,

2160
02:09:43.545 --> 02:09:46.825
it's a, it's the most popular pooling technique.

2161
02:09:47.285 --> 02:09:52.265
Um, so, uh, so max pooling

2162
02:09:52.875 --> 02:09:57.305
takes whatever is in the, uh, in the field of view for

2163
02:09:57.305 --> 02:10:00.265
that particular pooling filter and takes the max value.

2164
02:10:00.925 --> 02:10:01.825
Um, so

2165
02:10:06.825 --> 02:10:07.515
I'll keep moving.

2166
02:10:07.775 --> 02:10:09.555
Um, I'll, uh, answer questions

2167
02:10:09.555 --> 02:10:11.875
after the notebook, uh, as well.

2168
02:10:12.215 --> 02:10:16.755
So for the optimizer, we are using RMS prop, um, with, uh,

2169
02:10:17.045 --> 02:10:21.155
these default values, uh, learning rate 0.001,

2170
02:10:21.855 --> 02:10:24.315
um, and decay of 0.0.

2171
02:10:24.935 --> 02:10:29.685
Um, and we also used, uh, uh,

2172
02:10:29.835 --> 02:10:31.245
reduced LR on latitude.

2173
02:10:31.585 --> 02:10:33.525
So, um, uh, function.

2174
02:10:33.625 --> 02:10:37.045
So for that, we will monitor the validation loss, uh,

2175
02:10:37.345 --> 02:10:42.045
and for across three ocs, if the learning rate, uh,

2176
02:10:42.275 --> 02:10:45.885
does not decrease, uh, then we reduce the, sorry,

2177
02:10:45.905 --> 02:10:49.725
if the loss does not decrease, we reduce the learning rate

2178
02:10:49.745 --> 02:10:50.885
by a factor of 0.5.

2179
02:10:51.425 --> 02:10:54.165
So the new new learning rate is, is 0.5

2180
02:10:54.165 --> 02:10:55.365
of the previous learning rate,

2181
02:10:56.265 --> 02:10:58.485
and we won't let it go below this number.

2182
02:10:58.665 --> 02:11:00.565
The learning rate cannot go below this number.

2183
02:11:00.905 --> 02:11:03.645
Um, so, so that's the, that's a learning rate.

2184
02:11:03.705 --> 02:11:05.525
Any, any learn will use to

2185
02:11:06.085 --> 02:11:07.885
optimize the learning rate if the learning becomes

2186
02:11:08.025 --> 02:11:09.685
too, um, little.

2187
02:11:10.505 --> 02:11:13.975
Um, so, uh, that's something new we'll use here.

2188
02:11:14.545 --> 02:11:16.735
Batch size 86, uh,

2189
02:11:17.115 --> 02:11:19.615
and we'll use number of iCal eight.

2190
02:11:20.595 --> 02:11:23.735
Uh, we talked about data augmentation in theory earlier,

2191
02:11:23.955 --> 02:11:27.135
so we'll use the image data generator, uh, function.

2192
02:11:27.875 --> 02:11:32.015
Uh, this creates various versions of the images, uh,

2193
02:11:32.155 --> 02:11:33.935
in real time during training phase.

2194
02:11:34.635 --> 02:11:38.335
Uh, so it increases, explodes the training data, uh, so

2195
02:11:38.335 --> 02:11:40.855
that, uh, we'll get more training data for,

2196
02:11:40.955 --> 02:11:42.175
for our training purposes.

2197
02:11:43.475 --> 02:11:45.855
So here are some, some things will apply, like, you know,

2198
02:11:45.915 --> 02:11:50.255
we can rotate the image, um, zoom the image,

2199
02:11:50.755 --> 02:11:51.855
um, you know, uh,

2200
02:11:52.035 --> 02:11:55.815
and shift, uh, shift, uh, like, you know,

2201
02:11:55.815 --> 02:11:57.815
if there is a number in this portion of the image,

2202
02:11:57.815 --> 02:11:59.295
we'll shift it to the side.

2203
02:11:59.715 --> 02:12:03.055
Uh, so we'll create all newer versions of these images by,

2204
02:12:03.195 --> 02:12:04.895
uh, doing all these operations.

2205
02:12:05.395 --> 02:12:08.095
Um, we're not doing horizontal flip and vertical flip

2206
02:12:08.095 --> 02:12:10.375
because, you know, since these are numbers making,

2207
02:12:10.405 --> 02:12:12.455
that will actually change the number itself,

2208
02:12:12.455 --> 02:12:13.855
like nine can become a six.

2209
02:12:14.315 --> 02:12:16.575
So we are not doing, uh, some of that.

2210
02:12:17.355 --> 02:12:22.255
Um, and, um, so once we apply the data gen,

2211
02:12:22.255 --> 02:12:24.255
we'll get, uh, more training data.

2212
02:12:24.715 --> 02:12:26.895
Uh, this is only applied on the training data.

2213
02:12:28.165 --> 02:12:29.985
Um, so for this loss, uh,

2214
02:12:30.055 --> 02:12:32.465
here we are using a means code error loss,

2215
02:12:33.005 --> 02:12:35.265
but, you know, categorical cross entropy is,

2216
02:12:35.445 --> 02:12:36.665
uh, can be used as well.

2217
02:12:37.205 --> 02:12:40.145
Um, so it's just to show the variation that, hey,

2218
02:12:40.145 --> 02:12:44.025
we can also use this loss, uh, on this particular problem,

2219
02:12:44.365 --> 02:12:47.505
um, by you instead of using properties, by using logics,

2220
02:12:47.645 --> 02:12:49.545
we can use this means grid error loss.

2221
02:12:50.325 --> 02:12:54.305
Um, so, but traditionally for class problems, yeah,

2222
02:12:54.305 --> 02:12:56.865
categorically cross entropy loss is the right usage.

2223
02:12:57.445 --> 02:13:00.225
Uh, here it is done just to show that we can also do that.

2224
02:13:01.865 --> 02:13:06.165
Um, and then once we do that, let's, lemme run some

2225
02:13:06.165 --> 02:13:08.425
of these cells.

2226
02:13:21.565 --> 02:13:22.575
Alright, so

2227
02:13:30.775 --> 02:13:33.795
one OC is one run through the entire training data set.

2228
02:13:34.535 --> 02:13:38.995
Uh, one forward propagation is one, one sample, uh,

2229
02:13:39.295 --> 02:13:43.595
and, um, one batch is, you know,

2230
02:13:43.595 --> 02:13:47.915
when you consolidate the loss and do the back propagation.

2231
02:13:51.575 --> 02:13:54.835
So we are fitting the model, uh, after compiling it.

2232
02:13:55.535 --> 02:13:58.275
Um, so, uh, we'll see how the,

2233
02:13:58.935 --> 02:14:01.875
and we're emitting certain metrics in terms of validation,

2234
02:14:01.905 --> 02:14:05.275
loss and, um, training loss.

2235
02:14:08.685 --> 02:14:11.335
Like we have a grid random technique for hyper.

2236
02:14:11.795 --> 02:14:14.175
Is there any for fine suitable architecture?

2237
02:14:14.205 --> 02:14:17.455
Yeah, uh, I think, uh, you know, I'll, I'll try to put, uh,

2238
02:14:17.455 --> 02:14:18.935
the paper I was talking about.

2239
02:14:19.115 --> 02:14:22.645
So let's see. Uh,

2240
02:14:33.785 --> 02:14:38.645
so, uh, read through this, uh, saje, uh, maybe you can,

2241
02:14:39.145 --> 02:14:41.405
you know, get more detail around that.

2242
02:14:41.635 --> 02:14:43.365
Yeah, it's, it's similar to that,

2243
02:14:43.425 --> 02:14:47.285
but, you know, use some sort of techniques to, to be able to

2244
02:14:47.915 --> 02:14:49.565
find the right architecture set.

2245
02:14:52.765 --> 02:14:54.895
Oops, I think I only sent it to.

2246
02:15:02.705 --> 02:15:06.885
So while we are waiting on that model to train, um, again,

2247
02:15:07.015 --> 02:15:11.045
basic confusion, metrics, uh, plotting, uh,

2248
02:15:11.265 --> 02:15:14.525
so we can see like, you know, how the, uh, true

2249
02:15:14.525 --> 02:15:15.685
and predicted labels are,

2250
02:15:15.815 --> 02:15:17.245
looks like the model is doing well.

2251
02:15:18.785 --> 02:15:21.565
And, uh, also look at some error cases.

2252
02:15:22.585 --> 02:15:25.365
Um, so we can see like, you know,

2253
02:15:25.365 --> 02:15:26.845
where the model is making errors

2254
02:15:27.385 --> 02:15:29.005
and what are the reason for that.

2255
02:15:29.865 --> 02:15:33.765
Um, so, uh, you can see some instances where, you know,

2256
02:15:33.765 --> 02:15:37.165
the model is spreading, uh, incorrectly, uh,

2257
02:15:37.195 --> 02:15:40.205
because you know, the labels are not, uh, properly

2258
02:15:40.985 --> 02:15:43.085
or the handwritten digits are too noisy.

2259
02:15:44.505 --> 02:15:48.645
And, uh, yeah, essentially, uh, that brings us

2260
02:15:48.645 --> 02:15:50.045
to the end of the coding workbook.

2261
02:15:50.745 --> 02:15:53.045
Uh, so the key things that, new things

2262
02:15:53.045 --> 02:15:55.245
that you learned here are, you know, one is how

2263
02:15:55.245 --> 02:15:59.295
to set up the Ks, uh, layers, um,

2264
02:16:00.395 --> 02:16:04.935
and, uh, the kernel sizes, the filters, uh, the pool sizes,

2265
02:16:05.555 --> 02:16:09.175
uh, dropouts, uh, and the flat and dense layers.

2266
02:16:10.035 --> 02:16:13.295
Uh, second thing is, you know how to use a, uh,

2267
02:16:13.535 --> 02:16:18.415
learning rate alr where you can use some sort of, uh,

2268
02:16:18.715 --> 02:16:20.975
EMR to change the learning rate based on

2269
02:16:20.975 --> 02:16:22.775
how the validation loss is progressing.

2270
02:16:26.125 --> 02:16:30.725
Yeah. Uh, and also how to use an image data generator,

2271
02:16:31.025 --> 02:16:32.765
uh, to improve your training data.

2272
02:16:33.785 --> 02:16:36.565
Uh, yes, no brother, let me look at that.

2273
02:16:36.655 --> 02:16:38.205
These are just a matter of preference,

2274
02:16:38.205 --> 02:16:39.565
but in TensorFlow, yeah.

2275
02:16:39.565 --> 02:16:41.325
Yeah, it is mostly a matter of preference.

2276
02:16:42.115 --> 02:16:45.885
Back in the day, uh, TensorFlow was, was the go-to,

2277
02:16:45.905 --> 02:16:47.525
for CNNs, uh,

2278
02:16:47.525 --> 02:16:49.405
because, you know, uh, you know, how,

2279
02:16:49.505 --> 02:16:52.805
how the CNNs got introduced, like through, you know, Google,

2280
02:16:53.705 --> 02:16:56.525
um, and, um, um,

2281
02:16:57.225 --> 02:17:00.485
and uh, through, through, through the researchers at Google

2282
02:17:01.145 --> 02:17:04.845
and, um, uh, TensorFlow was also backed by Google.

2283
02:17:05.385 --> 02:17:08.565
Um, so, uh, back in the day, TensorFlow was the go-to,

2284
02:17:08.705 --> 02:17:11.805
and then they developed Caras framework on top of it

2285
02:17:11.805 --> 02:17:13.885
to make it much more easier to, uh,

2286
02:17:14.435 --> 02:17:16.805
it's an API framework on top of, uh, TensorFlow

2287
02:17:16.825 --> 02:17:18.245
to make it much more easier to use.

2288
02:17:18.945 --> 02:17:22.045
Uh, torch is, is, you know, coming from back by meta.

2289
02:17:22.745 --> 02:17:25.645
Uh, so it gained a lot of popularity, uh,

2290
02:17:25.645 --> 02:17:28.525
in the research world because of open source nature.

2291
02:17:29.185 --> 02:17:30.565
Uh, even 10 is open source,

2292
02:17:30.625 --> 02:17:33.325
but you know, there is a lot of research happening in touch

2293
02:17:33.385 --> 02:17:36.125
and open source papers with code based on touch.

2294
02:17:36.625 --> 02:17:38.605
Uh, so, uh, and it's also very easy.

2295
02:17:38.605 --> 02:17:40.645
The APIs are very i and easy to use.

2296
02:17:40.985 --> 02:17:43.445
So it's a matter of reference between touch

2297
02:17:43.545 --> 02:17:48.465
and, uh, TensorFlow using directly TensorFlow will give

2298
02:17:48.465 --> 02:17:52.465
you more, um, more, uh, configurability options, um,

2299
02:17:53.085 --> 02:17:54.385
if you need, need that.

2300
02:17:58.195 --> 02:17:58.415
Um,

2301
02:18:03.605 --> 02:18:03.895
cool.

2302
02:18:03.955 --> 02:18:05.135
So I think this is the confident

2303
02:18:05.135 --> 02:18:06.295
metrics you're talking about number.

2304
02:18:06.595 --> 02:18:10.495
So, uh, so again, this is, we are taking the,

2305
02:18:11.115 --> 02:18:12.175
uh, let's look at the code.

2306
02:18:12.315 --> 02:18:16.975
So we're taking the con, uh, data from validation,

2307
02:18:17.675 --> 02:18:21.735
uh, and, uh, we're predicting, um, the values

2308
02:18:22.275 --> 02:18:25.695
and plotting the predicted values, uh, across, uh,

2309
02:18:25.915 --> 02:18:27.695
and, uh, across the true values.

2310
02:18:28.635 --> 02:18:31.215
So you can see, like, you know, if the true label is zero

2311
02:18:31.555 --> 02:18:33.135
and the predicted label is zero,

2312
02:18:33.525 --> 02:18:34.935
then we are predicting it, right?

2313
02:18:35.555 --> 02:18:39.055
Uh, but, uh, uh, if it's predicted, uh,

2314
02:18:39.155 --> 02:18:41.295
as something else, then it is wrong.

2315
02:18:41.595 --> 02:18:44.215
So we want to see high number in this diagonal.

2316
02:18:44.755 --> 02:18:47.095
Uh, so that's what the confusion metrics is showing.

2317
02:18:47.715 --> 02:18:49.855
Uh, so that's what we visualizing with this.

2318
02:18:49.875 --> 02:18:52.055
So this will give you some visual indicators, like, hey,

2319
02:18:52.055 --> 02:18:55.175
if a class is being predicted, uh, wrongly, like, you know,

2320
02:18:55.205 --> 02:18:57.255
some class having high values here

2321
02:18:57.755 --> 02:19:00.575
or here, uh, that will tell me that, you know,

2322
02:19:00.575 --> 02:19:02.535
there are more errors happening in those instances.

2323
02:19:05.095 --> 02:19:07.675
Uh, suggest, I don't think this notebook is available in

2324
02:19:07.815 --> 02:19:10.755
by touch, so you, you can convert properly.

2325
02:19:14.825 --> 02:19:17.875
Cool. Uh, so let's see if our model ran, uh, yeah,

2326
02:19:17.875 --> 02:19:18.875
it's running a bit slow,

2327
02:19:18.975 --> 02:19:22.635
so you can see like we are emitting, um, from OCH one.

2328
02:19:23.135 --> 02:19:26.435
Um, you have a, a training loss of 0.02,

2329
02:19:26.435 --> 02:19:28.010
validation loss 0.03,

2330
02:19:28.425 --> 02:19:30.005
and we are using this much learning rate.

2331
02:19:30.785 --> 02:19:33.205
Um, so we'll see how this progresses.

2332
02:19:33.545 --> 02:19:34.685
Uh, let's not wait for it.

2333
02:19:35.145 --> 02:19:38.565
Uh, we'll come back to it, uh, in, in a bit. So,

2334
02:19:44.945 --> 02:19:45.945
Cool.

2335
02:20:01.705 --> 02:20:03.475
Alright, so let's see.

2336
02:20:04.065 --> 02:20:08.475
Okay, so just want to quickly summarize what we learned.

2337
02:20:09.135 --> 02:20:12.955
Uh, we looked at what con neural network is.

2338
02:20:13.375 --> 02:20:16.315
We looked at what their salient principles are, like,

2339
02:20:16.315 --> 02:20:18.315
you know, locality and translation variance.

2340
02:20:19.135 --> 02:20:24.105
Um, and we discussed about various layers, con layers,

2341
02:20:24.885 --> 02:20:28.825
um, pooling layers, um, flattening and dense layers.

2342
02:20:29.925 --> 02:20:32.145
Uh, we looked at, you know, various popular

2343
02:20:32.655 --> 02:20:37.065
convolution network architectures like resnet, AlexNet,

2344
02:20:38.165 --> 02:20:42.315
um, Google Net, and so on, so forth.

2345
02:20:43.055 --> 02:20:46.915
Uh, we saw what the advantages of CNN, especially our FNNR

2346
02:20:47.735 --> 02:20:49.835
and limitations in terms of, you know,

2347
02:20:49.835 --> 02:20:50.915
sequential processing.

2348
02:20:51.575 --> 02:20:54.405
Uh, as such, uh, we saw some applications

2349
02:20:54.545 --> 02:20:56.325
and working example on a data set.

2350
02:20:57.155 --> 02:20:59.775
Um, again, we also looked at, like in the convolution layer,

2351
02:20:59.775 --> 02:21:03.575
we spent a lot of time on filters, uh, uh, you know,

2352
02:21:03.595 --> 02:21:05.455
how filters are applied, uh,

2353
02:21:05.555 --> 02:21:07.975
and how that filtering application, uh,

2354
02:21:07.975 --> 02:21:12.175
filter application will result, result in a feature map, um,

2355
02:21:12.395 --> 02:21:13.455
and so on and so forth.

2356
02:21:14.805 --> 02:21:18.225
So now let's move on to, you know, I want to tease, uh,

2357
02:21:18.225 --> 02:21:21.305
these things for, uh, to set us up for r and s.

2358
02:21:22.365 --> 02:21:26.785
So, um, think of these questions,

2359
02:21:27.485 --> 02:21:30.825
uh, and, uh, you know, have that in the back of your mind.

2360
02:21:31.765 --> 02:21:35.305
Um, so that, uh, you know, as we learn about RNs,

2361
02:21:35.305 --> 02:21:37.985
these will, you know, uh, we can connect the dots,

2362
02:21:53.545 --> 02:21:56.405
Um, sequential limitation of the C nnn.

2363
02:21:56.425 --> 02:21:57.565
So in the C nnn,

2364
02:21:57.665 --> 02:21:59.805
we are still giving one input at a time, right?

2365
02:21:59.825 --> 02:22:03.605
So we're not, um, so, uh, the CNN layer,

2366
02:22:06.695 --> 02:22:09.075
you're giving input one

2367
02:22:09.815 --> 02:22:11.835
and you are computing the feature

2368
02:22:12.025 --> 02:22:13.275
maps and so on and so forth.

2369
02:22:13.335 --> 02:22:16.155
So next, if you give input two, does the,

2370
02:22:17.825 --> 02:22:20.155
does the CNN know anything about input

2371
02:22:20.155 --> 02:22:21.195
one at that point of time?

2372
02:22:21.655 --> 02:22:22.995
It doesn't, it does

2373
02:22:22.995 --> 02:22:24.795
because it doesn't have any inherent recurrent

2374
02:22:24.965 --> 02:22:26.205
connections, right?

2375
02:22:27.025 --> 02:22:30.165
So that's the whole aspect of sequential limitation of cnn.

2376
02:22:31.405 --> 02:22:33.015
Hopefully, uh, that helps and,

2377
02:22:33.035 --> 02:22:35.575
and that, that's what we'll look into in the RN space.

2378
02:22:43.345 --> 02:22:45.285
Uh, yeah, we're jumping a few steps ahead.

2379
02:22:45.865 --> 02:22:49.255
Yes, RN is, uh, um,

2380
02:22:50.615 --> 02:22:55.055
transformer is an evaluation, evaluation of RN architecture,

2381
02:22:55.675 --> 02:22:59.375
but it's important to know why RN is useful

2382
02:22:59.475 --> 02:23:01.215
and it's, it's like the backbone.

2383
02:23:10.345 --> 02:23:13.075
Cool. Uh, let's keep moving. Uh, let's see.

2384
02:23:13.505 --> 02:23:18.405
This is still running. Yeah, let it run.

2385
02:23:18.785 --> 02:23:20.525
Um, we'll come back to it.

2386
02:23:26.925 --> 02:23:28.895
Alright, here comes R Ns.

2387
02:23:29.755 --> 02:23:32.605
Um, so again, we would like

2388
02:23:32.605 --> 02:23:35.765
to know why RNN network architectures are suited

2389
02:23:36.025 --> 02:23:38.885
for sequential problems, just like how we understood how

2390
02:23:39.525 --> 02:23:43.165
CNN architectures are suited for, um, you know,

2391
02:23:43.315 --> 02:23:45.525
spatial problems like images and stuff.

2392
02:23:47.325 --> 02:23:48.665
So we'll talk about, you know,

2393
02:23:48.875 --> 02:23:52.185
these things like introduction to RN principles, uh,

2394
02:23:52.185 --> 02:23:56.225
different layers in an RNN types of RN models, advantages,

2395
02:23:56.225 --> 02:23:57.945
limitations, applications, and finally,

2396
02:23:57.945 --> 02:24:00.585
according walkthrough in the notebook.

2397
02:24:02.355 --> 02:24:04.015
So what are sequences, right?

2398
02:24:04.155 --> 02:24:08.655
So sequences can be, you know, a, a sequence of, um,

2399
02:24:09.625 --> 02:24:13.855
words, or it could be a, a speech, speech is a sequence

2400
02:24:13.855 --> 02:24:18.555
of sounds, or it could be, um, um,

2401
02:24:18.655 --> 02:24:21.995
you know, a sequence of frames in a video, uh,

2402
02:24:22.175 --> 02:24:25.475
or, uh, it could be like, uh, um,

2403
02:24:26.435 --> 02:24:27.715
a sequence of decisions.

2404
02:24:28.615 --> 02:24:31.715
Um, uh, so sequences are everywhere.

2405
02:24:31.935 --> 02:24:34.315
Uh, it could be like, you know, in stock market example,

2406
02:24:34.455 --> 02:24:38.915
you have sequence of, you know, stock prices at, uh,

2407
02:24:38.995 --> 02:24:40.275
different time times.

2408
02:24:40.775 --> 02:24:42.275
Uh, so that's a sequence of data.

2409
02:24:47.305 --> 02:24:49.405
So for this example, we'll, we'll try

2410
02:24:49.405 --> 02:24:51.245
to understand from a text point of view.

2411
02:24:54.135 --> 02:24:57.915
So let's assume like, you know, you, you have this, um,

2412
02:24:59.095 --> 02:25:00.325
particular, um,

2413
02:25:02.205 --> 02:25:04.965
sentence modeling word properties is really difficult.

2414
02:25:05.095 --> 02:25:07.405
Let's say you're trying to model

2415
02:25:07.625 --> 02:25:10.155
or predict that sequence, right?

2416
02:25:10.215 --> 02:25:13.915
So one way to think about naive way is say, Hey, I want

2417
02:25:13.915 --> 02:25:17.075
to predict the property of the word modeling, property

2418
02:25:17.075 --> 02:25:19.955
of the word, word, uh, and properties

2419
02:25:20.775 --> 02:25:21.795
and so on and so forth.

2420
02:25:22.615 --> 02:25:25.885
And, uh, and then multiply all of those properties.

2421
02:25:26.465 --> 02:25:27.885
So that would give me the probability

2422
02:25:27.885 --> 02:25:28.925
of this entire sentence.

2423
02:25:29.705 --> 02:25:33.045
Uh, that's a very naive way of thinking, uh, because, and,

2424
02:25:33.265 --> 02:25:35.205
and the naive nature comes from the fact

2425
02:25:35.205 --> 02:25:36.685
that we are thinking that all of these

2426
02:25:37.505 --> 02:25:39.125
things are independent to each other,

2427
02:25:39.905 --> 02:25:44.005
but in reality they're not, you know, uh, uh, this has a lot

2428
02:25:44.005 --> 02:25:45.805
of bearing on what the previous word is.

2429
02:25:46.115 --> 02:25:47.285
This has a lot of bearing on

2430
02:25:47.285 --> 02:25:49.685
what the previous two words are and so on and so forth.

2431
02:25:49.685 --> 02:25:51.285
So there is this, um,

2432
02:25:51.675 --> 02:25:54.605
conditional probability based on previous words.

2433
02:25:55.185 --> 02:25:58.565
So, so if you put that into factor, so you can think of,

2434
02:25:58.625 --> 02:26:01.855
you know, the probability can be represented as,

2435
02:26:02.435 --> 02:26:04.495
you know, p of modeling.

2436
02:26:05.075 --> 02:26:09.735
Now, for the word to occur, it's P of word given modeling.

2437
02:26:10.115 --> 02:26:12.455
So P of X two given X one, uh,

2438
02:26:12.675 --> 02:26:14.295
and, uh, it can be represented.

2439
02:26:14.455 --> 02:26:18.095
P of p of probability is given a X three given X two

2440
02:26:18.095 --> 02:26:20.895
and X one the world and modeling and so on and so forth.

2441
02:26:20.915 --> 02:26:24.335
So you have this problem now, like you can, um,

2442
02:26:25.195 --> 02:26:26.975
create the probabilities like this by,

2443
02:26:26.975 --> 02:26:28.295
based on conditional probabilities

2444
02:26:28.315 --> 02:26:29.495
of all the previous words.

2445
02:26:30.665 --> 02:26:33.245
So how do you, how does this come into practice in terms of,

2446
02:26:33.745 --> 02:26:35.045
you know, setting up the problem?

2447
02:26:36.075 --> 02:26:40.745
Um, so you can think of like, you know, for

2448
02:26:42.175 --> 02:26:45.605
there is a context at any given point of time, like all

2449
02:26:45.605 --> 02:26:49.145
of this is the context of at any time,

2450
02:26:49.385 --> 02:26:52.665
t all the previous words until the time T minus one.

2451
02:26:52.675 --> 02:26:55.785
These are the context words that you need to be able

2452
02:26:55.785 --> 02:26:57.985
to predict the word at at time T.

2453
02:26:59.635 --> 02:27:03.605
So, uh, so that's the part where, you know,

2454
02:27:03.625 --> 02:27:06.605
you have reliance on previous inputs, um,

2455
02:27:06.905 --> 02:27:11.655
and there is, um, a need for carrying that context.

2456
02:27:12.875 --> 02:27:14.975
So you can think of, you know, hey, I want

2457
02:27:14.975 --> 02:27:19.335
to model this problem in a naive way by doing this, so I can

2458
02:27:19.955 --> 02:27:22.255
create my labels and features like this.

2459
02:27:22.395 --> 02:27:27.025
So, uh, I'll put like, you know, modeling

2460
02:27:28.995 --> 02:27:30.415
as my feature

2461
02:27:31.655 --> 02:27:36.285
and label is, you know,

2462
02:27:36.635 --> 02:27:39.305
word, I'll put more

2463
02:27:41.365 --> 02:27:42.655
word as features,

2464
02:27:44.405 --> 02:27:46.905
and then ties is my label.

2465
02:27:47.805 --> 02:27:51.825
So we can create these, all these different, um, you know,

2466
02:27:51.985 --> 02:27:55.985
combinations based on the sequence of word occurrences.

2467
02:27:56.805 --> 02:27:59.825
And, uh, you can, uh, model the problem.

2468
02:28:01.285 --> 02:28:04.465
Uh, it, you see the problem right away that

2469
02:28:04.615 --> 02:28:07.385
that results in a huge explosion of features.

2470
02:28:07.405 --> 02:28:12.255
So that results in, so even with a 10 word

2471
02:28:13.095 --> 02:28:16.215
sentence, um, that creates, you know, a lot of combinations.

2472
02:28:17.545 --> 02:28:21.165
And second thing one is explosion of features

2473
02:28:24.235 --> 02:28:26.135
of samples, not features.

2474
02:28:26.595 --> 02:28:28.865
Two, uh, it's inefficient

2475
02:28:28.895 --> 02:28:32.815
because you are learning this thing in one example,

2476
02:28:33.315 --> 02:28:35.535
and you're learning this thing in the second example,

2477
02:28:36.195 --> 02:28:39.015
but there is so much commonality between them, uh,

2478
02:28:39.015 --> 02:28:42.365
which you're not capturing, uh, again, so

2479
02:28:42.485 --> 02:28:45.125
that there is a lot of inefficiency, there is a lot

2480
02:28:45.125 --> 02:28:46.285
of explosion of samples.

2481
02:28:46.585 --> 02:28:48.605
So these are the things we are trying to avoid

2482
02:28:48.665 --> 02:28:52.125
by creating an architecture that can be more efficient

2483
02:28:52.425 --> 02:28:53.645
to take these into account.

2484
02:28:55.235 --> 02:29:00.165
So for that, if we can think of a way that can summarize

2485
02:29:00.785 --> 02:29:03.685
the context at any given point of time,

2486
02:29:04.035 --> 02:29:08.165
like if I'm trying to predict ease, if all

2487
02:29:08.165 --> 02:29:09.685
of this context is somehow

2488
02:29:11.665 --> 02:29:15.855
concatenated into one feature, um,

2489
02:29:16.195 --> 02:29:19.575
before that can be used into the model as a predictor,

2490
02:29:20.035 --> 02:29:21.335
um, that would be useful.

2491
02:29:21.435 --> 02:29:25.735
So think of that concept, uh, which will exploit

2492
02:29:26.435 --> 02:29:28.895
in the, in the, in the next slides.

2493
02:29:32.225 --> 02:29:36.725
So the key feature that, uh, we want to use with the R ls,

2494
02:29:36.775 --> 02:29:37.805
gimme one second guys.

2495
02:29:49.825 --> 02:29:52.605
So the key feature we're trying to exploit is, um,

2496
02:29:52.655 --> 02:29:56.645
we're trying to, um, use a hidden state,

2497
02:29:57.025 --> 02:30:00.845
or, you know, we're trying to use a context state,

2498
02:30:02.525 --> 02:30:04.425
um, by, uh, which captures

2499
02:30:05.165 --> 02:30:08.305
all the previous dependencies in the sequential data.

2500
02:30:08.925 --> 02:30:11.145
So it's kind of maintaining internal state.

2501
02:30:11.885 --> 02:30:14.465
Um, often this is often referred as hidden state

2502
02:30:14.525 --> 02:30:18.665
or memory that represents the information processed from all

2503
02:30:18.665 --> 02:30:19.865
the previous time steps.

2504
02:30:20.765 --> 02:30:23.945
Um, at any given point of time, this hidden state is updated

2505
02:30:24.125 --> 02:30:28.945
and passed as the network process subsequent inputs.

2506
02:30:29.525 --> 02:30:32.865
So, so the hidden state is constantly updated

2507
02:30:33.055 --> 02:30:34.905
with the input at a given point of time

2508
02:30:35.165 --> 02:30:36.945
and then passed to the next time step.

2509
02:30:37.765 --> 02:30:42.235
Um, so RNs, we know about back propagation, right?

2510
02:30:42.615 --> 02:30:44.755
Uh, that's the technique that is used for

2511
02:30:46.455 --> 02:30:47.785
CNNs and fns.

2512
02:30:48.965 --> 02:30:53.105
For nns, we are, we use a technical back propagation

2513
02:30:53.105 --> 02:30:57.635
through type because we are not only processing, uh,

2514
02:30:57.735 --> 02:30:59.315
the inputs one

2515
02:30:59.315 --> 02:31:03.355
after another, we are also processing a temporal dimension

2516
02:31:03.355 --> 02:31:07.355
of the inputs, like, you know, uh, at the input at times,

2517
02:31:07.385 --> 02:31:10.875
step T is dependent on all the inputs till

2518
02:31:10.875 --> 02:31:12.075
times step T minus one.

2519
02:31:12.375 --> 02:31:15.075
So there is a back propagation happening through,

2520
02:31:15.075 --> 02:31:19.075
through time as well, and we look at how that works and,

2521
02:31:19.935 --> 02:31:21.755
and, uh, so back propagation

2522
02:31:21.755 --> 02:31:24.635
through time essentially computes gradients through time,

2523
02:31:25.335 --> 02:31:26.955
uh, enabling the network to learn

2524
02:31:26.955 --> 02:31:28.835
and update the weights, uh,

2525
02:31:28.835 --> 02:31:31.315
based on the signal propagated through the sequence.

2526
02:31:32.915 --> 02:31:36.255
So that's the core, uh,

2527
02:31:37.325 --> 02:31:41.375
principle, uh, within RN that we want to explore further

2528
02:31:41.885 --> 02:31:44.615
what is, how we can create this hidden state

2529
02:31:45.315 --> 02:31:48.215
and how we can do the back propagation through time

2530
02:31:48.835 --> 02:31:52.735
to be able to learn across time to be able to, uh,

2531
02:31:53.175 --> 02:31:54.815
generate the next word in the sequence.

2532
02:32:03.475 --> 02:32:06.485
Cool. So let me see if I can,

2533
02:32:09.215 --> 02:32:12.735
so in a generic layer, like you have your input

2534
02:32:15.795 --> 02:32:19.365
and your, uh, output, right?

2535
02:32:19.955 --> 02:32:24.505
With RNN. So let's add a time dimension to it.

2536
02:32:24.505 --> 02:32:28.015
So input at time t uh,

2537
02:32:30.405 --> 02:32:35.145
output at time t there is a hidden state

2538
02:32:35.645 --> 02:32:37.585
coming from time T minus one,

2539
02:32:39.795 --> 02:32:42.455
and there is a new hidden state time T.

2540
02:32:42.915 --> 02:32:47.175
And now this becomes the input at time t plus one.

2541
02:32:48.075 --> 02:32:50.855
Um, uh, sorry, uh, there is another input

2542
02:32:50.995 --> 02:32:54.145
and there is another hidden state, uh, so

2543
02:32:56.985 --> 02:32:58.165
and so on and so forth.

2544
02:32:58.665 --> 02:33:00.485
So that, that, that's how RN works

2545
02:33:00.585 --> 02:33:04.205
by capturing this hidden state and constantly updating it.

2546
02:33:04.865 --> 02:33:06.605
So it's not only processing the input,

2547
02:33:06.985 --> 02:33:09.845
but also hidden state at any given point of time

2548
02:33:10.305 --> 02:33:11.405
to produce the outputs.

2549
02:33:19.355 --> 02:33:24.215
So, um, process sequential data by maintaining

2550
02:33:24.235 --> 02:33:26.335
and utilizing this, uh,

2551
02:33:26.335 --> 02:33:28.255
information from previous times steps.

2552
02:33:29.435 --> 02:33:31.895
Um, they achieve this by introducing these

2553
02:33:32.405 --> 02:33:35.615
connections called recurrent connections, uh,

2554
02:33:35.665 --> 02:33:37.615
which allow the network to retain

2555
02:33:37.615 --> 02:33:41.135
and update these hidden states, uh, which we looked at.

2556
02:33:41.955 --> 02:33:46.695
Uh, so the main principle is one is sequential processing,

2557
02:33:46.755 --> 02:33:48.895
so you're processing it one step at a time.

2558
02:33:49.675 --> 02:33:52.535
So RNs are designed to handle sequential data

2559
02:33:52.665 --> 02:33:55.175
where the order of elements matter.

2560
02:33:55.875 --> 02:33:58.215
Uh, this can be like, you know, time series data

2561
02:33:59.075 --> 02:34:01.375
or sentences in natural language

2562
02:34:02.075 --> 02:34:03.815
or any other sequence of data points.

2563
02:34:03.835 --> 02:34:08.135
The order matters. So the network process input one by one,

2564
02:34:08.395 --> 02:34:10.415
taking into account the previous information.

2565
02:34:11.925 --> 02:34:14.425
And then there is a step called hidden state update.

2566
02:34:14.725 --> 02:34:18.005
So each time step an RN takes an input vector,

2567
02:34:18.875 --> 02:34:21.815
the previous hidden state as input,

2568
02:34:22.075 --> 02:34:24.535
and it computes a new hidden state, uh,

2569
02:34:24.605 --> 02:34:25.895
just like what we looked here.

2570
02:34:26.155 --> 02:34:27.855
So it takes input at times.

2571
02:34:27.855 --> 02:34:30.735
Step T uh, a hidden state at times, step

2572
02:34:31.505 --> 02:34:33.035
from times step T minus one,

2573
02:34:33.615 --> 02:34:37.075
and produces a new hidden state as well as an output.

2574
02:34:39.695 --> 02:34:44.075
So the information flow, uh, is in the RNN is through time.

2575
02:34:44.655 --> 02:34:48.115
Um, and so the hidden state at each time step serves

2576
02:34:48.135 --> 02:34:52.315
as both an output and an input for the next time step.

2577
02:34:52.455 --> 02:34:54.475
So, so this becomes an input

2578
02:34:54.475 --> 02:34:58.375
to the next time step weight sharing.

2579
02:34:59.035 --> 02:35:01.255
So how do RNAs become more efficient?

2580
02:35:01.355 --> 02:35:04.455
So the, the weights are shared through time.

2581
02:35:04.635 --> 02:35:07.215
So what are the network weights that are being learned

2582
02:35:07.755 --> 02:35:09.535
are not unique for each time step?

2583
02:35:09.835 --> 02:35:12.255
So the weights are the same across the time steps.

2584
02:35:12.835 --> 02:35:16.095
So, so what are the weights that the network is learning?

2585
02:35:16.795 --> 02:35:21.145
Uh, so if you're passing, modeling word

2586
02:35:23.095 --> 02:35:26.645
probabilities is difficult.

2587
02:35:27.495 --> 02:35:30.395
So these are 1, 2, 3, 4, 5, 5 times steps.

2588
02:35:30.975 --> 02:35:32.555
So through these five times steps,

2589
02:35:33.855 --> 02:35:36.395
the RNN is learning one set of weights.

2590
02:35:36.655 --> 02:35:39.515
So the net weights are shaded across the time.

2591
02:35:39.895 --> 02:35:42.635
So that's where the efficiency of r and m comes.

2592
02:35:43.585 --> 02:35:45.555
This weight sharing enables the network to learn

2593
02:35:45.555 --> 02:35:46.635
and capture patterns

2594
02:35:47.295 --> 02:35:49.915
and dependencies that occur across the sequential data.

2595
02:35:51.435 --> 02:35:52.895
And then the technique used

2596
02:35:52.915 --> 02:35:55.615
to train is called back propagation through time

2597
02:35:58.295 --> 02:36:01.745
because the gradients are back propagated through the time,

2598
02:36:02.885 --> 02:36:03.905
uh, time steps.

2599
02:36:16.885 --> 02:36:19.745
So, uh, let's put this in in equations.

2600
02:36:19.855 --> 02:36:23.185
Like, um, so if you roll this RNN network,

2601
02:36:23.215 --> 02:36:25.425
like the same thing I showed you earlier,

2602
02:36:26.215 --> 02:36:30.315
so if you have your input time, T HT minus one

2603
02:36:31.925 --> 02:36:36.065
HT ot, you roll this out, this is how it looks like,

2604
02:36:36.065 --> 02:36:38.705
you know, H zero, H one, H two, uh,

2605
02:36:38.805 --> 02:36:42.945
and then you're combining H zero with some weights

2606
02:36:43.845 --> 02:36:46.535
and with some inputs

2607
02:36:47.475 --> 02:36:50.535
and, uh, putting some activation on top of it

2608
02:36:51.275 --> 02:36:53.975
to create the next hidden state, H one

2609
02:36:54.435 --> 02:36:56.695
and also an output state.

2610
02:36:57.395 --> 02:36:59.255
Uh, why had one?

2611
02:37:00.075 --> 02:37:04.655
So, so why is the output here? Um, h is the hidden state.

2612
02:37:05.075 --> 02:37:06.495
Uh, X is the input.

2613
02:37:07.195 --> 02:37:11.045
So essentially your, uh, probability of,

2614
02:37:11.945 --> 02:37:16.685
of y uh, at, at a given timeframe is doing a soft max

2615
02:37:16.685 --> 02:37:19.485
between y weights and hidden state.

2616
02:37:20.305 --> 02:37:23.565
Um, and hidden state is computed by an activation

2617
02:37:23.565 --> 02:37:27.285
of tanh over, um, you know, hidden state weights,

2618
02:37:28.005 --> 02:37:32.725
previous hidden state input weights and current input.

2619
02:37:33.825 --> 02:37:37.205
Um, and essentially for each time step,

2620
02:37:37.345 --> 02:37:41.565
the loss is calculated by cross entropy laws.

2621
02:37:42.225 --> 02:37:45.125
So, so each word has a probability, right?

2622
02:37:45.225 --> 02:37:47.765
So, so it's measuring the loss between

2623
02:37:48.295 --> 02:37:52.285
those two different classes by using the cross entropy.

2624
02:37:53.185 --> 02:37:56.565
And that loss is calculated of after this is predicted,

2625
02:37:56.785 --> 02:37:58.725
and it is back propagated through time.

2626
02:37:59.705 --> 02:38:04.645
So that's, uh, how, you know, you can represent A RNN using

2627
02:38:05.235 --> 02:38:08.005
math, uh, and different, this is all plain vanilla,

2628
02:38:08.085 --> 02:38:10.045
RNN We'll look at different varieties,

2629
02:38:10.545 --> 02:38:13.165
but, uh, this is how the inputs

2630
02:38:13.165 --> 02:38:15.285
and hidden states are calculated, uh,

2631
02:38:15.285 --> 02:38:17.245
across the layers of the RNN.

2632
02:38:18.935 --> 02:38:21.355
So, let's look at it a bit more deeper.

2633
02:38:21.855 --> 02:38:25.235
Um, uh, there was a slight convention change on this one.

2634
02:38:25.335 --> 02:38:29.715
So here, um, you have inputs represented

2635
02:38:29.715 --> 02:38:31.555
with X, um,

2636
02:38:33.415 --> 02:38:37.975
and, uh, they have using a weight of u um,

2637
02:38:38.235 --> 02:38:42.315
hidden states are using a weight of w uh, outputs

2638
02:38:42.875 --> 02:38:46.115
represented as o uh, using a weight of V.

2639
02:38:47.385 --> 02:38:51.565
Um, the hidden state, I believe is represented with s uh,

2640
02:38:51.745 --> 02:38:56.105
so, um, so, so, yeah, uh, let's see how,

2641
02:38:56.285 --> 02:38:58.545
how we can, um, you know,

2642
02:38:58.765 --> 02:39:01.105
see this back propagation in practice.

2643
02:39:02.195 --> 02:39:03.135
Um, so,

2644
02:39:07.505 --> 02:39:10.965
so just to clarify, w here is right of

2645
02:39:11.735 --> 02:39:16.695
hidden state, um, u here as weights

2646
02:39:16.695 --> 02:39:19.415
of input XV is,

2647
02:39:20.795 --> 02:39:22.535
uh, weights of y.

2648
02:39:31.375 --> 02:39:34.735
Cool. So with that, we can come up

2649
02:39:34.735 --> 02:39:36.495
with these basic ations, right?

2650
02:39:36.495 --> 02:39:40.025
So HD is, you know,

2651
02:39:40.245 --> 02:39:41.625
you take the input weight

2652
02:39:43.555 --> 02:39:48.305
and, um, multiply with the input at a given times step T,

2653
02:39:48.845 --> 02:39:50.225
and take the hidden state weight

2654
02:39:50.445 --> 02:39:52.985
and multiply with the hidden state from

2655
02:39:52.985 --> 02:39:54.225
times step T minus one.

2656
02:39:54.725 --> 02:39:57.265
Uh, for simplicity sake, I'm not putting the activation

2657
02:39:57.265 --> 02:39:58.945
around it, but there is an activation

2658
02:39:58.945 --> 02:40:00.345
here like we've seen here.

2659
02:40:01.055 --> 02:40:04.075
You know, uh, so there is an activation that happens here,

2660
02:40:05.945 --> 02:40:09.545
um, and output at given times.

2661
02:40:09.865 --> 02:40:11.305
T is always, you know, you

2662
02:40:12.045 --> 02:40:15.215
use the output weight times the hidden state times.

2663
02:40:15.215 --> 02:40:18.145
Step two, again, there is, in practice,

2664
02:40:18.145 --> 02:40:19.305
there is an activation around it,

2665
02:40:19.305 --> 02:40:20.665
which we're not using here.

2666
02:40:21.885 --> 02:40:24.745
And loss is defined as overall loss

2667
02:40:25.255 --> 02:40:26.425
over all the times, steps.

2668
02:40:27.375 --> 02:40:30.795
Uh, we are comparing the output to the prediction

2669
02:40:31.745 --> 02:40:32.935
using a loss function.

2670
02:40:33.075 --> 02:40:34.975
In this case, it could be a soft max loss.

2671
02:40:36.725 --> 02:40:38.545
Um, so it's, um,

2672
02:40:39.845 --> 02:40:42.585
the loss is combined across summited across times,

2673
02:40:42.585 --> 02:40:45.305
steps t equal to 1, 2, 3, and times.

2674
02:40:45.305 --> 02:40:46.425
Steps are the number of

2675
02:40:50.725 --> 02:40:52.825
s What is this?

2676
02:40:53.005 --> 02:40:57.245
Uh, oh, here, uh, that is the hidden state.

2677
02:40:58.775 --> 02:41:02.145
Instead of h uh, there is a change in conversion here. So,

2678
02:41:09.015 --> 02:41:09.305
cool.

2679
02:41:09.725 --> 02:41:14.705
So let's see how we can derive the various, um, uh, loss,

2680
02:41:14.885 --> 02:41:18.505
um, derivatives, uh, for back backdrop getting.

2681
02:41:18.645 --> 02:41:20.385
So we need three derivatives, right?

2682
02:41:20.445 --> 02:41:24.845
So we need, um, so u derivative with respect

2683
02:41:24.845 --> 02:41:27.005
to UVW.

2684
02:41:27.185 --> 02:41:30.005
So U is your input weights.

2685
02:41:30.745 --> 02:41:33.965
We use output rates, and these are hidden state weights.

2686
02:41:34.505 --> 02:41:39.125
So we need wwl by W double now

2687
02:41:39.625 --> 02:41:41.605
by Dow V.

2688
02:41:42.585 --> 02:41:46.365
And now by now, W So if we have all these things,

2689
02:41:46.905 --> 02:41:50.405
all these three things, we can now, uh, use those gradients

2690
02:41:50.405 --> 02:41:52.725
to back, back, propagate the loss, uh,

2691
02:41:52.865 --> 02:41:54.765
and, uh, compute the gradients.

2692
02:41:55.185 --> 02:41:58.125
So, so we can, uh, do the back propagation through time.

2693
02:41:59.185 --> 02:42:00.565
So let's see how we can get to that.

2694
02:42:01.625 --> 02:42:06.295
So, uh, first thing is, uh, we'll try to compute

2695
02:42:06.565 --> 02:42:11.385
what the, um, with respect

2696
02:42:11.385 --> 02:42:12.385
to output rate is.

2697
02:42:13.085 --> 02:42:17.825
So, um, so we know the loss is, you know,

2698
02:42:18.005 --> 02:42:21.065
is computed as loss of, uh, you know, across all the time.

2699
02:42:21.065 --> 02:42:22.265
Stepss, ot, YT.

2700
02:42:22.925 --> 02:42:26.545
Um, so doel by DAO is essentially,

2701
02:42:26.845 --> 02:42:28.265
um, using chain rule.

2702
02:42:28.765 --> 02:42:32.705
We can represent it as doel by DA ot, uh, with

2703
02:42:33.505 --> 02:42:35.745
multiplied by da OT divide.

2704
02:42:35.965 --> 02:42:39.065
Uh, with respect to, with respect to do

2705
02:42:40.995 --> 02:42:44.415
so, uh, we can expand this, this, each,

2706
02:42:44.445 --> 02:42:45.535
each of these functions.

2707
02:42:46.235 --> 02:42:48.345
So, let's see.

2708
02:42:49.025 --> 02:42:51.555
I want to use a white page.

2709
02:42:52.825 --> 02:42:55.545
Okay, so we have do,

2710
02:43:06.425 --> 02:43:08.765
so you take this, this part.

2711
02:43:09.665 --> 02:43:14.565
So we already know that OT is, um,

2712
02:43:16.205 --> 02:43:20.295
V times hd, um, so

2713
02:43:22.725 --> 02:43:27.705
now, uh, OT by the V that reduce S two hd.

2714
02:43:29.775 --> 02:43:34.375
So that part is hd.

2715
02:43:34.475 --> 02:43:37.705
So now, uh, expanding the loss.

2716
02:43:38.205 --> 02:43:43.055
So we know that loss is, you know, one

2717
02:43:43.075 --> 02:43:47.295
by T Sigma, uh, loss of ot,

2718
02:43:48.505 --> 02:43:50.845
um, yt.

2719
02:43:52.495 --> 02:43:54.555
So you expand that across time,

2720
02:43:54.855 --> 02:43:56.755
but you're only derating with respect

2721
02:43:56.755 --> 02:43:58.475
to one particular time timeframe.

2722
02:43:59.255 --> 02:44:03.275
So that reduces it to double loss.

2723
02:44:04.755 --> 02:44:07.765
ot, yt divided

2724
02:44:07.985 --> 02:44:09.845
by ot.

2725
02:44:11.625 --> 02:44:15.235
So, so dwell by D

2726
02:44:16.095 --> 02:44:18.915
can be reduced to these two factors,

2727
02:44:19.305 --> 02:44:21.875
because essentially if you're derating with respect

2728
02:44:21.875 --> 02:44:24.675
to OT at point of time, everything else becomes zero.

2729
02:44:25.135 --> 02:44:27.995
So this summation reduces to just one point of time,

2730
02:44:28.935 --> 02:44:32.515
and this, uh, reduces to, uh, ht.

2731
02:44:35.125 --> 02:44:39.825
So let's see. Uh, so yeah, so as you can see, uh,

2732
02:44:40.255 --> 02:44:44.865
that reduces to one particular loss function, uh,

2733
02:44:44.925 --> 02:44:48.495
at a given point of time with HD pose.

2734
02:44:49.355 --> 02:44:54.335
And if you want to compute the overall double by, uh,

2735
02:44:54.475 --> 02:44:57.575
we are, uh, we'll suming this across all the time

2736
02:44:57.575 --> 02:44:58.655
steps T two T.

2737
02:45:00.275 --> 02:45:04.135
Now, let's look at, uh, the next, um,

2738
02:45:04.895 --> 02:45:07.935
der uh, for, uh, UNW.

2739
02:45:08.235 --> 02:45:11.775
So we already know U is, uh, the waits for input,

2740
02:45:12.845 --> 02:45:14.945
and w is wait for hidden state.

2741
02:45:17.445 --> 02:45:20.585
So let's look at, um,

2742
02:45:23.355 --> 02:45:27.505
how to get WT plus one

2743
02:45:29.605 --> 02:45:32.375
respect to the W for that.

2744
02:45:32.655 --> 02:45:34.295
Actually, um, I want to

2745
02:45:36.105 --> 02:45:38.175
think this from a different standpoint.

2746
02:45:38.235 --> 02:45:41.725
So think of, you know, how to get to loss.

2747
02:45:42.465 --> 02:45:44.965
So we have weight at times, step T,

2748
02:45:45.665 --> 02:45:49.725
and Hidden State at times Step T, that produces our output,

2749
02:45:49.935 --> 02:45:54.275
right output at times Step T, sorry, um,

2750
02:45:56.475 --> 02:45:58.975
uh, that produces our hidden state

2751
02:46:02.655 --> 02:46:04.875
at times step T minus T plus one.

2752
02:46:06.925 --> 02:46:11.525
And we use that hidden state at T one to produce the output

2753
02:46:12.265 --> 02:46:13.925
at t plus one,

2754
02:46:13.925 --> 02:46:18.125
because output is essentially, you know, V times, uh,

2755
02:46:18.565 --> 02:46:19.605
HT plus one, right?

2756
02:46:20.105 --> 02:46:21.445
So, uh,

2757
02:46:21.545 --> 02:46:24.005
we produce the output based on hidden state at t plus one,

2758
02:46:24.145 --> 02:46:27.745
and based on output at t plus one, uh,

2759
02:46:28.245 --> 02:46:29.985
we produce a loss,

2760
02:46:30.055 --> 02:46:33.465
because once you have the output, you can compare the output

2761
02:46:33.645 --> 02:46:37.265
to the actual Y value and you can compute the loss.

2762
02:46:37.725 --> 02:46:41.925
So, so we have this information flow from, uh,

2763
02:46:41.925 --> 02:46:43.365
hidden State Times Step T

2764
02:46:43.545 --> 02:46:45.765
to Hidden State Times step T plus one

2765
02:46:45.945 --> 02:46:47.965
to output at t plus one to loss.

2766
02:46:48.665 --> 02:46:51.005
Now, back propagate this loss.

2767
02:46:51.105 --> 02:46:55.325
So, so we can write this as this is loss at T plus one,

2768
02:46:55.905 --> 02:46:59.965
so loss at t plus one with respect to Dow W

2769
02:47:00.505 --> 02:47:04.485
can written as, let me erase some, some of this stuff.

2770
02:47:14.985 --> 02:47:16.365
So if you back propagate

2771
02:47:17.775 --> 02:47:22.645
and write this, that can be expressed as, you know,

2772
02:47:22.865 --> 02:47:25.125
you are doing the loss of

2773
02:47:26.775 --> 02:47:31.505
time step T plus one with respect to output or T plus one

2774
02:47:32.765 --> 02:47:35.105
and chain rule, I'm using chain rule,

2775
02:47:35.325 --> 02:47:38.705
and you are aerating output of T plus one with respect

2776
02:47:38.705 --> 02:47:40.105
to hidden state T plus one,

2777
02:47:41.625 --> 02:47:45.045
and then hidden state T plus one with respect to

2778
02:47:46.925 --> 02:47:48.145
hidden state T,

2779
02:47:48.925 --> 02:47:53.805
and then hidden state T with respect to the weight

2780
02:47:54.825 --> 02:47:58.205
of, uh, then the hidden state.

2781
02:47:58.745 --> 02:48:02.085
So by applying chain rule backwards from the loss,

2782
02:48:02.625 --> 02:48:04.285
you can get to an equation like this.

2783
02:48:05.285 --> 02:48:08.825
Um, so let me use the, yeah,

2784
02:48:11.065 --> 02:48:12.445
you can get to any question like this

2785
02:48:13.145 --> 02:48:16.765
by applying chain rule backwards from the loss, uh, to get

2786
02:48:16.765 --> 02:48:20.245
to, uh, the of loss with respect

2787
02:48:20.245 --> 02:48:21.925
to hidden state weights.

2788
02:48:23.685 --> 02:48:28.625
Um, essentially we'll apply the same thing

2789
02:48:29.045 --> 02:48:33.945
for, uh, the you as well, you as the input weight,

2790
02:48:34.685 --> 02:48:38.945
and we'll get to a similar, uh, equation

2791
02:48:39.685 --> 02:48:43.505
per, uh, um, the DER rate of loss with respect

2792
02:48:43.505 --> 02:48:45.145
to, uh, input rates.

2793
02:48:46.775 --> 02:48:49.355
Uh, so this is how we can, uh, derivate

2794
02:48:49.735 --> 02:48:53.195
and compute the DER rate of losses with respect to each

2795
02:48:53.195 --> 02:48:56.675
of the weights by back propagating through time.

2796
02:48:58.925 --> 02:48:59.045
I,

2797
02:49:08.015 --> 02:49:09.115
uh, one quick note.

2798
02:49:09.295 --> 02:49:12.795
Um, again, these equations are, you know, derived

2799
02:49:12.855 --> 02:49:17.715
to understand how BPTT works in airplane vanilla, RNN,

2800
02:49:18.495 --> 02:49:22.515
um, haven't seen this in my experience, like as these,

2801
02:49:22.515 --> 02:49:25.995
these kind of ations asked in like interest, this is more

2802
02:49:26.055 --> 02:49:29.555
for your understanding, uh, so that, you know, if you want

2803
02:49:29.555 --> 02:49:31.835
to think through, like from your mind, like, you know

2804
02:49:31.855 --> 02:49:35.435
how the data rate is propagate backwards by, you know,

2805
02:49:35.435 --> 02:49:38.635
looking at, you know, uh, something like this, like,

2806
02:49:38.635 --> 02:49:40.355
you know how the loss is calculated

2807
02:49:40.455 --> 02:49:44.155
and then it's back propagated, uh, using chain rule.

2808
02:49:44.455 --> 02:49:48.115
Um, it, it makes, makes sense for you for the,

2809
02:49:48.115 --> 02:49:49.355
those kind of understandings.

2810
02:49:52.765 --> 02:49:54.025
Um, cool.

2811
02:49:54.485 --> 02:49:58.805
So, um, now we look at like, you know, some

2812
02:49:58.805 --> 02:50:00.845
of the problems that would create, like, you know,

2813
02:50:00.845 --> 02:50:03.285
we are using the same weights across the times steps, right?

2814
02:50:03.705 --> 02:50:06.605
So we are using hidden same weights, WH

2815
02:50:07.305 --> 02:50:10.365
and multiplying that the same weight.

2816
02:50:11.035 --> 02:50:14.965
Were at times step zero, we're multiplying with WH to get

2817
02:50:14.965 --> 02:50:16.925
to H one and times step one.

2818
02:50:17.415 --> 02:50:20.405
We're multiplying again, the same weight, uh, to get

2819
02:50:20.405 --> 02:50:22.845
to H two and so on till we get to ht.

2820
02:50:24.085 --> 02:50:28.425
So if I take a number, uh, let's say X, um,

2821
02:50:28.985 --> 02:50:32.805
X can be anything, and let's say I'm multiplying

2822
02:50:32.805 --> 02:50:35.245
with 0.1, uh,

2823
02:50:35.955 --> 02:50:38.565
what would this number be in related to X?

2824
02:50:39.065 --> 02:50:40.165
Is it smaller or larger?

2825
02:50:47.245 --> 02:50:49.015
It's, uh, smaller, right?

2826
02:50:49.195 --> 02:50:52.055
So, so, and then if you keep on doing the same thing

2827
02:50:52.455 --> 02:50:54.615
multiple times, so if you take this

2828
02:50:57.135 --> 02:51:01.315
and multiply again with 0.1, um, so if you,

2829
02:51:01.695 --> 02:51:02.795
and it may not be 0.1,

2830
02:51:02.795 --> 02:51:04.835
it can be any number less than one, right?

2831
02:51:04.975 --> 02:51:06.275
So, and keep on multiplying that.

2832
02:51:06.335 --> 02:51:07.995
So this becomes super small.

2833
02:51:08.055 --> 02:51:10.035
So as you propagate through time,

2834
02:51:10.455 --> 02:51:12.235
if the weights are less than one,

2835
02:51:12.935 --> 02:51:15.195
it can result in a vanishing gradient.

2836
02:51:15.735 --> 02:51:18.755
So, so weights can become super small.

2837
02:51:19.735 --> 02:51:21.875
And let's say if this number is greater than one,

2838
02:51:21.885 --> 02:51:23.235
let's say it's 1.5,

2839
02:51:23.575 --> 02:51:24.995
and if you keep on multiplying,

2840
02:51:24.995 --> 02:51:26.555
it becomes exponentially larger.

2841
02:51:26.895 --> 02:51:31.515
So that's the, um, exploding gradient problem that we see

2842
02:51:32.025 --> 02:51:34.515
with, uh, uh, with these RNs.

2843
02:51:34.775 --> 02:51:39.195
So those are the two, um, uh, uh, limitations

2844
02:51:39.295 --> 02:51:43.555
or kind of disadvantages of plain RNs, um, that happen

2845
02:51:43.555 --> 02:51:46.355
because of the way the back propagation

2846
02:51:46.355 --> 02:51:49.755
through time happens using the same weight again and again.

2847
02:51:50.375 --> 02:51:53.155
And we'll see how, how to mitigate that, uh,

2848
02:51:53.175 --> 02:51:54.275
in the later sections.

2849
02:52:04.175 --> 02:52:08.475
Um, again, this is again, uh, uh,

2850
02:52:09.055 --> 02:52:12.075
you can do this through code to see, to visualize how the,

2851
02:52:12.735 --> 02:52:14.315
uh, vanishing gradients happen.

2852
02:52:14.775 --> 02:52:17.475
Um, um, so, uh,

2853
02:52:17.475 --> 02:52:20.075
you can essentially replicate this in code and see how that works.

2854
02:52:25.435 --> 02:52:29.155
Cool. So, um, now

2855
02:52:29.155 --> 02:52:32.795
that we know why the vanishing gradients happen, um,

2856
02:52:33.215 --> 02:52:37.985
so let's see, uh, you know, um, uh, learn a little bit more.

2857
02:52:38.205 --> 02:52:42.305
So one, they occur when the gradients diminish exponentially

2858
02:52:42.685 --> 02:52:44.385
or explode exponentially.

2859
02:52:45.005 --> 02:52:47.825
Uh, when you are multiplying it iteratively

2860
02:52:47.825 --> 02:52:49.585
across multiple types steps.

2861
02:52:50.405 --> 02:52:53.385
So when you look at long sequences, like, you know,

2862
02:52:53.565 --> 02:52:56.785
we looked at a 10, a six letter, a six word sentence,

2863
02:52:56.965 --> 02:52:59.625
but let's say a sentence has a hundred words.

2864
02:53:00.125 --> 02:53:01.225
So that's a long sequence.

2865
02:53:01.225 --> 02:53:04.385
So when you do some operation a hundred times, so

2866
02:53:04.965 --> 02:53:08.185
on long sequences, this becomes a more bigger problem.

2867
02:53:08.925 --> 02:53:10.345
So those are some things

2868
02:53:10.375 --> 02:53:12.265
that are limitations of it, plain man.

2869
02:53:12.845 --> 02:53:15.505
So you can't handle long sequences inherently.

2870
02:53:16.725 --> 02:53:20.965
Um, so, uh, on the other hand,

2871
02:53:21.705 --> 02:53:25.045
you know, same thing exploiting gradients can also occur.

2872
02:53:25.075 --> 02:53:27.485
Like, you know, if your weights are, you know,

2873
02:53:27.515 --> 02:53:29.885
greater than one and you are using long,

2874
02:53:30.185 --> 02:53:33.245
and you have long sequences as your inputs, uh,

2875
02:53:33.305 --> 02:53:34.525
it can cause instability

2876
02:53:35.225 --> 02:53:37.805
and, you know, prevent the network from converting

2877
02:53:37.805 --> 02:53:39.285
because the gradients become one

2878
02:53:39.285 --> 02:53:40.565
of the gradients become too big.

2879
02:53:41.265 --> 02:53:45.525
Um, so RNs due to that are, you know,

2880
02:53:45.585 --> 02:53:49.045
are not great at, you know, handling, uh, long sequences.

2881
02:53:54.835 --> 02:53:59.515
So there comes the other types of RNs,

2882
02:53:59.515 --> 02:54:02.955
like L SDMs, uh, which stand for long short-term networks,

2883
02:54:03.775 --> 02:54:06.355
or G gated recurrent units

2884
02:54:07.815 --> 02:54:10.075
and bi direct bidirectional RNs.

2885
02:54:10.415 --> 02:54:14.755
Uh, these are different varieties of RNs, um, uh,

2886
02:54:14.885 --> 02:54:18.925
which kind of, instead of, uh, looking at, um,

2887
02:54:20.585 --> 02:54:24.665
so for r and n, we are looking

2888
02:54:25.485 --> 02:54:26.705
in one direction, right?

2889
02:54:26.895 --> 02:54:31.585
More word ties is hard.

2890
02:54:33.345 --> 02:54:38.195
Instead, bidirectional RNs, uh, go through this direction

2891
02:54:38.415 --> 02:54:43.115
as well as this direction with, uh, in one pass, the

2892
02:54:44.425 --> 02:54:48.275
time steps would be T zero, T one, T two, T three like this.

2893
02:54:48.775 --> 02:54:51.515
And in the other path, the times steps would be,

2894
02:54:51.625 --> 02:54:53.235
this would be times step zero.

2895
02:54:54.035 --> 02:54:57.125
This would be 1, 2, 3, and so on.

2896
02:54:57.505 --> 02:55:00.445
So bidirectional, learn it in two directions as well,

2897
02:55:00.865 --> 02:55:04.225
to capture the semantics better, uh,

2898
02:55:04.295 --> 02:55:06.225
hierarchy and attention based.

2899
02:55:06.775 --> 02:55:09.705
This is where we are getting into, like, you know, uh,

2900
02:55:09.705 --> 02:55:12.145
these are the motivation for transformer architectures.

2901
02:55:12.565 --> 02:55:14.945
So attention was actually used in ENS

2902
02:55:14.945 --> 02:55:17.545
before it was, uh, you know, used in transformers.

2903
02:55:18.405 --> 02:55:21.905
Um, and, um, we'll talk about L SDMs

2904
02:55:21.905 --> 02:55:25.025
and gr, uh, which kind of mitigate the limitations

2905
02:55:25.045 --> 02:55:28.225
of exploding and vanishing gradients to a larger extent.

2906
02:55:28.885 --> 02:55:31.745
Um, uh, so we'll talk about that in detail.

2907
02:55:32.365 --> 02:55:35.865
Uh, we just looked at, um, right now.

2908
02:55:49.775 --> 02:55:53.785
Cool. So, uh, let's look at some of the salient features

2909
02:55:53.805 --> 02:55:56.025
and advantages of ENS in general.

2910
02:55:56.445 --> 02:55:59.385
So again, ENS are suited for,

2911
02:55:59.385 --> 02:56:01.305
because of the way the back

2912
02:56:01.305 --> 02:56:02.665
propagation through time happens.

2913
02:56:03.335 --> 02:56:05.505
They're specifically designed for sequential data.

2914
02:56:06.085 --> 02:56:09.425
Uh, this makes them well suited for tasks where order, uh,

2915
02:56:09.425 --> 02:56:13.145
and, you know, temporal dependencies of data are important.

2916
02:56:13.925 --> 02:56:16.825
Um, there is a context preservation happening.

2917
02:56:17.285 --> 02:56:20.025
So they have the ability to maintain internal hidden state,

2918
02:56:20.025 --> 02:56:23.185
like, you know, through updating a hidden state through time

2919
02:56:23.725 --> 02:56:26.025
hd, uh, they're able to maintain the state

2920
02:56:26.025 --> 02:56:28.665
of all the previous states somehow.

2921
02:56:29.785 --> 02:56:31.605
Uh, this allows the network to retain

2922
02:56:31.605 --> 02:56:33.805
and utilize information from previous time steps.

2923
02:56:34.545 --> 02:56:38.095
Um, when processing the current time step, um, they,

2924
02:56:38.125 --> 02:56:39.855
they can handle variable inputs.

2925
02:56:40.395 --> 02:56:43.455
So, so that's one of the flexibility of r and m.

2926
02:56:43.455 --> 02:56:46.615
So you don't have to preset the input size.

2927
02:56:47.355 --> 02:56:50.615
Uh, it can handle, you know, multiple sent, uh,

2928
02:56:50.615 --> 02:56:54.015
words in a sentence, um, easily, uh,

2929
02:56:54.435 --> 02:56:57.375
and sentence sentences can have different lens.

2930
02:56:58.075 --> 02:57:00.735
Uh, so RNs can adaptively processing inputs

2931
02:57:00.735 --> 02:57:02.415
of different lens by adjusting their

2932
02:57:02.415 --> 02:57:03.575
hidden states accordingly.

2933
02:57:05.765 --> 02:57:09.145
Um, parameter sharing, uh, as we talked,

2934
02:57:09.285 --> 02:57:11.065
the hidden states are shared.

2935
02:57:11.285 --> 02:57:13.585
Uh, the weights are shared across time steps.

2936
02:57:14.365 --> 02:57:16.625
So that's how the parameter sharing happens.

2937
02:57:16.845 --> 02:57:19.505
So you're not creating one weight per time step,

2938
02:57:19.965 --> 02:57:22.625
but rather the same weight is used across the time steps.

2939
02:57:23.375 --> 02:57:27.345
This parameter sharing makes R ns computationally efficient,

2940
02:57:28.125 --> 02:57:30.825
uh, and enables them to generalize better.

2941
02:57:32.345 --> 02:57:36.205
Uh, time series analysis, uh, again, this is one area.

2942
02:57:36.365 --> 02:57:38.525
R Ns can excel better, uh, like, uh,

2943
02:57:38.535 --> 02:57:40.005
since time series analysis.

2944
02:57:40.115 --> 02:57:42.925
Also temporal and sequential in nature, uh,

2945
02:57:42.925 --> 02:57:45.725
like stock market production, whether forecasting,

2946
02:57:46.025 --> 02:57:49.685
signal processing, and as such, uh, language modeling

2947
02:57:50.065 --> 02:57:52.845
and generation is also another, uh, use case

2948
02:57:52.845 --> 02:57:54.245
where our RNN is excel.

2949
02:57:54.745 --> 02:57:57.525
Um, they can, you know, capture dependencies between words

2950
02:57:57.525 --> 02:57:59.805
because language can be looked at as a sequence, right?

2951
02:58:00.105 --> 02:58:04.285
And, um, uh, there is some temporal dependency there.

2952
02:58:04.825 --> 02:58:07.575
Uh, that R ns excel at, uh,

2953
02:58:07.765 --> 02:58:10.015
streaming data is another example.

2954
02:58:10.395 --> 02:58:12.495
RNs are suitable for online learning scenarios.

2955
02:58:13.115 --> 02:58:14.895
Um, they can process data in real time

2956
02:58:14.895 --> 02:58:16.655
as it becomes available, uh,

2957
02:58:16.655 --> 02:58:18.655
and maintaining that hidden state

2958
02:58:18.655 --> 02:58:20.255
and updating it incrementally.

2959
02:58:20.955 --> 02:58:23.895
Uh, so, uh, it is useful in those scenarios as well.

2960
02:58:28.275 --> 02:58:33.055
Cool. Uh, so, so as we looked at vanishing

2961
02:58:33.055 --> 02:58:34.455
and exploding gradients, so there,

2962
02:58:34.455 --> 02:58:35.735
there are some limitations, right?

2963
02:58:35.735 --> 02:58:39.095
So, uh, the RNs cannot process like very long sentences, so

2964
02:58:39.835 --> 02:58:42.615
the weights become super small or super big,

2965
02:58:43.115 --> 02:58:45.455
and the network struggles to learn, uh,

2966
02:58:45.485 --> 02:58:46.855
when the sentences are big.

2967
02:58:47.355 --> 02:58:50.135
So that's one inherent limitation of plain rrn,

2968
02:58:50.675 --> 02:58:52.895
and there is no explicit memory state.

2969
02:58:53.195 --> 02:58:55.095
Uh, everything is in the hidden state.

2970
02:58:55.755 --> 02:58:59.655
Um, so, um, it's hard for RNs to

2971
02:59:01.055 --> 02:59:02.255
maintain memory, uh,

2972
02:59:02.395 --> 02:59:05.495
or conditional memory across the states.

2973
02:59:06.515 --> 02:59:08.335
And this can limit their ability to capture

2974
02:59:09.075 --> 02:59:11.455
and utilize context that span a

2975
02:59:11.455 --> 02:59:12.535
large number of times, steps.

2976
02:59:13.635 --> 02:59:17.895
Uh, since RNs are sequential in nature, uh, it is, uh,

2977
02:59:18.195 --> 02:59:21.175
computationally, it restricts parallelization,

2978
02:59:21.445 --> 02:59:24.415
like we saw in CNN's where you have multiple filters

2979
02:59:24.765 --> 02:59:28.095
that can be paralyzed here, everything is sequential,

2980
02:59:28.195 --> 02:59:30.695
so it restricts parallelization in that sense.

2981
02:59:31.795 --> 02:59:34.965
Um, and, um,

2982
02:59:36.855 --> 02:59:40.365
while R Ns can handle variable length inputs,

2983
02:59:40.365 --> 02:59:42.725
they can face challenges when presented with like,

2984
02:59:42.795 --> 02:59:43.885
very long inputs.

2985
02:59:43.885 --> 02:59:45.965
So, so that's why, you know, it's still,

2986
02:59:45.965 --> 02:59:49.605
there is this difficulty in handling, uh, sequences

2987
02:59:49.605 --> 02:59:51.085
that are very long in length.

2988
02:59:51.905 --> 02:59:55.725
Um, um, we see gradient vanishing and exploding

2989
02:59:55.725 --> 02:59:57.485
before lack of paralyzation.

2990
02:59:57.735 --> 03:00:01.445
Again, that ties back to, you know, sequential, uh,

2991
03:00:01.445 --> 03:00:02.565
computation limitations.

2992
03:00:03.425 --> 03:00:05.205
Um, uh, this one is interesting.

2993
03:00:05.225 --> 03:00:06.845
So handling irregular times steps.

2994
03:00:06.865 --> 03:00:08.525
So RN inherently assume

2995
03:00:08.525 --> 03:00:11.365
that all times steps are equally spaced, uh, right?

2996
03:00:11.425 --> 03:00:14.485
So, but let's say you are getting data that is, you know,

2997
03:00:14.955 --> 03:00:18.725
this is times step T, this is times step T plus three,

2998
03:00:19.515 --> 03:00:22.295
and this is T plus, you know, nine.

2999
03:00:22.875 --> 03:00:25.135
Uh, so if you're getting irregular times, step data

3000
03:00:26.015 --> 03:00:28.855
s don't inherently handle, they assume that, you know, all

3001
03:00:28.855 --> 03:00:30.135
of these are equally spaced.

3002
03:00:30.835 --> 03:00:34.015
Uh, so you need to have like some sort of pre-processing

3003
03:00:34.405 --> 03:00:36.575
outside of RN to be able to handle that.

3004
03:00:42.455 --> 03:00:47.085
So, let's see, some, you know, uh,

3005
03:00:47.235 --> 03:00:49.695
real life areas.

3006
03:00:50.155 --> 03:00:53.055
So, um, let's see this slide before.

3007
03:00:53.075 --> 03:00:53.735
So let,

3008
03:00:58.195 --> 03:00:59.775
there is some animations here.

3009
03:01:11.275 --> 03:01:15.015
So, so, yeah. So essentially, I mean, um,

3010
03:01:15.875 --> 03:01:19.925
ordinance are used for, uh, you know, in terms

3011
03:01:19.925 --> 03:01:23.125
of natural language processing, widely used in NLP tasks,

3012
03:01:23.755 --> 03:01:26.745
like, you know, language modeling, um,

3013
03:01:27.885 --> 03:01:30.385
and, um, sentiment analysis.

3014
03:01:30.965 --> 03:01:34.265
Uh, and as such, uh, they're used in time series analysis

3015
03:01:34.265 --> 03:01:37.905
for stock market prediction, uh, speech and audio crossing.

3016
03:01:38.285 --> 03:01:42.665
Uh, like Alex advisors, uh, RNs are implemented, uh, image

3017
03:01:42.665 --> 03:01:44.905
and video captioning, again, sequence of frames where,

3018
03:01:45.165 --> 03:01:48.705
you know, um, you take an image and take each frame

3019
03:01:49.325 --> 03:01:50.865
and, uh, uh,

3020
03:01:51.325 --> 03:01:54.945
and, uh, create, uh, captioning, uh, based on, you know,

3021
03:01:54.945 --> 03:01:56.185
what's happening in those frames.

3022
03:01:56.185 --> 03:01:59.505
So imagine video captioning is one other important aspect,

3023
03:01:59.565 --> 03:02:01.385
uh, where is used.

3024
03:02:02.365 --> 03:02:06.145
Um, so yeah, uh, so these are different real,

3025
03:02:06.175 --> 03:02:09.545
real life use cases where an applications where RNs get used

3026
03:02:10.185 --> 03:02:13.305
anywhere, you see sequential data, uh, especially like,

3027
03:02:13.305 --> 03:02:14.505
you know, uh, text

3028
03:02:15.085 --> 03:02:18.785
or signals that are spanned across temporal dimension

3029
03:02:19.525 --> 03:02:22.385
RNs are, are, are practical implementation there.

3030
03:02:26.905 --> 03:02:30.565
So, um, we'll go through a working example of RN dataset,

3031
03:02:30.625 --> 03:02:33.245
but I want to quickly go through, uh,

3032
03:02:33.245 --> 03:02:35.325
the remaining theoretical slides.

3033
03:02:35.985 --> 03:02:38.845
So, uh, since, uh, I want to cover the theory first,

3034
03:02:38.865 --> 03:02:40.645
and then we can go through the coding workbook.

3035
03:02:44.085 --> 03:02:45.875
Let's go back to this.

3036
03:02:46.095 --> 03:02:49.155
Um, so, so vanishing gradient problem, right?

3037
03:02:49.165 --> 03:02:51.355
We've seen that, you know, because of the nature r

3038
03:02:51.355 --> 03:02:52.955
and n use as the same way it's

3039
03:02:53.475 --> 03:02:55.275
multiplied several times through time.

3040
03:02:55.615 --> 03:02:57.875
You can see vanishing and exploding gradient problem.

3041
03:02:58.575 --> 03:03:00.115
So how can we mitigate that?

3042
03:03:00.615 --> 03:03:02.955
Uh, LSTM is one of the network car architecture.

3043
03:03:02.955 --> 03:03:07.715
This is a sub, uh, you can say, uh, it's, it's, it's one

3044
03:03:07.715 --> 03:03:11.955
of the architecture within the RN ecosystem, uh, which kind

3045
03:03:11.955 --> 03:03:16.275
of mitigates that, uh, uh, vanishing in gradient,

3046
03:03:16.775 --> 03:03:17.875
uh, vanishing gradient

3047
03:03:18.055 --> 03:03:20.555
and exploding gradient problem, uh,

3048
03:03:20.555 --> 03:03:23.235
through a different, uh, technique.

3049
03:03:23.815 --> 03:03:25.755
Uh, we'll talk about what that technic is.

3050
03:03:26.745 --> 03:03:30.645
So, LS TM stands for long short term memory.

3051
03:03:31.425 --> 03:03:33.885
Uh, again, it's a type of RNN architecture.

3052
03:03:34.465 --> 03:03:36.285
Um, it's under the same umbrella.

3053
03:03:36.945 --> 03:03:39.085
So they're designed, lstm are designed to capture

3054
03:03:39.265 --> 03:03:42.605
and remember information over long sequences.

3055
03:03:43.345 --> 03:03:45.685
Um, this makes them suitable for tasks where

3056
03:03:46.335 --> 03:03:47.625
understanding context

3057
03:03:47.925 --> 03:03:51.425
and dependencies over a longer period of time is essential.

3058
03:03:52.085 --> 03:03:55.025
So they do that by using a mechanism called gates.

3059
03:03:55.845 --> 03:03:58.365
Uh, the gates in L ls TMS allow the model

3060
03:03:58.505 --> 03:04:02.525
to control the flaw of information, uh, making it capable

3061
03:04:03.025 --> 03:04:07.925
of learning to remember and forget information as needed.

3062
03:04:08.465 --> 03:04:10.765
So in RN there is no forgetting part.

3063
03:04:10.765 --> 03:04:12.125
There is all always, like, you know,

3064
03:04:12.125 --> 03:04:14.205
hidden state gets keep on updating.

3065
03:04:15.065 --> 03:04:18.405
Um, so here l SDMs have the ability to both remember

3066
03:04:18.405 --> 03:04:21.765
and forget this makes L SDMs particularly effective

3067
03:04:21.785 --> 03:04:23.965
for tasks in natural language processing.

3068
03:04:24.805 --> 03:04:27.645
LS TM reigned the era for, for a while, like, you know,

3069
03:04:27.645 --> 03:04:28.965
before transformers came.

3070
03:04:29.445 --> 03:04:32.005
L SDMs are the go-to, for a lot of language tasks.

3071
03:04:32.685 --> 03:04:36.265
Um, l SDMs, bidirectional L SDMs, as I said,

3072
03:04:36.265 --> 03:04:38.745
like bidirectional meaning, like, you, you don't go

3073
03:04:38.745 --> 03:04:40.185
through the sequence in one direction,

3074
03:04:40.205 --> 03:04:42.385
but also in the opposite direction as well.

3075
03:04:43.045 --> 03:04:47.825
Um, and, uh, they're heavily used for series like, you know,

3076
03:04:48.315 --> 03:04:51.625
tasks like time series forecasting, language understanding,

3077
03:04:52.115 --> 03:04:55.065
where, you know, longer time periods are, uh,

3078
03:04:55.165 --> 03:04:56.425
are, are present.

3079
03:04:57.405 --> 03:04:58.905
So we'll see how these, you know,

3080
03:04:59.435 --> 03:05:01.785
gated mechanisms work in lstm.

3081
03:05:03.485 --> 03:05:07.865
So the core, uh, areas of LSTM, uh, you know, we have the

3082
03:05:08.755 --> 03:05:13.655
cell state, and then we have three gates, uh, input gate

3083
03:05:14.235 --> 03:05:15.815
or GI gate and output gate.

3084
03:05:16.155 --> 03:05:18.095
Uh, we'll look at each of these in detail,

3085
03:05:18.635 --> 03:05:21.095
but those are the core components of lstm.

3086
03:05:25.345 --> 03:05:29.765
So, cell state, think of it as like most of you are aware

3087
03:05:29.765 --> 03:05:32.405
of, like, you know, some sort of assembly line, right?

3088
03:05:32.565 --> 03:05:34.165
Conveyor belt kind of thing.

3089
03:05:34.705 --> 03:05:37.085
So where, you know, things keep on moving

3090
03:05:37.085 --> 03:05:38.725
through the manufacturing process.

3091
03:05:39.065 --> 03:05:40.765
So think of cell state as that.

3092
03:05:41.185 --> 03:05:43.405
So it's like a conveyor belt of information

3093
03:05:44.475 --> 03:05:47.055
and gates are like, you know, um,

3094
03:05:47.515 --> 03:05:51.015
or what control what gets put on this conveyor belt.

3095
03:05:51.875 --> 03:05:56.465
So we have three gates, right? So we have your input gate.

3096
03:05:57.845 --> 03:06:00.025
So input gate controls, what gets added

3097
03:06:00.125 --> 03:06:02.825
to this convey belt, right?

3098
03:06:03.745 --> 03:06:08.505
And then we have target gate so far, gate,

3099
03:06:08.505 --> 03:06:13.025
gate controls what gets removed from this control belt, uh,

3100
03:06:13.205 --> 03:06:14.705
con, uh, yeah, conveyor belt.

3101
03:06:15.365 --> 03:06:19.655
And then we have output gate, um,

3102
03:06:20.025 --> 03:06:23.495
which kind of, you know, uh, that controls

3103
03:06:23.645 --> 03:06:25.655
what gets outputted, uh,

3104
03:06:25.675 --> 03:06:27.295
as an output from this conveyor belt.

3105
03:06:27.675 --> 03:06:31.815
So using this three different gate mechanism, input, gait,

3106
03:06:31.845 --> 03:06:35.855
fargate, gait output, gait, we control the cell state,

3107
03:06:36.635 --> 03:06:37.775
uh, c

3108
03:06:38.395 --> 03:06:41.615
and that cell state, cell state is what passes through time,

3109
03:06:41.725 --> 03:06:43.935
like, you know, uh, from T minus one

3110
03:06:43.935 --> 03:06:47.255
to T just like our hidden state in plain vanillas.

3111
03:06:47.955 --> 03:06:51.775
So with these more granular control

3112
03:06:52.005 --> 03:06:56.485
through these gates, lstm achieve the, uh, ability

3113
03:06:56.545 --> 03:07:00.365
to process long sequences without ex exploding

3114
03:07:00.365 --> 03:07:01.445
or managing gradients.

3115
03:07:09.835 --> 03:07:14.095
So as we discussed, um, you know, uh, fargate decides, um,

3116
03:07:14.605 --> 03:07:16.535
what information we are going to retain

3117
03:07:17.195 --> 03:07:18.935
and what we are going to filter out.

3118
03:07:19.755 --> 03:07:22.495
So, uh, it uses this by using,

3119
03:07:23.495 --> 03:07:25.275
uh, sigmoid function.

3120
03:07:25.855 --> 03:07:29.355
Uh, you know, the output of sigmoid function is, is what?

3121
03:07:29.465 --> 03:07:31.115
It's in the range of zero and one.

3122
03:07:31.655 --> 03:07:35.075
So it can, you can usually use a sigmoid function to

3123
03:07:35.775 --> 03:07:36.835
as an on off switch.

3124
03:07:37.735 --> 03:07:40.635
So it applies to sigmoid function on

3125
03:07:42.235 --> 03:07:43.655
the hidden state

3126
03:07:44.675 --> 03:07:48.625
and current input, um, multiplied

3127
03:07:48.625 --> 03:07:50.825
with certain weights and adding a bias.

3128
03:07:51.565 --> 03:07:53.825
And it applies hidden state to create a

3129
03:07:55.405 --> 03:07:56.695
fargate gate vector.

3130
03:07:56.795 --> 03:07:59.735
So it's like 0 1, 0 0, 0, 0, whatever.

3131
03:08:00.635 --> 03:08:04.895
So it's, let's say it's a dimension of, uh, you know, uh,

3132
03:08:05.075 --> 03:08:06.895
it has 10 values in this vector.

3133
03:08:07.635 --> 03:08:11.815
Um, and let's say we have a cell

3134
03:08:11.815 --> 03:08:13.495
state of same dimension.

3135
03:08:13.995 --> 03:08:18.695
Uh, cell state is 0.1, 0.3, 0.5,

3136
03:08:18.945 --> 03:08:20.495
0.7, so on and so forth.

3137
03:08:21.035 --> 03:08:23.895
So we have your cell state at t

3138
03:08:24.155 --> 03:08:26.615
and you have your fargate gate at time.

3139
03:08:26.815 --> 03:08:28.295
T uh, keep this in mind.

3140
03:08:28.715 --> 03:08:30.655
And the way this Fargate gate is created is

3141
03:08:30.655 --> 03:08:33.455
by using a sigmoid function on the hidden state

3142
03:08:33.555 --> 03:08:35.895
and, uh, current input.

3143
03:08:36.755 --> 03:08:39.855
And that creates a list of binary values, zeros and ones.

3144
03:08:46.075 --> 03:08:48.855
And similarly, there is also an input gate,

3145
03:08:49.305 --> 03:08:51.455
which is created based on hidden state

3146
03:08:52.355 --> 03:08:55.375
and, um, current input.

3147
03:08:56.075 --> 03:08:58.455
Um, again, input gate is also a list

3148
03:08:58.455 --> 03:09:00.535
of binary values of the same size.

3149
03:09:00.995 --> 03:09:03.655
So you have 0, 1, 0, 0, 0, whatever.

3150
03:09:04.075 --> 03:09:07.455
Uh, all of these are same dimension as the cell state.

3151
03:09:08.785 --> 03:09:10.865
And for cell state,

3152
03:09:12.575 --> 03:09:15.715
for every new input, we create a candidate vector.

3153
03:09:16.375 --> 03:09:18.875
So this candidate vector is computed again,

3154
03:09:18.995 --> 03:09:20.595
by using the same hidden state.

3155
03:09:21.415 --> 03:09:23.835
And, uh, current input,

3156
03:09:24.375 --> 03:09:26.355
we use 10 hitch activation in this case,

3157
03:09:26.355 --> 03:09:29.915
because this we're creating a candidate cell state.

3158
03:09:30.575 --> 03:09:34.435
Uh, this candidate cell state is, uh, is probably, again,

3159
03:09:34.825 --> 03:09:36.875
same 10 number vector,

3160
03:09:37.535 --> 03:09:40.635
but this would be, uh, between range of minus one

3161
03:09:40.635 --> 03:09:43.315
to plus one because it's using 10 hitch activation.

3162
03:09:44.635 --> 03:09:48.215
So by using the Fargate gate, we computed earlier,

3163
03:09:48.505 --> 03:09:52.285
which is a binary, by using the input gate,

3164
03:09:52.625 --> 03:09:55.645
we complete area, which is again, a binary, uh, vector.

3165
03:09:57.345 --> 03:10:01.565
Uh, we apply that on previous cell state,

3166
03:10:02.095 --> 03:10:06.765
which is a tanh, again, ranges in minus one to plus one,

3167
03:10:08.375 --> 03:10:12.155
and on the candidate cell state minus one to plus one

3168
03:10:13.505 --> 03:10:15.645
to create the next cell state.

3169
03:10:16.105 --> 03:10:18.525
So here we're telling what

3170
03:10:18.545 --> 03:10:21.955
to forget from a previous cell state, what

3171
03:10:21.955 --> 03:10:25.195
to input from the current candidate cell state,

3172
03:10:25.655 --> 03:10:27.115
and creating a new cell state.

3173
03:10:27.995 --> 03:10:31.735
So that's how the L SDMs, you know, forget,

3174
03:10:32.365 --> 03:10:36.535
have more granular control on what to forget what

3175
03:10:36.595 --> 03:10:37.735
to input.

3176
03:10:38.965 --> 03:10:42.745
All of these are learned parameters like, you know, wc,

3177
03:10:43.755 --> 03:10:46.415
um, um, uh, w

3178
03:10:47.715 --> 03:10:48.935
or learned parameters.

3179
03:10:48.955 --> 03:10:53.695
And based on those learned para BIBC, uh, based on that, uh,

3180
03:10:53.875 --> 03:10:58.415
the, uh, and wf, this is a learned parameter.

3181
03:10:58.835 --> 03:11:00.535
Uh, BF is a learned parameter.

3182
03:11:00.535 --> 03:11:03.615
So based on that, it's, uh, the target gates,

3183
03:11:03.755 --> 03:11:07.055
the input gates that learning what to control

3184
03:11:07.075 --> 03:11:09.375
and what not to control, uh, what

3185
03:11:09.375 --> 03:11:10.575
to filter, what not to filter.

3186
03:11:11.755 --> 03:11:15.935
Um, so sigmoid layers are used for binary, uh,

3187
03:11:16.135 --> 03:11:19.255
creating something into binary tanh layers are used to

3188
03:11:19.765 --> 03:11:21.375
zero center it, uh,

3189
03:11:21.375 --> 03:11:25.895
because that inherently tanh makes, uh, squish everything

3190
03:11:25.895 --> 03:11:27.335
between minus one and plus one.

3191
03:11:28.115 --> 03:11:31.135
So, so that's how the gating mechanism works

3192
03:11:31.595 --> 03:11:34.895
to update the cell state in a more granular way.

3193
03:11:35.155 --> 03:11:38.105
In the LSTM networks, this level

3194
03:11:38.125 --> 03:11:41.265
of control is not available in the plain vanilla s we looked

3195
03:11:41.265 --> 03:11:44.585
at where the hidden state is, you know, updated

3196
03:11:44.775 --> 03:11:46.385
with same set of weights.

3197
03:11:47.085 --> 03:11:48.945
Uh, so that's, that makes, that's

3198
03:11:48.945 --> 03:11:52.185
what makes the LSTM sufficient for long sequences.

3199
03:11:54.675 --> 03:11:55.935
Uh, I'll come back to this.

3200
03:11:56.115 --> 03:11:59.895
Uh, again, this is a bit more advanced topic, um, uh,

3201
03:12:00.255 --> 03:12:02.855
constant error corro, uh, we'll come back to this

3202
03:12:02.915 --> 03:12:04.495
and talk about this in a little bit.

3203
03:12:06.505 --> 03:12:10.645
So, output gate, um, output gate, as I said, controls

3204
03:12:10.645 --> 03:12:13.045
what is outputted from the LSTM network

3205
03:12:13.145 --> 03:12:14.325
at a given point of time.

3206
03:12:15.025 --> 03:12:16.845
Uh, the output gate is, again,

3207
03:12:16.955 --> 03:12:18.405
uses a very similar structure.

3208
03:12:19.455 --> 03:12:20.835
Uh, it uses the hidden state

3209
03:12:21.215 --> 03:12:26.115
and, uh, input to create a binary output gate,

3210
03:12:26.145 --> 03:12:27.435
like a bunch of zeros

3211
03:12:27.435 --> 03:12:30.455
and ones, uh, again, same dimension

3212
03:12:30.555 --> 03:12:32.135
as all the previous gates.

3213
03:12:32.955 --> 03:12:36.015
Uh, this output gate is multiplied with 10 H

3214
03:12:36.035 --> 03:12:40.895
of the current cell state to, uh, compute the current,

3215
03:12:42.355 --> 03:12:43.855
uh, the next hidden state.

3216
03:12:44.955 --> 03:12:49.845
Uh, so that's how, uh, you know, um, different, uh,

3217
03:12:50.415 --> 03:12:55.245
gates are used to control what to, um, forget

3218
03:12:55.435 --> 03:12:58.485
what to input and what to c uh, carry over

3219
03:12:58.665 --> 03:12:59.845
for the next hidden state.

3220
03:13:03.605 --> 03:13:06.665
So again, the conveyor belt analogy is,

3221
03:13:06.725 --> 03:13:08.385
is the best way to visualize this.

3222
03:13:08.845 --> 03:13:10.465
So think of that as, you know,

3223
03:13:10.465 --> 03:13:12.665
information passing across the time steps.

3224
03:13:12.975 --> 03:13:14.945
This is, this access is time,

3225
03:13:15.885 --> 03:13:19.585
and cell state, uh, goes through various versions, C zero,

3226
03:13:19.785 --> 03:13:22.425
C one, c, C two to ct,

3227
03:13:23.245 --> 03:13:26.645
and all these different gates input, forget

3228
03:13:26.705 --> 03:13:30.885
and output gates are used to update the cell state, uh,

3229
03:13:30.885 --> 03:13:32.885
through various mechanisms.

3230
03:13:33.625 --> 03:13:38.565
So input gate times input, candidate cell state

3231
03:13:44.785 --> 03:13:46.455
connection, closed by.

3232
03:13:46.675 --> 03:13:47.815
So let's see.

3233
03:14:05.055 --> 03:14:08.085
Right. Let's see. Uh, okay, okay, we are back.

3234
03:14:08.155 --> 03:14:08.725
Yeah, so,

3235
03:14:09.705 --> 03:14:14.655
so, yeah.

3236
03:14:14.835 --> 03:14:17.935
Um, so in, so the way, uh, a

3237
03:14:18.985 --> 03:14:23.725
cell state is updated is, you know, uh, we use, um,

3238
03:14:25.605 --> 03:14:30.185
uh, the f gate times the previous cell state, uh, plus, uh,

3239
03:14:30.235 --> 03:14:32.625
input gate times the candidate cell state,

3240
03:14:33.325 --> 03:14:37.745
and then the hidden state itself is updated using, um,

3241
03:14:38.325 --> 03:14:42.275
the, uh, output gate at any given point of time.

3242
03:14:43.175 --> 03:14:46.635
Um, so, so with use of these, all these, uh,

3243
03:14:46.655 --> 03:14:49.995
gating mechanisms, there is more granular control in L SDMs,

3244
03:14:50.535 --> 03:14:53.875
uh, to be able to, uh, uh, retain information.

3245
03:14:54.455 --> 03:14:56.955
Uh, and that helps with, you know, vanishing gradient

3246
03:14:57.015 --> 03:15:00.675
and exploding gradient problem significantly with L SDMs.

3247
03:15:01.735 --> 03:15:06.135
Um, so again, uh, we talked about, you know,

3248
03:15:06.595 --> 03:15:07.775
the L SDMs.

3249
03:15:08.235 --> 03:15:12.475
Uh, what are the different, uh, okay,

3250
03:15:13.475 --> 03:15:15.445
what are the different, um, you know,

3251
03:15:15.445 --> 03:15:17.325
gating mechanisms within LSMs?

3252
03:15:18.025 --> 03:15:20.245
Uh, I think in the previous section we talked about

3253
03:15:20.245 --> 03:15:22.645
advantages of RN limitations and applications.

3254
03:15:23.385 --> 03:15:25.285
Uh, we'll go through the working example

3255
03:15:25.395 --> 03:15:27.525
dataset, uh, in a bit.

3256
03:15:28.385 --> 03:15:32.205
Uh, so GRU, uh, I'll just hint, hint on that a bit.

3257
03:15:32.315 --> 03:15:33.805
That material is not there here.

3258
03:15:34.305 --> 03:15:36.645
So GRU stands for gated recurrent unit.

3259
03:15:37.465 --> 03:15:42.165
Uh, they're also good at, um, uh, exploding

3260
03:15:42.165 --> 03:15:43.285
and vanishing gradient problem.

3261
03:15:44.225 --> 03:15:48.255
Uh, so you look at, uh,

3262
03:15:49.845 --> 03:15:54.685
uh, so if you

3263
03:15:54.685 --> 03:15:57.685
compare these three plain vanilla,

3264
03:15:57.805 --> 03:16:02.805
R-N-N-L-S-T-M-G-R-U,

3265
03:16:04.775 --> 03:16:08.425
so, so L SDMs can address vanishing

3266
03:16:09.635 --> 03:16:14.165
and exploring gradient, uh, so does GRU.

3267
03:16:14.305 --> 03:16:16.685
So GRU, instead of using three gates,

3268
03:16:16.685 --> 03:16:18.205
like L sst m they use like, you know,

3269
03:16:18.295 --> 03:16:19.765
reset gate and update gate.

3270
03:16:20.105 --> 03:16:22.165
So they, they reduce the number of gates,

3271
03:16:22.505 --> 03:16:24.685
but kind of implement similar functionality.

3272
03:16:25.905 --> 03:16:29.685
So reset gate and update gate, uh, l SDMs use

3273
03:16:30.755 --> 03:16:32.845
input or gate

3274
03:16:33.065 --> 03:16:37.315
and output gate, um,

3275
03:16:39.295 --> 03:16:40.615
G use because, you know,

3276
03:16:40.615 --> 03:16:42.335
they reduce the number of parameters.

3277
03:16:42.555 --> 03:16:45.495
Uh, they're kind of slightly more efficient than LSTM,

3278
03:16:47.685 --> 03:16:52.225
but also, uh, less, uh, accurate, uh, compared

3279
03:16:52.225 --> 03:16:55.225
to l sst m uh, so there is slight trade off

3280
03:16:55.225 --> 03:16:57.465
between accuracy versus efficiency.

3281
03:16:57.805 --> 03:17:01.425
So j work well, like, you know, in where, you know, you,

3282
03:17:01.485 --> 03:17:03.785
you don't have much compute compared to

3283
03:17:03.895 --> 03:17:05.145
what you have for lstm.

3284
03:17:06.165 --> 03:17:07.665
So it's all in comparison.

3285
03:17:07.775 --> 03:17:09.305
It's not like J are like, you know,

3286
03:17:09.305 --> 03:17:10.865
you can put them on a mobile or something,

3287
03:17:10.865 --> 03:17:14.305
but j are slightly more efficient than lstm.

3288
03:17:14.485 --> 03:17:17.425
But, uh, they do that by giving up some accuracy.

3289
03:17:18.975 --> 03:17:22.715
Um, so th that's, those are the main salient differences

3290
03:17:22.715 --> 03:17:26.275
between g lstm and plain vanilla irons.

3291
03:17:27.175 --> 03:17:28.795
Um, cool.

3292
03:17:29.495 --> 03:17:33.675
So now let's go back to this one. Constant error corro.

3293
03:17:34.695 --> 03:17:38.735
So in the way the R ns work, um,

3294
03:17:39.395 --> 03:17:41.335
and in the way the r

3295
03:17:41.395 --> 03:17:42.455
and architectures work,

3296
03:17:43.455 --> 03:17:45.775
a particular error can keep on repeating.

3297
03:17:46.475 --> 03:17:50.445
So for example, in LSDM, if fargate is set

3298
03:17:50.445 --> 03:17:54.235
to the wrong values, uh, of zeros and ones,

3299
03:17:55.255 --> 03:17:59.915
and an error occurs in the, uh, RNN,

3300
03:18:00.455 --> 03:18:04.885
uh, the l SST m it keeps on repeating those errors

3301
03:18:05.385 --> 03:18:08.365
if the forget gate doesn't forget the wrong thing.

3302
03:18:09.105 --> 03:18:13.605
So that's what's meant by constant error carros, uh,

3303
03:18:13.795 --> 03:18:16.445
it's mitigated by various techniques, uh,

3304
03:18:16.445 --> 03:18:17.885
like teacher forcing and stuff.

3305
03:18:18.385 --> 03:18:22.005
Uh, again, out of scope of this particular class, um,

3306
03:18:22.345 --> 03:18:24.645
you might learn that in the NLP classes,

3307
03:18:25.545 --> 03:18:29.485
but, um, what constant error carros in definition is,

3308
03:18:31.555 --> 03:18:33.695
is because the way the gates work,

3309
03:18:33.805 --> 03:18:37.015
sometimes Fargate gates can be set to the wrong states,

3310
03:18:37.715 --> 03:18:38.775
and due to that,

3311
03:18:38.915 --> 03:18:42.055
the same error can keep on getting propagated.

3312
03:18:42.155 --> 03:18:44.655
So that's the, if you look, uh,

3313
03:18:44.655 --> 03:18:46.935
like if you saw some in carnivals, like caral,

3314
03:18:47.250 --> 03:18:49.285
they're like turning around and around, right?

3315
03:18:49.665 --> 03:18:51.605
So it's the constant error carousal.

3316
03:18:53.845 --> 03:18:55.105
Um, cool.

3317
03:18:55.325 --> 03:18:59.105
Uh, so now with that said, let's see what other slides.

3318
03:19:01.605 --> 03:19:04.415
Okay, so don't have more slides.

3319
03:19:05.035 --> 03:19:08.135
So let's go through the coding workbook

3320
03:19:19.665 --> 03:19:20.805
and let's see this finished.

3321
03:19:23.765 --> 03:19:25.785
Uh, this is the CNN workbook.

3322
03:19:25.845 --> 03:19:30.505
Uh, so, so we are running eight, right?

3323
03:19:30.605 --> 03:19:35.375
So looks like, you know, uh, the loss

3324
03:19:36.055 --> 03:19:37.575
training loss went down.

3325
03:19:38.965 --> 03:19:41.185
Uh, validation loss also went down.

3326
03:19:42.395 --> 03:19:45.255
Um, given the loss kept on going,

3327
03:19:45.915 --> 03:19:47.815
our learning rate was never changed.

3328
03:19:48.675 --> 03:19:50.495
It was the same.

3329
03:19:50.805 --> 03:19:52.135
This didn't come into effect

3330
03:19:52.485 --> 03:19:54.895
because we implemented a learning rate any either, right?

3331
03:19:54.955 --> 03:19:59.615
So, so the learning rate, uh, did not reduce up

3332
03:19:59.635 --> 03:20:04.625
for three ocs, then we will, uh, reduce it

3333
03:20:04.645 --> 03:20:05.985
by half rate.

3334
03:20:06.245 --> 03:20:07.945
So this did not come into effect here.

3335
03:20:08.635 --> 03:20:11.265
Maybe if you run it for, like, if you try it on your own,

3336
03:20:11.265 --> 03:20:14.545
like for like 10, 15 ocs, you can see that happen.

3337
03:20:15.795 --> 03:20:19.575
Um, but yeah, so looks like the model converged.

3338
03:20:19.845 --> 03:20:23.495
Well, uh, let's see if we can run these now.

3339
03:20:39.305 --> 03:20:40.845
Yeah. Cool.

3340
03:20:40.945 --> 03:20:44.825
Um, yeah, let's move on to, uh,

3341
03:20:45.105 --> 03:20:46.665
RNN workbook.

3342
03:20:49.345 --> 03:20:52.245
So here, I think we are using an LSTM, that's why I wanted

3343
03:20:52.245 --> 03:20:53.765
to go through the LSTM material

3344
03:20:53.865 --> 03:20:57.205
before, uh, showing you this workbook.

3345
03:20:58.025 --> 03:20:59.725
So for this, we'll use, uh,

3346
03:21:00.045 --> 03:21:02.005
a New York Stock Exchange data set.

3347
03:21:02.825 --> 03:21:06.565
Um, the data span from 2010 to 2016.

3348
03:21:07.505 --> 03:21:11.365
Um, and, uh, we are using stock for

3349
03:21:12.035 --> 03:21:15.325
like your just apple, uh, for this one.

3350
03:21:15.985 --> 03:21:19.385
Um, so, uh,

3351
03:21:19.445 --> 03:21:21.665
and we'll use an LSTM network, uh,

3352
03:21:21.725 --> 03:21:25.885
to predict the stock price, uh, in the next time step.

3353
03:21:27.615 --> 03:21:31.195
So for this, again, we are using kras, uh,

3354
03:21:32.065 --> 03:21:33.555
APIs from TensorFlow

3355
03:21:34.785 --> 03:21:38.595
and all the

3356
03:21:38.595 --> 03:21:39.875
basic imports in place.

3357
03:21:46.085 --> 03:21:47.915
Let's see how this data set looks like,

3358
03:21:48.185 --> 03:21:50.795
because this is the first time we are looking at it.

3359
03:21:55.455 --> 03:21:59.065
Okay. So we have the stock symbol date, stock symbol.

3360
03:21:59.765 --> 03:22:01.985
We have the open price, close price, low, high,

3361
03:22:02.685 --> 03:22:06.235
and volume, uh, volume of the, like, you know,

3362
03:22:06.335 --> 03:22:07.555
the stock transactions.

3363
03:22:09.575 --> 03:22:09.795
Um,

3364
03:22:18.035 --> 03:22:21.615
we also have some fundamental data related to stock tickers,

3365
03:22:22.325 --> 03:22:25.815
like, you know, the period ending, uh,

3366
03:22:25.835 --> 03:22:28.625
the accountable receivable, um,

3367
03:22:29.965 --> 03:22:31.505
tax related stuff and all of that.

3368
03:22:31.665 --> 03:22:33.105
I don't think we are using fundamentals

3369
03:22:33.105 --> 03:22:36.305
for this particular exercise, but yeah, good to know.

3370
03:22:37.045 --> 03:22:39.705
Um, so we'll only use Apple sticker,

3371
03:22:39.885 --> 03:22:44.545
so we'll just filter the talk trading data for the sticker.

3372
03:22:45.605 --> 03:22:49.635
Um, so again, just for Apple,

3373
03:22:50.525 --> 03:22:53.765
we're looking at open, close, low, high volume, uh, values.

3374
03:22:56.835 --> 03:23:01.435
Um, so we have 1762 rows here, um,

3375
03:23:01.905 --> 03:23:03.955
over a period of, uh, five, six years.

3376
03:23:06.515 --> 03:23:07.535
So just doing head

3377
03:23:07.535 --> 03:23:09.735
and tail to see how the data set looks like.

3378
03:23:09.815 --> 03:23:14.255
I think, I believe we have daily data from 2010

3379
03:23:14.915 --> 03:23:19.075
fourth January to 2016, December 30.

3380
03:23:21.765 --> 03:23:25.055
So we are using the close prices for this.

3381
03:23:25.835 --> 03:23:29.655
So not the open, not, not lower or high,

3382
03:23:29.655 --> 03:23:30.695
but just the close prices.

3383
03:23:32.025 --> 03:23:35.075
So just plotting that, uh, or a period of time.

3384
03:23:35.975 --> 03:23:38.075
Um, so those are the close prices.

3385
03:23:39.575 --> 03:23:42.115
Uh, looks like a big variation, uh, in the prices

3386
03:23:50.075 --> 03:23:54.785
and doing some basic date, uh, manipulations, um,

3387
03:23:55.365 --> 03:23:59.345
so that we can use the date appropriately.

3388
03:24:06.825 --> 03:24:09.045
Um, so, uh,

3389
03:24:09.305 --> 03:24:11.965
we are using a MinMax scaler.

3390
03:24:12.025 --> 03:24:14.925
Uh, this is again, another form of normalization,

3391
03:24:14.925 --> 03:24:16.805
just like we did with the pixel values.

3392
03:24:17.915 --> 03:24:22.215
Uh, we are using a MinMax scaler here, so, uh,

3393
03:24:23.635 --> 03:24:26.455
on the stock prices, because there is huge variation in the

3394
03:24:26.455 --> 03:24:29.815
stock prices, right, like from 40 to one 20.

3395
03:24:30.515 --> 03:24:33.775
So by using this carer, uh, that helps the

3396
03:24:34.925 --> 03:24:38.405
LSTM to generalize better and learn better.

3397
03:24:44.345 --> 03:24:47.515
Cool. Um, so splitting the dataset.

3398
03:24:47.775 --> 03:24:51.295
Um, so we are using, um,

3399
03:24:53.045 --> 03:24:54.415
70% for training

3400
03:24:55.155 --> 03:24:58.205
and, uh, 30% for testing.

3401
03:24:59.425 --> 03:25:03.915
Um, so, um, we'll split the dataset as such.

3402
03:25:04.575 --> 03:25:06.675
So maybe tell me one thing, like

3403
03:25:07.175 --> 03:25:09.235
how do you, how can we split here?

3404
03:25:09.255 --> 03:25:11.755
Can, does random split work? Just curious.

3405
03:25:13.115 --> 03:25:17.905
So, so we have our talk data,

3406
03:25:17.905 --> 03:25:22.305
right from 2010 to all the way

3407
03:25:22.325 --> 03:25:23.705
to 2016.

3408
03:25:25.855 --> 03:25:28.275
So how does, uh,

3409
03:25:30.175 --> 03:25:31.865
what is the best way to split this data?

3410
03:25:37.485 --> 03:25:41.545
Can I do just, uh, a scale train to split on this?

3411
03:26:03.625 --> 03:26:08.605
So, uh, for temporal data, it's important to separate,

3412
03:26:09.385 --> 03:26:13.965
uh, the data temporarily, uh, for 70%.

3413
03:26:13.965 --> 03:26:15.965
Yeah, I think that's a good one. Wean.

3414
03:26:16.065 --> 03:26:20.755
So especially when you are using temporal data,

3415
03:26:21.215 --> 03:26:24.675
uh, it's important to separate them temporarily.

3416
03:26:24.895 --> 03:26:29.035
So you can use, you know, trained data, let's say from 2010

3417
03:26:29.035 --> 03:26:33.635
to 2015 and test data from 2015 to 2016.

3418
03:26:34.105 --> 03:26:37.955
Otherwise, uh, it, there will be a data leakage.

3419
03:26:38.255 --> 03:26:41.855
So if the model is we are doing random, um,

3420
03:26:43.325 --> 03:26:47.295
splitting, uh, then you're saying that the model can look at

3421
03:26:47.955 --> 03:26:52.775
future data to predict the past, uh, examples

3422
03:26:52.915 --> 03:26:55.855
or test it on the past examples, which is not right.

3423
03:26:56.475 --> 03:27:00.965
So it's always important to temporarily separate the test

3424
03:27:01.615 --> 03:27:04.925
validation and train samples, uh,

3425
03:27:04.925 --> 03:27:07.125
especially when you're dealing with sequential data.

3426
03:27:07.705 --> 03:27:11.205
And that's one difference you have to keep in mind compared

3427
03:27:11.205 --> 03:27:14.805
to working with, uh, any normal, uh, data.

3428
03:27:17.165 --> 03:27:20.425
In the previous example, like handwritten digit recognition,

3429
03:27:20.925 --> 03:27:22.745
you, we don't care, like, you know,

3430
03:27:23.045 --> 03:27:25.585
it could be split randomly, uh,

3431
03:27:25.805 --> 03:27:30.225
and, uh, that, that, that wouldn't make any difference.

3432
03:27:30.445 --> 03:27:33.785
But with temporal data, uh, you need to keep attention

3433
03:27:33.845 --> 03:27:38.745
to the temporal scale and split randomly.

3434
03:27:44.625 --> 03:27:49.345
Cool, uh, again, uh, doing some basic, um,

3435
03:27:49.815 --> 03:27:54.705
metrics, uh, um, uh, conversions,

3436
03:27:55.605 --> 03:27:59.425
uh, to, to be able to use the data, uh, for our lstm.

3437
03:28:00.125 --> 03:28:04.865
So here, uh, just,

3438
03:28:05.205 --> 03:28:09.625
uh, going back here, we are splitting it this way

3439
03:28:09.725 --> 03:28:11.665
by using these indices.

3440
03:28:12.485 --> 03:28:15.665
Uh, here, since the dataset is already sorted, it kind

3441
03:28:15.665 --> 03:28:17.065
of works, but, uh,

3442
03:28:17.095 --> 03:28:19.505
sometimes dataset is not always sorted, right?

3443
03:28:19.605 --> 03:28:23.385
So, um, here looks like it's very well sorted already.

3444
03:28:23.525 --> 03:28:26.865
So that's why by taking the first 70% indices,

3445
03:28:27.045 --> 03:28:29.105
we are creating the train train set,

3446
03:28:29.125 --> 03:28:31.545
and the next 30% of the samples,

3447
03:28:31.685 --> 03:28:32.905
we are creating the test set.

3448
03:28:33.565 --> 03:28:35.065
But sometimes the data set may,

3449
03:28:35.165 --> 03:28:36.545
may not be sorted temporarily.

3450
03:28:36.605 --> 03:28:37.905
So that's something to keep in mind.

3451
03:28:46.255 --> 03:28:49.035
So for LSTM network, uh, as we discussed,

3452
03:28:49.035 --> 03:28:51.715
like there are different kinds of, uh, gates

3453
03:28:51.715 --> 03:28:55.515
and such, uh, again, we don't have to configure all of that.

3454
03:28:55.855 --> 03:29:00.515
And, um, uh, when we are creating the network, um,

3455
03:29:00.695 --> 03:29:04.715
so just like in the Caras implementation layer for CNN, we,

3456
03:29:04.765 --> 03:29:06.155
we'll use a sequential module

3457
03:29:07.255 --> 03:29:12.065
and use LSTM layer, uh, with the input shape,

3458
03:29:12.935 --> 03:29:14.075
uh, of one.

3459
03:29:14.335 --> 03:29:15.315
And, uh,

3460
03:29:21.345 --> 03:29:23.125
so here we are using mean squared error

3461
03:29:23.125 --> 03:29:26.605
because we are predicting the stock market prices, right?

3462
03:29:26.605 --> 03:29:29.525
Closing prices. So we'll use the MSC error

3463
03:29:29.525 --> 03:29:30.885
because it's a regression problem.

3464
03:29:32.735 --> 03:29:35.275
Uh, after the LSTM layer, we are using a dense layer.

3465
03:29:36.125 --> 03:29:40.185
Um, and, uh, for model optimization, we are using aada.

3466
03:29:41.655 --> 03:29:45.065
Um, let's see, uh,

3467
03:29:45.295 --> 03:29:46.865
what else, what other settings?

3468
03:29:48.185 --> 03:29:51.765
Uh, so for the LSTM layer, the key things are, you know,

3469
03:29:52.025 --> 03:29:56.965
we are using, um, 20, uh, LSTM blocks

3470
03:29:57.465 --> 03:29:59.005
or 20 LSTM neurons.

3471
03:29:59.925 --> 03:30:01.225
Um, and,

3472
03:30:02.045 --> 03:30:05.985
and for lookback, I believe we are using lookback of,

3473
03:30:06.635 --> 03:30:08.545
let's see, uh, 15.

3474
03:30:09.125 --> 03:30:13.025
So we are using previous 15 time steps for look back.

3475
03:30:13.645 --> 03:30:16.345
Uh, so, uh, so for any given data point,

3476
03:30:16.365 --> 03:30:17.945
we are using a look back of 15.

3477
03:30:18.445 --> 03:30:23.295
Um, so what that means is, uh, like, you know,

3478
03:30:23.315 --> 03:30:24.415
if your data is,

3479
03:30:29.175 --> 03:30:30.035
um, like, you know,

3480
03:30:30.035 --> 03:30:35.035
0 1, 2, 3, 4, 5, 15, 16,

3481
03:30:37.275 --> 03:30:42.265
so, uh, for 15, we'll use everything from zero

3482
03:30:42.325 --> 03:30:46.345
to 14 times steps as our, uh, uh,

3483
03:30:47.235 --> 03:30:51.665
input window for times, steps 16, we'll use one to 15

3484
03:30:52.365 --> 03:30:53.465
as our look back.

3485
03:30:53.885 --> 03:30:54.945
So that's what look back means.

3486
03:31:02.085 --> 03:31:06.945
Um, Another thing, uh,

3487
03:31:07.065 --> 03:31:10.065
I want to mention is like, you know, for LSTM, like,

3488
03:31:10.065 --> 03:31:11.425
you know, you have your input layer

3489
03:31:13.205 --> 03:31:15.025
and you have your LSTM layer.

3490
03:31:17.285 --> 03:31:19.865
Uh, this is 20 block LSM, right?

3491
03:31:20.525 --> 03:31:24.505
So all the recurrences are happening here, so don't think

3492
03:31:24.505 --> 03:31:26.385
of, you know, for recurrent connection,

3493
03:31:26.385 --> 03:31:27.545
there are additional layers.

3494
03:31:27.975 --> 03:31:29.705
It's all happening internal to that layer,

3495
03:31:30.355 --> 03:31:31.455
all the recurrent connections.

3496
03:31:31.455 --> 03:31:33.455
So all the 15 look backs are happening here

3497
03:31:33.755 --> 03:31:34.775
as you pass the input.

3498
03:31:35.735 --> 03:31:38.355
So, so you only have input layer, LTM layer,

3499
03:31:38.455 --> 03:31:40.835
and then, uh, fully connected layer,

3500
03:31:40.855 --> 03:31:42.195
and then your output layer.

3501
03:31:43.465 --> 03:31:46.005
So this kind of the recurrent nature

3502
03:31:46.065 --> 03:31:47.765
of the LSTM back propagation

3503
03:31:47.765 --> 03:31:51.085
through time is abstracted into this one particular layer,

3504
03:31:51.825 --> 03:31:53.565
uh, which is kind of, uh, neat.

3505
03:31:53.865 --> 03:31:55.765
Uh, you don't have to worry too much about, you know,

3506
03:31:55.765 --> 03:31:57.165
all the time steps and all of that.

3507
03:32:00.125 --> 03:32:03.105
No, uh, Vincent, uh, those are two different parameters.

3508
03:32:03.285 --> 03:32:05.025
Uh, you can change these

3509
03:32:05.205 --> 03:32:08.425
and see how the network, um, uh, will learn differently.

3510
03:32:08.885 --> 03:32:12.625
So the Altium block is the size of the block.

3511
03:32:13.095 --> 03:32:16.265
Look back is how much based on the input data, Hey,

3512
03:32:16.305 --> 03:32:17.985
I only want to look back for 15 days.

3513
03:32:18.325 --> 03:32:21.185
That's a look back. Uh, if I want to change that to,

3514
03:32:21.185 --> 03:32:23.545
I only look back for 10 days of previous stock prices,

3515
03:32:23.565 --> 03:32:24.625
that's a different look back.

3516
03:32:24.625 --> 03:32:28.985
So those two are in not related, independent to each other.

3517
03:32:35.275 --> 03:32:38.125
Cool. Uh, so that's the Lian structure.

3518
03:32:39.155 --> 03:32:41.135
So let's start running this.

3519
03:32:41.235 --> 03:32:44.245
Uh, we are running this for 20 bucks, uh,

3520
03:32:44.585 --> 03:32:47.365
and see how the loss, uh, changes.

3521
03:32:49.555 --> 03:32:52.175
Uh, luckily the lium is running pretty fast,

3522
03:32:59.365 --> 03:33:02.425
So it looks like the loss is gradually, uh, reducing.

3523
03:33:02.845 --> 03:33:07.815
Um, and, um, kind of, I don't know, kind of,

3524
03:33:09.315 --> 03:33:12.695
uh, going back

3525
03:33:12.695 --> 03:33:14.415
and forth in the last few steps.

3526
03:33:14.915 --> 03:33:18.975
Um, so maybe there is an opportunity to

3527
03:33:20.185 --> 03:33:22.725
reduce the learning rate here using the learning rate

3528
03:33:22.925 --> 03:33:27.465
granular, but let's use the model for predictions

3529
03:33:27.845 --> 03:33:29.225
and see how we do.

3530
03:33:30.165 --> 03:33:32.825
Uh, so essentially using the model predict function

3531
03:33:33.565 --> 03:33:37.965
and, uh, predicting, uh, given we use a MinMax scaler, uh,

3532
03:33:38.105 --> 03:33:39.725
we should use an inverse transform

3533
03:33:39.945 --> 03:33:44.915
to transform back based on the, um, MinMax scaling,

3534
03:33:45.695 --> 03:33:50.035
um, and then calculate the mean squared error.

3535
03:33:51.025 --> 03:33:55.125
So RMSE for train is 1.93.

3536
03:33:55.625 --> 03:33:58.405
Uh, test is 3.3, not, not that great.

3537
03:33:59.145 --> 03:34:02.565
So given the simplistic model, we are not doing

3538
03:34:02.565 --> 03:34:05.645
that great in terms of, you know, test RMC.

3539
03:34:07.315 --> 03:34:12.175
Um, so let's, uh, plot these, uh, see how this looks.

3540
03:34:13.215 --> 03:34:16.635
So for that, we'll plot the actual values as well

3541
03:34:16.635 --> 03:34:17.715
as the predicted values.

3542
03:34:24.085 --> 03:34:27.265
Um, so even though this graph is not, you know,

3543
03:34:27.475 --> 03:34:28.785
super informative,

3544
03:34:28.805 --> 03:34:33.425
but it looks pretty close, uh, I think given RMC of 3.38,

3545
03:34:34.345 --> 03:34:36.265
I think we're within like three

3546
03:34:36.325 --> 03:34:40.905
or $4 range of the output for the most of the time.

3547
03:34:44.155 --> 03:34:48.125
Yeah. So again, that's a quick demonstration of

3548
03:34:48.185 --> 03:34:50.565
how LSTM can be trained.

3549
03:34:51.105 --> 03:34:56.045
Uh, what are some, um, basic inputs to LSTM, uh,

3550
03:34:56.465 --> 03:34:59.125
and how to set it up in the model block.

3551
03:34:59.905 --> 03:35:03.925
Um, so here we are just training basic LSTM with, you know,

3552
03:35:03.925 --> 03:35:05.005
20 LSTM blocks

3553
03:35:05.385 --> 03:35:09.035
and, uh, one dense layer, um,

3554
03:35:09.495 --> 03:35:10.715
and an output layer.

3555
03:35:11.625 --> 03:35:14.445
Um, but, uh, but yeah, uh, that's, uh,

3556
03:35:14.785 --> 03:35:16.485
that's the end of the coding walkthrough.

3557
03:35:17.485 --> 03:35:21.025
Um, so let me quickly go back and recap stuff,

3558
03:35:21.165 --> 03:35:24.455
and then I can stay over for, um,

3559
03:35:25.155 --> 03:35:26.215
you know, some questions.

3560
03:35:27.155 --> 03:35:30.195
So in yesterday's session

3561
03:35:30.215 --> 03:35:33.075
and today's session, we looked at three things.

3562
03:35:34.245 --> 03:35:36.745
Uh, Adam is one of the optimizer type.

3563
03:35:36.925 --> 03:35:41.465
Uh, so, so it's a combination of AdaGrad and RMS prop.

3564
03:35:42.205 --> 03:35:45.065
Um, so read, read up on those optimizers.

3565
03:35:45.245 --> 03:35:50.185
So, um, RMS prop uses a method called, um,

3566
03:35:51.345 --> 03:35:55.085
it uses the, the mean square, uh,

3567
03:35:55.425 --> 03:35:58.925
losses over a period of time, um, to, uh,

3568
03:35:59.785 --> 03:36:01.525
to compute the gradients.

3569
03:36:02.485 --> 03:36:05.885
AdaGrad is kind of, it's customized to each parameter.

3570
03:36:06.505 --> 03:36:08.085
Uh, so it's a combination of both.

3571
03:36:08.085 --> 03:36:12.285
It's a very efficient optimizer algorithm, uh, used, uh,

3572
03:36:12.745 --> 03:36:14.405
in a lot of practical scenarios.

3573
03:36:15.995 --> 03:36:20.455
Um, so going back to, um, you know, uh,

3574
03:36:20.995 --> 03:36:24.975
our class over the last two days, so we looked at fs,

3575
03:36:25.865 --> 03:36:30.685
um, we looked at CNNs, and we looked at s

3576
03:36:31.225 --> 03:36:34.725
and essentially we kind of understood,

3577
03:36:36.025 --> 03:36:38.405
uh, what are the core differences

3578
03:36:38.405 --> 03:36:42.365
between these three architectures and why they're applied,

3579
03:36:42.425 --> 03:36:45.885
or why they're, they're actually designed that, that way,

3580
03:36:46.425 --> 03:36:49.445
um, because FMS are, you know, densely connected, right?

3581
03:36:49.985 --> 03:36:52.765
Um, they're good for, you know, uh, when you have like,

3582
03:36:52.785 --> 03:36:56.965
you know, features that are, uh, not spatial in nature

3583
03:36:57.065 --> 03:36:58.445
or not sequential in nature.

3584
03:36:59.065 --> 03:37:02.565
Uh, so they kind of learn very well, uh,

3585
03:37:02.565 --> 03:37:04.885
across a densely connected feature set.

3586
03:37:06.135 --> 03:37:09.925
CNN's kind of exploit the, uh, the principles of locality

3587
03:37:10.825 --> 03:37:12.845
and translation variance,

3588
03:37:13.625 --> 03:37:16.445
and use different kinds of techniques like, you know, um,

3589
03:37:17.115 --> 03:37:21.775
convolution layer, uh, pooling layers, uh,

3590
03:37:21.955 --> 03:37:25.855
to, to be able to, um, exploit that for more efficiency

3591
03:37:26.355 --> 03:37:28.895
and, uh, better learning capabilities.

3592
03:37:29.995 --> 03:37:33.815
Um, RN Ns, on the other hand, are good at dealing

3593
03:37:33.815 --> 03:37:36.095
with sequential data, uh,

3594
03:37:36.315 --> 03:37:37.495
and they kind of do that

3595
03:37:37.555 --> 03:37:40.055
by using a technical back propagation through time

3596
03:37:40.995 --> 03:37:44.535
and, uh, use various mechanisms like, you know, uh,

3597
03:37:44.955 --> 03:37:48.675
for example, lstm uses gates, um, uh,

3598
03:37:48.935 --> 03:37:52.675
and, uh, uh, gates to, to be able to retain

3599
03:37:53.095 --> 03:37:57.835
and information through cell states and hidden states.

3600
03:37:59.845 --> 03:38:01.665
And, uh, we looked at, like, you know,

3601
03:38:01.735 --> 03:38:04.025
from a coding standpoint, we looked at like, you know, how

3602
03:38:04.025 --> 03:38:07.705
to use, uh, uh, how, how PY touch framework can be used

3603
03:38:07.725 --> 03:38:09.265
for FNN application,

3604
03:38:10.045 --> 03:38:14.905
or, you know, a kras uh, sequential can be used

3605
03:38:14.925 --> 03:38:18.025
for CNN application or kras.

3606
03:38:18.125 --> 03:38:20.865
Uh, again, sequential LSTM block can be used

3607
03:38:20.885 --> 03:38:22.345
for RNN application.

3608
03:38:23.245 --> 03:38:27.345
Um, so, um, that's a quick summary of what we learned

3609
03:38:27.645 --> 03:38:30.545
or went through in the last, uh, day and a half.

3610
03:38:31.245 --> 03:38:35.225
Um, so I'll, I think that's the end of the lecture.

3611
03:38:36.125 --> 03:38:40.015
So we'll, uh, we'll stay over for a few minutes here,

3612
03:38:40.275 --> 03:38:41.295
uh, for any questions.

3613
03:38:41.675 --> 03:38:44.215
Uh, so, uh, thank you for attending.

3614
03:38:44.835 --> 03:38:47.695
Uh, I know it's a lot of new material to go through,

3615
03:38:47.955 --> 03:38:52.295
but, um, I think, uh, once you get a more intuition

3616
03:38:52.295 --> 03:38:54.535
around these basics, F-N-N-C-N

3617
03:38:54.535 --> 03:38:56.935
and RN, uh, that will set you up well

3618
03:38:57.035 --> 03:38:58.535
for the next, uh, lectures.

3619
03:38:58.655 --> 03:39:02.455
I think this is one of the more, um, deeper classes

3620
03:39:02.455 --> 03:39:03.455
because it's kind

3621
03:39:03.455 --> 03:39:04.975
of touching base on several different

3622
03:39:04.975 --> 03:39:06.095
concepts at the same time.

3623
03:39:06.835 --> 03:39:09.375
So, uh, so if you can spend some time

3624
03:39:09.435 --> 03:39:12.215
to learn a little bit more about these three architectures,

3625
03:39:12.215 --> 03:39:15.055
that will set you up well for the next, uh,

3626
03:39:15.055 --> 03:39:16.055
classes in the course.

3627
03:39:16.595 --> 03:39:18.055
Uh, but again, thanks for attending.

3628
03:39:18.195 --> 03:39:22.975
Uh, wish you all the best. And, um, yeah, uh,

3629
03:39:23.255 --> 03:39:25.975
I don't think we have any GaN slides in this, uh, class.

3630
03:39:26.795 --> 03:39:27.855
Uh, just to be clear,

3631
03:39:41.225 --> 03:39:42.885
I'm happy to discuss from my own knowledge,

3632
03:39:42.905 --> 03:39:44.805
but I don't think we have any GaN slides

3633
03:39:44.905 --> 03:39:45.925
in this particular class.

3634
03:40:05.035 --> 03:40:08.245
Cool. Um, again, thanks everyone for attending.

3635
03:40:08.305 --> 03:40:09.925
Uh, I wish you all the best.

3636
03:40:10.065 --> 03:40:12.245
Uh, we'll see you on the coaching class as well

3637
03:40:12.245 --> 03:40:13.605
as the as interview class.

3638
03:40:14.105 --> 03:40:16.805
Uh, I'll stay over for a bit for any questions,

3639
03:40:16.985 --> 03:40:19.125
but feel free to drop off if you don't have any.

3640
03:40:19.785 --> 03:40:21.685
Um, have a good rest of the weekend.

3641
03:40:28.905 --> 03:40:29.685
Oh, um,

3642
03:40:33.825 --> 03:40:38.325
so I think GaN is, uh, post class GaN video.

3643
03:40:38.745 --> 03:40:40.285
Um, I'm not aware of that.

3644
03:40:41.375 --> 03:40:44.835
Um, so maybe, um, point me to that

3645
03:40:44.935 --> 03:40:46.595
or, you know, you can watch that

3646
03:40:46.615 --> 03:40:47.635
and we can discuss like,

3647
03:40:47.635 --> 03:40:50.755
if you have any questions on gans in the coaching session,

3648
03:40:51.655 --> 03:40:55.475
uh, but, uh, yeah,

3649
03:40:55.735 --> 03:40:56.875
in your given slides,

3650
03:41:09.125 --> 03:41:10.455
Yeah, I'm not sure, um,

3651
03:41:14.595 --> 03:41:18.005
typically gans are not covered in the sessions,

3652
03:41:18.745 --> 03:41:20.085
or at least in my understanding.

3653
03:41:20.905 --> 03:41:22.485
Um, but, you know, um,

3654
03:41:23.465 --> 03:41:28.025
if the given video presentation is not, um,

3655
03:41:28.655 --> 03:41:30.345
informative, I'm happy to discuss

3656
03:41:30.345 --> 03:41:31.345
that in the coaching session.

3657
03:41:34.905 --> 03:41:38.475
Vincent, uh, would it be good idea to add dropout players

3658
03:41:38.565 --> 03:41:39.755
after every layer?

3659
03:41:43.015 --> 03:41:47.785
Um, so

3660
03:41:48.175 --> 03:41:51.145
what are the negative effects of too much dropout?

3661
03:41:51.145 --> 03:41:54.825
Right. So, so too much dropout means you are essentially

3662
03:41:55.745 --> 03:41:58.225
blocking the network from learning.

3663
03:41:59.005 --> 03:42:03.875
So that's why you need to calibrate the dropout factor,

3664
03:42:05.155 --> 03:42:08.335
uh, uh, uh, in the right way.

3665
03:42:08.555 --> 03:42:12.095
Uh, so, uh, if you put like dropout equal to 0.7

3666
03:42:12.195 --> 03:42:15.695
or 0.8, um, that could cause like, you know, a significant,

3667
03:42:16.315 --> 03:42:18.975
uh, learning impairment for the network.

3668
03:42:19.795 --> 03:42:21.815
So, uh, dropout

3669
03:42:21.815 --> 03:42:25.135
after every layer is not, uh, uncommon.

3670
03:42:25.565 --> 03:42:28.935
I've seen that, uh, in CNN's after, like, you know,

3671
03:42:29.185 --> 03:42:33.365
after every block, not every layer, uh, like every block

3672
03:42:33.365 --> 03:42:36.485
of CNN pooling, then there is a dropout.

3673
03:42:37.385 --> 03:42:40.405
Um, maybe after every layer, it's a bit too much.

3674
03:42:41.145 --> 03:42:44.285
Um, so you think that would cause, uh,

3675
03:42:46.255 --> 03:42:49.405
low bias in the network, uh,

3676
03:42:49.405 --> 03:42:51.925
and it would cause lower learning capabilities.

3677
03:43:00.585 --> 03:43:03.605
And also as I think about dropout, um,

3678
03:43:05.235 --> 03:43:07.655
it is more beneficial to use

3679
03:43:07.655 --> 03:43:09.455
after like a fully connected layer,

3680
03:43:10.205 --> 03:43:12.455
because in convolution there is a sort of

3681
03:43:13.265 --> 03:43:14.965
generalization happening, right?

3682
03:43:15.065 --> 03:43:19.105
So you are not, uh, trying to, uh, um,

3683
03:43:20.205 --> 03:43:24.425
uh, so the convolution process itself tries to look at, uh,

3684
03:43:24.425 --> 03:43:28.025
different parts of the input data at different points

3685
03:43:28.025 --> 03:43:31.185
of time, and there is like a pooling operation happening,

3686
03:43:31.185 --> 03:43:34.665
which kind of de noises, um, the whole thing.

3687
03:43:35.125 --> 03:43:38.145
So, so it's, it might be beneficial

3688
03:43:38.145 --> 03:43:39.785
after every fully connected layer.

3689
03:43:40.245 --> 03:43:42.225
Um, so that's, that's how I would see it.

3690
03:43:51.055 --> 03:43:53.475
Uh, we are providing access to collab notebook.

3691
03:43:55.465 --> 03:43:59.385
Um, I don't have access

3692
03:43:59.385 --> 03:44:00.545
to the link you shared as well.

3693
03:44:02.035 --> 03:44:05.335
Um, so a good

3694
03:44:06.935 --> 03:44:09.175
feedback for the operations.

3695
03:44:09.515 --> 03:44:12.055
So maybe send that note there.

3696
03:44:13.075 --> 03:44:16.295
Uh, if you are not getting any response, I'm happy to

3697
03:44:17.325 --> 03:44:19.295
help you there, but, uh, yeah,

3698
03:44:19.375 --> 03:44:20.695
I don't have access to that notebook.

3699
03:44:20.695 --> 03:44:21.575
You shared now number,

3700
03:44:29.255 --> 03:44:32.515
but does open CV will play a role in CV glasses

3701
03:44:32.855 --> 03:44:35.985
or, yeah,

3702
03:44:38.685 --> 03:44:41.815
um, let me see.

3703
03:44:41.885 --> 03:44:42.935
Best way to answer that.

3704
03:44:43.115 --> 03:44:47.325
So, so open CV is a framework, right?

3705
03:44:47.465 --> 03:44:50.965
So CNN is like more of an architecture.

3706
03:44:51.465 --> 03:44:55.535
Um, so we, I don't think we use open cv.

3707
03:44:55.535 --> 03:44:57.815
So Open CV is traditionally used in a lot of,

3708
03:44:57.815 --> 03:44:59.415
you know, software engineering.

3709
03:44:59.765 --> 03:45:03.095
Like, you know, there is open CV libraries for, um,

3710
03:45:03.675 --> 03:45:05.655
you know, JavaScript or Node

3711
03:45:05.755 --> 03:45:10.095
or, uh, Python to do a lot of image processing, like,

3712
03:45:10.095 --> 03:45:13.575
you know, image manipulation, application transformations.

3713
03:45:14.235 --> 03:45:18.335
Uh, so it has less to do with the actual ML aspects

3714
03:45:18.335 --> 03:45:19.455
of it that we'll be learning.

3715
03:45:20.195 --> 03:45:23.255
Um, so, so no, uh, so the answer to that is no.

3716
03:45:27.875 --> 03:45:30.245
Okay, cool. Um, so, so I think

3717
03:45:32.105 --> 03:45:33.835
when we can decide on time series,

3718
03:45:43.165 --> 03:45:46.625
uh, so I, I'm guessing time series forecasting,

3719
03:45:46.625 --> 03:45:49.065
you're thinking of arima or something like that, right?

3720
03:45:55.675 --> 03:45:56.845
Yeah. Okay. Um,

3721
03:46:02.355 --> 03:46:02.825
let's see.

3722
03:46:02.965 --> 03:46:06.685
Um, I think

3723
03:46:08.115 --> 03:46:10.535
one is the scale of data, right?

3724
03:46:10.715 --> 03:46:13.935
The, um, and type of data.

3725
03:46:14.235 --> 03:46:18.855
So, so if you are trying to model a very complex pattern,

3726
03:46:19.275 --> 03:46:22.135
um, usually, you know, neural networks

3727
03:46:22.155 --> 03:46:24.655
or like in this case specific type

3728
03:46:24.655 --> 03:46:27.455
of neural network is LSTM is more applicable.

3729
03:46:28.355 --> 03:46:31.425
Um, also model interpretability.

3730
03:46:31.565 --> 03:46:34.305
So do you need the model to tell you why you came to

3731
03:46:34.305 --> 03:46:36.825
that particular conclusion or not?

3732
03:46:36.885 --> 03:46:39.025
So for that, you might want to use arima.

3733
03:46:39.645 --> 03:46:41.025
So ARIMA is like, you know,

3734
03:46:41.605 --> 03:46:44.495
you using progressive methods, right?

3735
03:46:44.635 --> 03:46:47.895
So with moving averages, so it's very explainable.

3736
03:46:48.475 --> 03:46:49.695
So simplicity

3737
03:46:49.835 --> 03:46:52.655
and interpretability are important, go with arima.

3738
03:46:53.525 --> 03:46:57.425
Um, and also it's very fast compared to lstm.

3739
03:46:57.425 --> 03:46:59.625
LS DM takes a lot of time and needs a lot of compute.

3740
03:47:00.605 --> 03:47:04.805
Um, so, but again, if there is less,

3741
03:47:06.305 --> 03:47:08.165
uh, linear relationship,

3742
03:47:08.185 --> 03:47:11.205
if the relationships are more non-linear, uh,

3743
03:47:11.455 --> 03:47:12.765
ARIMA won't do great

3744
03:47:12.765 --> 03:47:14.605
because it's just a moving average method.

3745
03:47:15.285 --> 03:47:18.505
So in those cases, ts will, will be more beneficial.

3746
03:47:24.125 --> 03:47:28.025
Uh, yeah, uh, uh, please beat

3747
03:47:28.685 --> 03:47:30.065
the market and let us know.

3748
03:47:30.205 --> 03:47:34.145
Uh, uh, uh, no, I, I,

3749
03:47:34.585 --> 03:47:39.505
I think I know it's, it's a very, uh, uh, I,

3750
03:47:39.505 --> 03:47:41.785
when I looked at it, it kind of, I kind

3751
03:47:41.785 --> 03:47:43.185
of thought similarly,

3752
03:47:43.205 --> 03:47:45.825
but, uh, the set is kind of a,

3753
03:47:46.025 --> 03:47:47.705
a manufactured data set in my opinion.

3754
03:47:47.945 --> 03:47:52.185
I don't think it's, uh, uh, uh, it's generalizable.

3755
03:47:52.295 --> 03:47:54.785
What we are seeing that there is not generalizable.

3756
03:47:55.165 --> 03:47:56.625
So stock market is very random.

3757
03:47:57.925 --> 03:48:02.025
So, uh, uh, obviously there are a lot of hedge funds

3758
03:48:02.025 --> 03:48:06.185
that use like very sophisticated models, um, uh,

3759
03:48:06.735 --> 03:48:10.265
like, um, you know, not just past data,

3760
03:48:10.405 --> 03:48:14.645
but also, you know, uh, factors from external, like,

3761
03:48:14.645 --> 03:48:17.965
you know, news articles coming out, kind of news articles,

3762
03:48:17.965 --> 03:48:20.965
the sentiment, uh, the volatility in the market,

3763
03:48:21.405 --> 03:48:22.485
economic events happening.

3764
03:48:23.225 --> 03:48:25.005
Uh, so there are a bunch of features

3765
03:48:25.115 --> 03:48:28.325
that go into these models, like even data from, you know,

3766
03:48:28.915 --> 03:48:30.325
what are the top stocks on Wall

3767
03:48:30.325 --> 03:48:31.565
Street bets and stuff like that.

3768
03:48:31.675 --> 03:48:35.325
Like all of this data is pumped into models these days to,

3769
03:48:35.985 --> 03:48:37.525
uh, create algo algos.

3770
03:48:38.385 --> 03:48:41.485
And these algos kind of, you know, uh,

3771
03:48:41.485 --> 03:48:43.325
currently work in the stock markets, like a lot

3772
03:48:43.325 --> 03:48:47.165
of hedge funds use those algos to trade, uh, especially

3773
03:48:47.165 --> 03:48:48.805
during, you know, lunch hours

3774
03:48:48.985 --> 03:48:51.965
and, uh, other, other, uh, low peak covers

3775
03:48:51.965 --> 03:48:55.205
where you see like algos taking over manual trades.

3776
03:48:55.585 --> 03:48:59.365
Uh, but again, uh, I don't think anyone perfected this, uh,

3777
03:48:59.385 --> 03:49:02.325
market, uh, prediction in my opinion.

3778
03:49:02.665 --> 03:49:02.885
So,

3779
03:49:33.095 --> 03:49:34.465
yeah, you're welcome everyone.

3780
03:49:34.525 --> 03:49:37.505
Uh, yeah, feel free to, you know, I'll stay for a few more,

3781
03:49:37.605 --> 03:49:40.185
uh, uh, and I'll drop off at one time,

3782
03:49:40.205 --> 03:49:42.265
but, uh, keep the questions coming.

3783
03:49:42.485 --> 03:49:44.065
Uh, you know, happy to answer.

3784
03:49:51.825 --> 03:49:52.825
I think, uh, at the end

3785
03:49:52.825 --> 03:49:54.185
of the slides, there are some good topics.

3786
03:49:54.605 --> 03:49:58.745
Uh, I want you to go through those, uh, on your own time

3787
03:49:58.885 --> 03:50:01.945
and, uh, maybe, you know, ask me questions.

3788
03:50:02.245 --> 03:50:05.225
Um, uh, a lot of things we already discussed, like,

3789
03:50:05.225 --> 03:50:08.625
you know, our learning rate can affect, uh,

3790
03:50:08.645 --> 03:50:11.985
or, you know, um, um,

3791
03:50:13.995 --> 03:50:15.495
talked about learning rate schedule

3792
03:50:15.555 --> 03:50:19.255
or how to change the learning rate based on

3793
03:50:19.255 --> 03:50:20.695
what we are seeing in terms of loss.

3794
03:50:22.675 --> 03:50:24.095
We talked about loss functions.

3795
03:50:24.835 --> 03:50:29.115
Um, uh, I think contrast to loss is an important one.

3796
03:50:29.175 --> 03:50:31.235
You can take a look, read about that.

3797
03:50:32.215 --> 03:50:35.715
Um, loss versus metrics, I think this is a good one.

3798
03:50:36.415 --> 03:50:37.835
Uh, it can help you.

3799
03:50:38.345 --> 03:50:41.115
Like, maybe this can come up as an interview question.

3800
03:50:41.555 --> 03:50:43.595
I don't know. Uh, this is a good one though.

3801
03:50:44.335 --> 03:50:48.195
Um, why, why we use laws versus just the metric itself,

3802
03:50:49.895 --> 03:50:51.315
um, or fitting under fitting.

3803
03:50:51.315 --> 03:50:55.115
We talked about it. Um, we looked at it like, you know,

3804
03:50:55.435 --> 03:50:57.355
training laws versus validation laws.

3805
03:51:03.385 --> 03:51:08.205
Um, I think this goes back to gradient descent

3806
03:51:08.855 --> 03:51:13.085
topic, uh, batch versus stochastic, how the updates happen.

3807
03:51:14.805 --> 03:51:17.955
Um, we talked about this,

3808
03:51:18.535 --> 03:51:20.395
and CNNs

3809
03:51:20.395 --> 03:51:24.315
and RNs, like how weights are shared, like in RNs, uh,

3810
03:51:24.425 --> 03:51:29.285
sharing is, uh, applied across time steps, right?

3811
03:51:29.545 --> 03:51:31.325
And CNN's uh,

3812
03:51:31.555 --> 03:51:34.805
sharing is used in convolution layers across the filters.

3813
03:51:36.955 --> 03:51:37.975
Um, yeah.

3814
03:51:46.685 --> 03:51:48.955
Right. Uh, thanks everyone.

3815
03:51:49.175 --> 03:51:51.155
Um, I'm gonna close the session now.

3816
03:51:51.295 --> 03:51:54.195
Um, um, so wish you all the best

3817
03:51:54.295 --> 03:51:56.835
and, uh, see you all on, I think Tuesday

3818
03:51:57.135 --> 03:52:00.585
and Thursday for the, uh, coaching

3819
03:52:00.585 --> 03:52:02.825
and assignment, uh, review sessions.

3820
03:52:03.475 --> 03:52:03.905
Thank you.
